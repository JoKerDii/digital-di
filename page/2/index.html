<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.1.1">

  <link rel="apple-touch-icon" sizes="180x180" href="/digital-di/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/digital-di/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/digital-di/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/digital-di/images/logo.svg" color="#222">

<link rel="stylesheet" href="/digital-di/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css" integrity="sha256-wiz7ZSCn/btzhjKDQBms9Hx4sSeUYsDrTLg7roPstac=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"jokerdii.github.io","root":"/digital-di/","images":"/digital-di/images","scheme":"Muse","darkmode":false,"version":"8.19.2","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/digital-di/js/config.js"></script>

    <meta property="og:type" content="website">
<meta property="og:title" content="Di&#39;s Blog">
<meta property="og:url" content="https://jokerdii.github.io/digital-di/page/2/index.html">
<meta property="og:site_name" content="Di&#39;s Blog">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Di Zhen">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://jokerdii.github.io/digital-di/page/2/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"en","comments":"","permalink":"","path":"page/2/index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Di's Blog</title>
  








  <noscript>
    <link rel="stylesheet" href="/digital-di/css/noscript.css">
  </noscript>
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
    </div>
  </div>

  <div class="site-meta">

    <a href="/digital-di/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Di's Blog</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
    </div>
  </div>
</div>







</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Di Zhen</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/digital-di/archives/">
          <span class="site-state-item-count">21</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://jokerdii.github.io/digital-di/2024/05/15/2024-May/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/digital-di/images/avatar.gif">
      <meta itemprop="name" content="Di Zhen">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Di's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Di's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/digital-di/2024/05/15/2024-May/" class="post-title-link" itemprop="url">2024 May</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2024-05-15 09:08:51" itemprop="dateCreated datePublished" datetime="2024-05-15T09:08:51-04:00">2024-05-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-07-13 11:29:16" itemprop="dateModified" datetime="2024-07-13T11:29:16-04:00">2024-07-13</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="Substack"><a href="#Substack" class="headerlink" title="Substack"></a>Substack</h3><blockquote>
<p><em>To me, the best model going forward is going to be based on the <strong>weighted performance per parameter and training token count.</strong> Ultimately, a model keeps getting better the longer you train it. Most open model providers could train longer, but it hasn‚Äôt been worth their time. We‚Äôre starting to see that change.</em></p>
<p><em><strong>The most important models will represent improvements in capability density</strong>, rather than shifting the frontier.</em></p>
<p><em>In some ways, it‚Äôs easier to make the model better by training longer compared to anything else, if you have the data.</em></p>
<p><em>The core difference between open and closed LLMs on these charts is <strong>how undertrained open LLMs often are</strong>. The only open model confirmed to be trained on a lot of tokens is DBRX.</em> </p>
<p><strong>‚Äï  The End of the ‚ÄúBest Open LLM‚Äù - Interconnects</strong> [<a target="_blank" rel="noopener" href="https://www.interconnects.ai/p/compute-efficient-open-llms?utm_source=profile&utm_medium=reader2">Link</a>]</p>
</blockquote>
<p>Good analysis of the direction of open LLM development in 2023 and 2024. In 2023, models were progressing in MMLU by leveraging more compute budgets to handle scaled active parameters and training tokens. In 2024, the progressing direction is slightly changed to be orthogonal to previous - which is improving on MMLU while keeping compute budgets constant.</p>
<blockquote>
<p><em>The companies that have users interacting with their models consistently have moats through data and habits. The models themselves are not a moat, as I discussed at the end of last year when I tried to <a target="_blank" rel="noopener" href="https://www.interconnects.ai/p/ml-moats">predict machine learning moats</a>, but there are things in the modern large language model (LLM) space that open-source will really struggle to replicate. Concretely, that difference is access to quality and diverse training prompts for fine-tuning. While I want open-source to win out for personal philosophical and financial factors, this obviously is not a walk in the park for the open-source community. It‚Äôll be a siege of a castle with, you guessed it, a moat. We‚Äôll see if the moat holds.</em></p>
<p><strong>‚Äï  Model commoditization and product moats - Interconnects</strong> [<a target="_blank" rel="noopener" href="https://www.interconnects.ai/p/gpt4-commoditization-and-moats">Link</a>]</p>
</blockquote>
<blockquote>
<p><em>The goal of promoting scientific understanding for the betterment of society has a long history. Recently I was pointed to the essay <a target="_blank" rel="noopener" href="https://www.ias.edu/sites/default/files/library/UsefulnessHarpers.pdf">The Usefulness of Useless Knowledge</a> by Abraham Flexner in 1939 which argued how basic scientific research without clear areas for profit will eventually turn into societally improving technologies. If we want LLMs to benefit everyone, my argument is that we need far more than just computer scientists and big-tech-approved social scientists working on these models. We need to continue to promote openness to support this basic feedback loop that has helped society flourish over the last few centuries.</em></p>
<p><em>The word openness has replaced the phrase open-source among most leaders in the open AI movement. It‚Äôs the easiest way to get across what your goals are, but it is not better in indicating how you‚Äôre actually supporting the open ecosystem. The three words that underpin the one messy word are <strong>disclosure</strong> (the details), <strong>accessibility</strong> (the interfaces and infrastructure), and <strong>availability</strong> (the distribution).</em></p>
<p><strong>‚Äï  We disagree on what open-source AI should mean - Interconnects</strong> [<a target="_blank" rel="noopener" href="https://www.interconnects.ai/p/flavors-of-open-source-ai?utm_source=profile&utm_medium=reader2">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Google: ‚ÄúA Positive Moment‚Äù</strong> [<a target="_blank" rel="noopener" href="https://www.appeconomyinsights.com/p/alphabet-a-positive-moment?utm_source=profile&utm_medium=reader2">Link</a>]</p>
</blockquote>
<p>The report of Google Search‚Äôs death is exaggerated so far. In fact, search advertising has grown faster at Google than at Microsoft. User searching behavior is harder to change than people expected. Also, Google is leading the development of AI powered tools for Search: 1) ‚Äúcircle to search‚Äù is feature allowing a search from an image, text, or video without switching apps. 2) ‚ÄúPoint your camera, ask a question‚Äù is a feature allowing for multisearch with both images and text for complex questions given an image to the tool. Overall, SGE (Search Generative Experience) is revolutionizing search experience (‚Äú10 blue links‚Äù) by  introducing a dynamic AI-enhanced experience. So far from I observed AI powers Google Search rather than weakens it.</p>
<blockquote>
<p><strong>Amazon: Wild Margin Expansion - App Economy Insights</strong> [<a target="_blank" rel="noopener" href="https://www.appeconomyinsights.com/p/amazon-wild-margin-expansion?utm_source=profile&utm_medium=reader2">Link</a>]</p>
</blockquote>
<p>Amazon‚Äôs margin expansion: AWS hit $100 B run rate with a 38% operating margin; Ads is surging; delivery costs have been reduced.</p>
<blockquote>
<p><em>The biggest risk is not correctly projecting demand for end-user AI consumption, which would threaten the utilization of the capacity and capital investments made by tech firms today. This would leave them exposed at the height of the valuation bubble, if and when it bursts, just like Cisco‚Äôs growth story that<a target="_blank" rel="noopener" href="https://www.wsj.com/amp/articles/SB973215201314032825"> began to unravel in 2000</a>.</em> <em>After all, history may not repeat, but it often rhymes.</em></p>
<p><em>At the Upfront Ventures confab mentioned earlier, Brian Singerman, a partner at Peter Thiel‚Äôs Founders Fund, was asked about contrarian areas worth investing in given the current landscape. <a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=QBzhRVRIf2Q&t=927s">His response</a>: ‚ÄúAnything not AI‚Äù.</em></p>
<p><strong>‚Äï  AI‚Äôs Bubble Talk Takes a Bite Out Of The Euphoria - AI Supremacy</strong> [<a target="_blank" rel="noopener" href="https://open.substack.com/pub/aisupremacy/p/ais-bubble-talk-takes-a-bite-out">Link</a>]</p>
</blockquote>
<p>When we talk about investment, we talk about economic values. Current situation of AI is very similar to Cisco‚Äôs in 2000. Cisco as an internet company spread the capacity of the World Wide Web, but sooner people realized that there is no economic value in internet company, instead, opportunities are in e-commerce etc. AI is a tool very similar to web tech. Currently, with heightened expectations, people are allocating investments and capital expenditure in AI model development, however, end-user demand is unclear and revenue is relatively minimal. This situation makes AI look like a bubble from a very long term perspective.</p>
<blockquote>
<p><em>Steve Jobs famously said that Apple stands at the intersection of technology and liberal arts. Apple is supposed to enhance and improve our lives in the physical realm, not to replace cherished physical objects indiscriminately.</em></p>
<p><strong>‚Äï  Apple‚Äôs Dystopian iPad Video - The Rational Walk Newsletter</strong> [<a target="_blank" rel="noopener" href="https://newsletter.rationalwalk.com/p/apples-dystopian-ipad-video">Link</a>]</p>
</blockquote>
<blockquote>
<p><em>Key pillars of the new strategy (on gaming):</em></p>
<ul>
<li><em>Expanding PC and cloud gaming options.</em></li>
<li><em>Powerful consoles (still a core part of the vision).</em></li>
<li><em>Game Pass subscriptions as the primary access point.</em></li>
<li><em>Actively bringing Xbox games to rival platforms (PS5, Switch).</em></li>
<li><em>Exploring mobile gaming with the potential for handheld hardware.</em></li>
</ul>
<p><em>Microsoft‚Äôs ‚Äúevery screen is an Xbox‚Äù approach is a gamble and may take a long time to pay off. But the industry is bound to be device-agnostic over time as it shifts to the cloud and offers cross-play and cross-progression. It‚Äôs a matter of when not if.</em></p>
<p><strong>‚Äï  Microsoft: AI Inflection - App Economy Insights</strong> [<a target="_blank" rel="noopener" href="https://www.appeconomyinsights.com/p/microsoft-ai-inflection">Link</a>]</p>
</blockquote>
<p>Highlights: Azure‚Äôs growth accelerated sequentially thanks to AI services and was the fastest-growing of the big three (Amazon AWS, Google Cloud, Microsoft Azure). On Search, Microsoft is losing market share to Alphabet. Capex on AI grows roughly 80% YoY. On gaming, it‚Äôs diversifying approaches from selling consoles. Copilot and the Office succeed with Enterprise customers.</p>
<blockquote>
<p><em>To founders, my advice is to remain laser-focused on building products and services that customers love, and be thoughtful and rational when making capital allocation decisions. Finding product-market fit is about testing and learning from small bets before doubling down, and it is often better to grow slower and more methodically as that path tends to lead to a more durable and profitable business. An axiom that doesn‚Äôt seem to be well understood is that the time it takes to build a company is also often its half-life.</em></p>
<p><strong>‚Äï  2023 Annual Letter - Chamath Palihapitiya</strong> [<a target="_blank" rel="noopener" href="https://chamath.substack.com/p/2023-annual-letter">Link</a>]</p>
</blockquote>
<p>This is a very insightful letter about how economic and tech trends of 2023 have shaped their thinking and investment portfolio. What I have learned from this letter: </p>
<ol>
<li><p>Tech industry has shifted their focus from unsustainable ‚Äúgrowth at any cost‚Äù to more prudent forms of capital allocation. This results in laying off employees and slashing projects that are not relevant to the core business.</p>
</li>
<li><p>Rising of interest rate is one of the reasons of bank crisis. During zero interest rate decade, banks sought higher rates of return by purchasing longer duration assets while the value of them are negatively correlated to interest rate. As those caused losses are known by the public, a liquidity crisis ensued.</p>
</li>
<li><p>The advancement of Gen AI has lowered the barriers of starting a software company, and lowered capital requirement in Bio Tech and material sciences, and changed the process of building companies fundamentally, and empowered new entrants to challenge established businesses.</p>
<ul>
<li><p>The key question is: where will value creation and capture take place? when and where should capital be allocated and company should be started? Some author‚Äôs opinions:</p>
<ul>
<li><p>It‚Äôs premature to declare winners now. Instead, author suggested people should deeply understand the underlying mechanisms that will be responsible for value creation over next few years.</p>
</li>
<li><p>There are at least two areas of value creation now</p>
<ol>
<li><p>Proprietary data</p>
<p>Example: recent partnership between Reddit and Google</p>
</li>
<li><p>Infrastructure used to run AI application</p>
<p>For apps built on top of language models, responsiveness is a critical lynchpin. However GPUs are not well-suited to run inference.</p>
<p>Example: Author‚Äôs investment in Groq‚Äôs LPU for inference</p>
</li>
</ol>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Heightened geopolitical tensions due to Russia-Ukraine conflict, Israel and Hamas, escalating tensions between China and Taiwan, resulted in a de-globalization trend and also a strategic shift in the US. US legislative initiatives aims to fuel a domestic industrial renaissance by incentivizing reshoring and fostering a more secure and resilient supply chain. They include CHIPS Act, Infrastructure Investment, Job Act, Inflation Reduction Act, etc. </p>
<ul>
<li>The author highlights the opportunity for allocators and founders: companies can creatively and strategically tap into different pools of capital-debt, equity, and government funding.</li>
</ul>
</li>
</ol>
<blockquote>
<p><em>OpenAI‚Äôs strategy to get its technology in the hands of as many developers as possible ‚Äî to build as many use cases as possible ‚Äî is more important than the bot‚Äôs flirty disposition, and perhaps even new features like its translation capabilities (<a target="_blank" rel="noopener" href="https://twitter.com/Kantrowitz/status/1790073470753165538">sorry</a>). If OpenAI can become the dominant AI provider by delivering quality intelligence at bargain prices, it could maintain its lead for some time. That is, as long as the cost of this technology doesn‚Äôt drop near zero.</em></p>
<p><em>A tight integration with Apple could leave OpenAI with a strong position in consumer technology via the iPhone and an ideal spot in enterprise via its partnership with Microsoft.</em> </p>
<p><strong>‚Äï  OpenAI Wants To Get Big Fast, And Four More Takeaways From a Wild Week in AI News - Big Technology</strong> [<a target="_blank" rel="noopener" href="https://www.bigtechnology.com/p/openai-wants-to-get-big-fast">Link</a>]</p>
</blockquote>
<p>As GPT-4o is 2x faster and 50% cheaper, this discourages competitors to develop LLMs to compete and encourages companies to build with OpenAI‚Äôs model for their business. This shows that OpenAI wants to get big fast. However, making GPT-4o free disincentivizes users from subscribing the Plus version.</p>
<p>There is a tight and deep bond between OpenAI and Apple. The desktop app has been debuted on Mac and Apple will build OpenAI‚Äôs GPT Tech into mobile iOS. </p>
<blockquote>
<p><em>‚ÄúYou can borrow someone else‚Äôs stock ideas but you can‚Äôt borrow their conviction. True conviction can only be obtained by trusting your own research over that of others. Do the work so you know when to sell. Do the work so you can hold. Do the work so you can stand alone.‚Äù</em></p>
<p><em>Investing isn‚Äôt about blindly following the herd. It‚Äôs about carving your own path, armed with knowledge, patience, and a relentless pursuit of growth and learning.</em></p>
<p><strong>‚Äï Hedge Funds‚Äô Top Picks in Q1 - App Economy Insights</strong> [<a target="_blank" rel="noopener" href="https://www.appeconomyinsights.com/p/hedge-funds-top-picks-in-q1">Link</a>]</p>
</blockquote>
<blockquote>
<p><em>As I‚Äôve dug into this in more detail, I‚Äôve become convinced that they are doing something powerful by <strong>searching over language steps via tree-of-thoughts reasoning</strong>, but it is much smaller of a leap than people believe. The reason for the hyperbole is the goal of linking large language model training and usage to the core components of Deep RL that enabled success like AlphaGo: self-play and look-ahead planning.</em></p>
<p><em>To create the richest optimization setting, having the ability to generate diverse reasoning pathways for scoring and learning from is essential. This is where Tree-of-Thoughts comes in. <strong>The prompting from ToT gives diversity to the generations, which a policy can learn to exploit with access to a PRM</strong>.</em></p>
<p><em>Q seems to be using PRMs to score Tree of Thoughts reasoning data that then is optimized with Offline RL. This wouldn‚Äôt look too different from existing RLHF toolings that use offline algorithms like DPO or ILQL that do not need to generate from the LLM during training. The ‚Äòtrajectory‚Äô seen by the RL algorithm is the sequence of reasoning steps, so we‚Äôre finally doing RLHF in a multi-step fashion rather than contextual bandits!</em></p>
<p><em><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2305.20050">Let‚Äôs Verify Step by Step</a></em>: <em>a good introduction to PRMs.</em></p>
<p><em><em>‚Äï The Q</em> hypothesis: Tree-of-thoughts reasoning, process reward models, and supercharging synthetic data - Interconnects</em>*  [<a target="_blank" rel="noopener" href="https://www.interconnects.ai/p/q-star">Link</a>]</p>
</blockquote>
<blockquote>
<p><em>It‚Äôs well known on the street that Google DeepMind has split all projects into three categories: Gemini (the large looming model), Gemini-related in 6-12months (applied research), and fundamental research, which is oddly only &gt; 12 months out. <strong>All of Google DeepMind‚Äôs headcount is in the first two categories, with most of it being in the first</strong>.</em></p>
<p><em>Everyone on <strong>Meta‚Äôs GenAI technical staff should spend</strong> <strong>about 70% of the time directly on incremental model improvements and 30% of the time on ever-green work.</strong></em> </p>
<p><em>A great <a target="_blank" rel="noopener" href="https://fchollet.substack.com/p/how-i-think-about-llm-prompt-engineering">read</a> from Francois Chollet on links between prompting LLMs, word2vec, and attention. One of the best ML posts I‚Äôve read in a while.</em></p>
<p><em><a target="_blank" rel="noopener" href="https://docs.google.com/presentation/d/1636wKStYdT_yRPbJNrf8MLKpQghuWGDmyHinHhAKeXY/mobilepresent?pli=1&slide=id.g2885e521b53_0_5">Slides</a> from Hyung Won Chung‚Äôs (OpenAI) talk on LLMs. Great summary of intuitions for the different parts of training. The key point: We can get further with RLHF because the objective function is flexible.</em></p>
<p><strong>‚Äï The AI research job market shit show (and my experience) - Interconnects</strong> [<a target="_blank" rel="noopener" href="https://www.interconnects.ai/p/ai-research-job-market">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>10 Lessons From 2024 Berkshire Hathaway Annual Shareholder Meeting - Capitalist Letters</strong>  [<a target="_blank" rel="noopener" href="https://www.capitalist-letters.com/p/10-lessons-from-2024-berkshire-hathaway">Link</a>]</p>
</blockquote>
<p>What I‚Äôve learned from this article:</p>
<ol>
<li><p>Why did Berkshire trimmed its APPL position?</p>
<p>No concern about Apple‚Äôs earnings potential, make sense to take some profits as value is now too high.</p>
</li>
<li><p>Right way to look at share buybacks</p>
<p>A business should pay dividends only if it cannot make good use of the excess capital it has. Good use capital means the Return of Equity, which is on average 12% for American companies. If the company is able to allocate capital better than shareholders themselves and provide them with above average returns, it should retain the earnings and allocate capital itself.</p>
<p>Buybacks only makes sense at the right price and buying back shares just to support stock price is not the best action ti take for shareholders. All investment decisions should be price dependent.</p>
</li>
<li><p>How would he invest small sums of money?</p>
<p>At the time of market crashes or economic downturns, you find exceptional companies trading at ridiculously cheap prices and that‚Äôs your opportunity, When you find those companies fairly priced or overvalued and you look for special situations while holding onto your positions in those exceptional companies.</p>
</li>
<li><p>Views on capital allocation</p>
<p>Study picking businesses, not stocks.</p>
</li>
<li><p>Investing in foreign countries</p>
<p>America has been a great country for building wealth and capitalist democracy is the best system of governance ever invented.</p>
</li>
<li><p>Advice on job picking</p>
<p>Remember Steve Jobs‚Äô famous words in the Stanford Commencement speech he gave before his death: ‚ÄúKeep looking, don‚Äôt settle!‚Äù</p>
</li>
<li><p>On the importance of culture</p>
<p>In Berkshire culture, shareholders feel themselves as the owners of the businesses. Greg Abel will keep the culture alive in the post-Buffett period and this will automatically attract top talent to a place where they are given full responsibility and trust.</p>
</li>
<li><p>When to sell stocks</p>
<ol>
<li>A bigger opportunity comes up, 2.  something drastically changes in the business, and 3. to raise money</li>
</ol>
</li>
<li><p>Effects of consumer behavior on investment decisions</p>
<p>Two types of businesses have durable competitive advantage: 1) Lowest cost suppliers of products and services, 2) suppliers of unique products and services.</p>
</li>
<li><p>How to live a good life?</p>
<p>‚ÄúI‚Äôve written my obituary the way I‚Äôve lived my life‚Äù‚Äò - Charlie Munger</p>
</li>
</ol>
<blockquote>
<p><strong>NVIDIA: Industrial Revolution - App Economy Insights</strong> [<a target="_blank" rel="noopener" href="https://www.appeconomyinsights.com/p/nvidia-industrial-revolution">Link</a>]</p>
</blockquote>
<p>Primary drivers of Data Center Revenue: 1) Strong demand (up 29% sequentially) for the Hopper GPU computing platform used for training and inferencing with LLMs, recommendation engines, and GenAl apps, 2) InfiniBand end-to-end solutions (down 5% sequentially due to timing of supply) for networking. NVIDIA started shipping the Spectrum-X Ethernet networking solutions optimized for Al.</p>
<p>In the earning call, three major customer categories are provided: 1) cloud service providers (CSPs) including hyperscalers Amazon Microsoft and Google. 2) enterprise usage: Tesla expanded training Al cluster to 35000 H100 GPUs and used NVIDIA Al for FSD V12. 3) consumer internet companies: Meta‚Äôs Llama 3 powering Meta Al was trained on a cluster of 24000 H100 GPUs.</p>
<p>Huang explained in the earning call that AI is not a chip problem only but also a systems problem now. They build AI factories.</p>
<p>For further growth, Blackwell platform is coming, Spectrum-X networking is expanding, new software tools like NIMs is developing.</p>
<blockquote>
<p><em>A lot of current research focuses on LLM architectures, data sources prompting, and alignment strategies. While these can lead to better performance, such developments have 3 inter-related critical flaws-</em> </p>
<ol>
<li><em>They mostly work by increasing the computational costs of training and&#x2F;or inference.</em> </li>
<li><em>They are a lot more fragile than people realize and don‚Äôt lead to the across-the-board improvements that a lot of Benchmark Bros pretend.</em> </li>
<li><em>They are incredibly boring. A focus on getting published&#x2F;getting a few pyrrhic victories on benchmarks means that these papers focus on making tweaks instead of trying something new, pushing boundaries, and trying to address the deeper issues underlying these processes.</em></li>
</ol>
<p><strong>‚Äï Revolutionizing AI Embeddings with Geometry [Investigations] - Devansh</strong> [<a target="_blank" rel="noopener" href="https://artificialintelligencemadesimple.substack.com/p/revolutionizing-ai-embeddings-with">Link</a>]</p>
</blockquote>
<p>Very few AI research work don‚Äôt have # 1 and # 3 flaws and they are really good hard-core work. Time is required to verify whether they are generalizable, widely applicable or not. Especially nowadays the process of scientific research is very different from previous years where there was usually a decade between starting your work and publishing it.</p>
<p>This article highlights some publications in complex embedding and looked into how they improved embeddings by using complex numbers. Current challenges in embedding are 1) sensitivity to outliers 2) limited capacity in capture complex relationship in unstructured text, 3) inconsistency in pairwise rankings of similarities, and 4) computational cost. The next generation complex embedding is benefitting from the following pillars: 1) complex geometry provides richer space to capture nuanced relationships and handle outliers, 2) orthogonality allows each dimension to be independent and distinct, 3) contrastive learning can be used to minimize the distance between similar pairs and maximize the distance between dissimilar pairs. Complex embeddings have a lot of advantages: 1) increasing representation capacity with two components (real and imaginary) of complex numbers, 2) complex geometry allows for orthogonality and thus improves generalization, and also allows use to reach stable convergence quickly, 3) robust features can be captured which improves robustness, and 4) solved limitation of cosine similarity (saturation zones which lead to vanishing gradients during optimization) by angle optimization in complex space.</p>
<blockquote>
<p><em>Llama 3 8B might be the most interesting all-rounder for fine-tuning as it can be fine-tuned no a single GPU when using LoRA.</em></p>
<p><em>Phi-3 is very appealing for mobile devices. A quantized version of it can run on an iPhone 14.</em></p>
<p><strong>‚Äï How Good Are the Latest Open LLMs? And Is DPO Better Than PPO?</strong> [<a target="_blank" rel="noopener" href="https://magazine.sebastianraschka.com/p/how-good-are-the-latest-open-llms">Link</a>]</p>
</blockquote>
<p>Good paper review article. Highlights key discussions:</p>
<ul>
<li><p>Mixtral 8x22B: The key idea is to replace each feed-forward module in a transformer architecture with 8 expert layers. It achieves lower active parameters (cost) and higher performance (MMLU).</p>
</li>
<li><p>Llama 3: The main difference between Llama 3 and Llama 2 are 1) vocab size has been increased, 2) used grouped-query attention, 3) used both PPO &amp; DPO. The key research finding is that the more data the better performance, no matter what model size is. </p>
<p>‚ÄúLlama 3 8B might be the most interesting all-rounder for fine-tuning as it can be fine-tuned no a single GPU when using LoRA.‚Äù</p>
</li>
<li><p>Phi-3: Key characteristics are 1) it‚Äôs based on Llama architecture, 2) trained on 5x fewer tokens than Llama 3, 3) used the same tokenizer with a vocab size of 32064 as Llama2, much smaller than Llama 3 vocab size, 4) has only 3.8B parameters, less than half the size of Llama 3 8B, 5) secret sauce is dataset quality over quantity - it‚Äôs trained on heavily filtered web data and synthetic data. </p>
<p>‚ÄúPhi-3 is very appealing for mobile devices. A quantized version of it can run on an iPhone 14.‚Äù</p>
</li>
<li><p>OpenELM: key characteristics are 1) 4 relatively small sizes: 270M, 450M,1.1B, and 3B, 2) instruct version trained with rejection sampling and DPO, 3) slightly better than OLMo in performance, even though trained on 2x fewer tokens, 4) main architecture teak - a layer-wise scaling strategy, 5) sampled a relatively smaller subset of 1.8T tokens from various public datasets, but no clear rationale for subsampling, 6) one main research finding is that there is no clear difference between LoRA and DoRA for parameter efficient fine-tuning.</p>
<p>About the layer-wise scaling strategy: 1) there are N transformer blocks in a model, 2) layers are gradually widened from the early to the later transformer blocks, so for each block: a) number of heads are increased, b) dimension of each layer is increased.</p>
</li>
<li><p>DPO vs PPO: The main difference between DPO and PPO is that ‚ÄúDPO does not require training a separate reward model but uses a classification-like objective to update LLM directly‚Äù. </p>
<p>Key findings of the paper and best practices suggested: 1) PPO is generally better than DPO if you use it correctly. DPO suffers from out-of-distribution data, which means instruction data is different from preference data. The solution could be to ‚Äúadd a supervised instruction fine-tuning round on the preference dataset before following up with DPO fine-tuning.‚Äù, 2) If you use DPO, make sure to perform SFT on preference data first, 3) ‚Äúiterative DPO which involves labeling additional data with an existing reward model is better than DPO on existing preference data.‚Äù, 4) ‚ÄúIf you use PPO, the key is to use large batch sizes, advantage normalization, and parameter update via exponential moving average.‚Äù, 5) though PPO is generally better, DPO is more straightforward and will still be a popular go-to option, 6) both can be used. Recall the pipeline behind Llama3: pretraining -&gt; SFT -&gt; rejection sampling -&gt; PPO -&gt; DPO.</p>
</li>
</ul>
<blockquote>
<p><strong>Google I&#x2F;O AI keynote updates 2024 - AI Supremacy</strong> [<a target="_blank" rel="noopener" href="https://www.ai-supremacy.com/p/google-io-ai-keynote-updates-2024?r=333z5o&utm_medium=ios&triedRedirect=true">Link</a>] </p>
<p><strong>Streaming Wars Visualized - App Economy Insights</strong> [<a target="_blank" rel="noopener" href="https://open.substack.com/pub/appeconomyinsights/p/streaming-wars-visualized?r=333z5o&utm_medium=ios">Link</a>]</p>
<p><strong>This Week in Visuals - App Economy Insights</strong> [<a target="_blank" rel="noopener" href="https://www.appeconomyinsights.com/p/this-week-in-visuals?lli=1&utm_source=profile&utm_medium=reader2">Link</a>]</p>
<p><strong>Gig Economy Shakeup - App Economy Insights</strong> [<a target="_blank" rel="noopener" href="https://www.appeconomyinsights.com/p/gig-economy-shakeup?r=333z5o&utm_medium=ios&triedRedirect=true">Link</a>]</p>
</blockquote>
<h3 id="Articles"><a href="#Articles" class="headerlink" title="Articles"></a>Articles</h3><blockquote>
<p><strong>Musings on building a Generative AI product - LinkedIn Engineering Blog</strong> [<a target="_blank" rel="noopener" href="https://www.linkedin.com/blog/engineering/generative-ai/musings-on-building-a-generative-ai-product">Link</a>]</p>
</blockquote>
<p>This is a very good read about developing Gen AI product for business by using pre-trained LLM. This article elaborates how this product is designed, how each part works specifically, what works and what does not work, what has been improving, and what has been struggling. Some takeaways for me are </p>
<ol>
<li><p>Supervised fine tuning step was done by embedding-based retrieval (EBR) powered by an in-memory database to inject response examples into prompts. </p>
</li>
<li><p>An organizational structure was designed to ensure communication consistency: one horizontal engineering pod for global templates and styles, and several vertical engineering pods for specific tasks such as summarization, job fit assessment, interview tips, etc.</p>
</li>
<li><p>Tricky work: </p>
<ol>
<li><p>Developing end to end automatic evaluation pipeline.</p>
</li>
<li><p>Skills in dynamically discover and invoke APIs &#x2F; agents.</p>
<p>This requires input and output to be ‚ÄòLLM friendly‚Äô - JSON or YAML schemes. </p>
</li>
<li><p>Supervised fine tuning by injected responses of internal database.</p>
<p>As evaluation becoming more sophisticated, prompt engineering needs to be improved to reach high quality&#x2F;evaluation scores. The difficulty is that quality scores shoot up fast then plateau so it‚Äôs hard to reach a very high score in the late improvement stage. This makes prompt engineering more like an art rather than science. </p>
</li>
<li><p>Tradeoff of capacity and latency</p>
<p>Chain of Thoughts can improve quality and accuracy of responses, but increase latency. TimeToFirstToken (TTFT) &amp; TimeBetweenTokens (TBT) are important to utilization but need to be bounded to limit latency. Besides, they also intend to implement end to end streaming and async non-blocking pipeline.</p>
</li>
</ol>
</li>
</ol>
<blockquote>
<p><em>The concept of open source was devised to ensure developers could use, study, modify, and share software without restrictions. But AI works in fundamentally different ways, and key concepts don‚Äôt translate from software to AI neatly, says Maffulli.</em></p>
<p><em>But depending on your goal, dabbling with an AI model could require access to the trained model, its training data, the code used to preprocess this data, the code governing the training process, the underlying architecture of the model, or a host of other, more subtle details.</em></p>
<p><em>Which ingredients you need to meaningfully study and modify models remains open to interpretation.</em> </p>
<p><em>both Llama 2 and Gemma come with licenses that restrict what users can do with the models. That‚Äôs anathema to open-source principles: one of the key clauses of the Open Source Definition outlaws the imposition of any restrictions based on use cases.</em></p>
<p><em>All the major AI companies have simply released pretrained models, without the data sets on which they were trained. For people pushing for a stricter definition of open-source AI, Maffulli says, this seriously constrains efforts to modify and study models, automatically disqualifying them as open source.</em></p>
<p><strong>‚Äï  The tech industry can‚Äôt agree on what open-source AI means. That‚Äôs a problem. ‚Äï MIT Technology Review</strong> [<a target="_blank" rel="noopener" href="https://www.technologyreview.com/2024/03/25/1090111/tech-industry-open-source-ai-definition-problem/">Link</a>]</p>
</blockquote>
<p>This article argues that the definitions of open-source AI are problematic. ‚ÄòOpen‚Äô models either have restriction on usage or don‚Äôt release details of training data. This does not fit traditional definition of ‚Äòopen source‚Äô. However, people argue that for the special case of AI, we need different definition of open source. As long as the definition remains vague, it‚Äôs problematic, because big tech will define open-source AI to be what suits it. </p>
<blockquote>
<p><strong>Everything I know about the XZ backdoor</strong> [<a target="_blank" rel="noopener" href="https://boehs.org/node/everything-i-know-about-the-xz-backdoor">Link</a>]</p>
<p>Some great high-level technical overview of XZ backdoor [<a target="_blank" rel="noopener" href="https://gynvael.coldwind.pl/?lang=en&id=782">Link</a>] [<a target="_blank" rel="noopener" href="https://gist.github.com/thesamesam/223949d5a074ebc3dce9ee78baad9e27">Link</a>] [<a target="_blank" rel="noopener" href="https://gist.github.com/thesamesam/223949d5a074ebc3dce9ee78baad9e27">Link</a>] [<a target="_blank" rel="noopener" href="https://infosec.exchange/@fr0gger/112189232773640259">Infographic</a>] [<a target="_blank" rel="noopener" href="https://research.swtch.com/xz-script">Link</a>] [<a target="_blank" rel="noopener" href="https://bsky.app/profile/filippo.abyssdomain.expert/post/3kowjkx2njy2b">Link</a>]</p>
</blockquote>
<p>A backdoor in xz-utils (used for lossless compression) was recently revealed by Andres Freund (Principle SDE at Microsoft). The backdoor only shows up when a few specific criteria are met at least: 1) running a distro that uses glibc, 2) have version 5.6.0 or 5.6.1 xz installed or liblzma installed. There is a malicious script called <code>build-to-host.m4</code> which checks for various conditions like the architecture of the machine. If those conditions check, the payload is injected into the source tree. The intention of payload is still under investigation. Lasse Collin, one of the maintainer of the repo, has posted an <a target="_blank" rel="noopener" href="https://tukaani.org/xz-backdoor/">update</a> and is working on carefully analyzing the situation. The author Evan Boehs in the article present a timeline of the attack and online investigators‚Äô discoveries of Jia Tan identity (from IP address, LinkedIn, <a target="_blank" rel="noopener" href="https://rheaeve.substack.com/p/xz-backdoor-times-damned-times-and">commit timings</a>, etc), and raises our awareness of the human costs of open source.</p>
<blockquote>
<p><em>Having a crisp mental model around a problem, being able to break it down into steps that are tractable, perfect first-principle thinking, sometimes being prepared (and able to) debate a stubborn AI ‚Äî these are the skills that will make a great engineer in the future, and likely the same consideration applies to many job categories.</em></p>
<p><strong>‚Äï  Why Engineers Should Study Philosophy ‚Äï  Harvard Business Review</strong> [<a target="_blank" rel="noopener" href="https://hbr.org/2024/04/why-engineers-should-study-philosophy?utm_medium=email&utm_source=circ_other&utm_campaign=subbenemail_digitalcontent_monthinreview&hideIntromercial=true&tpcc=subbenemail&deliveryName=SUB_Ben_DigitialContent_MonthInReview_20240507">Link</a>]</p>
</blockquote>
<p>Human comes into a new stage of learning: smartly asking AI questions to get answers as accurate as possible. So prompt engineering is a very important skill in AI era. In order to master prompt engineering, we need to have divide and conquer mindset, perfect first-principle thinking, critical thinking, and skepticism.</p>
<blockquote>
<p><em>If we had infinite capacity for memorisation, it‚Äôs clear the transformer approach is better than the human approach - it truly is more effective. But it‚Äôs less efficient - transformers have to store so much information about the past that might not be relevant. Transformers (ü§ñ) only decide what‚Äôs relevant at <strong>recall time</strong>. The innovation of Mamba (üêç) is allowing the model better ways of forgetting earlier - it‚Äôs focusing by choosing what to discard using <strong>Selectivity</strong>, throwing away less relevant information at <strong>memory-making time</strong>.</em></p>
<p><strong>‚Äï Mamba Explained</strong> [<a target="_blank" rel="noopener" href="https://thegradient.pub/mamba-explained/">Link</a>]</p>
</blockquote>
<p>A very in-depth explanation of Mamba architecture. So the main difference between Transformer and Mamba is that Transformer stores all past information and decides what is relevant at recall time. While Mamba uses Selectivity to decide what to discard earlier. Mamba ensures both efficiency and effectiveness (space complexity reduces from O(n) to O(1), time complexity reduces from O(n^2) to O(n)). If Transformer has high effectiveness and low efficiency due to large state, and RNN has high efficiency and low effectiveness due to small state, Mamba is in between - Mamba selectively and dynamically compress data into the state.</p>
<blockquote>
<p><strong>The Power of Prompting ‚Äï Microsoft Research Blog</strong> [<a target="_blank" rel="noopener" href="https://www.microsoft.com/en-us/research/blog/the-power-of-prompting/">Link</a>]</p>
</blockquote>
<p>Basically this study demonstrates that GPT-4 is able to outperform a leading model that was fine-tuned specifically for medical application by Medprompt - a composition of several prompting strategies. This shows that fine-tuning might not be necessary in the future though it can boost performance, it is resource-intensive and cost-prohibitive. Simple prompting strategies could serve to transform generalist models into specialists and extending benefits of models to new domains and applications. Similar study was also done in finance domain by JP Morgan with similar results.</p>
<blockquote>
<p><em>Previously, we made some progress matching patterns of neuron activations, called features, to human-interpretable concepts. We used a technique called ‚Äúdictionary learning‚Äù, borrowed from classical machine learning, which isolates patterns of neuron activations that recur across many different contexts.</em></p>
<p><em>In turn, any internal state of the model can be represented in terms of a few active features instead of many active neurons. Just as every English word in a dictionary is made by combining letters, and every sentence is made by combining words, every feature in an Al model is made by combining neurons, and every internal state is made by combining teatures.</em></p>
<p><em>The features are likely to be a faithful part of how the model internally represents the world, and how it uses these representations in its behavior.</em></p>
<p><strong>‚Äï Mapping the Mind of a Large Language Model - Anthropic</strong> [<a target="_blank" rel="noopener" href="https://www.anthropic.com/news/mapping-mind-language-model">Link</a>]</p>
</blockquote>
<p>This is an amazing work towards AI safety by Anthropic. The main goal is to understand the inner workings of AI models and identify how millions of concepts are represented inside Claude Sonnet, so that developers can better control AI safety. Previous progress of this work was to match pattern of neuron activations (‚Äúfeatures‚Äù) to human-interpretable concepts by technique called ‚Äúdictionary learning‚Äù. Now they are scaling up the technique to the vastly larger AI language models. Below is a list of key experiments and findings. </p>
<ol>
<li>Extracted millions of features from the middle layer of Claude 3.0 Sonnet. Features have a depth, breadth, and abstraction reflecting Sonnet‚Äôs advanced capabilities.</li>
<li>Find more abstract features - responding to bugs in code, discussion of gender biases in professions, etc.</li>
<li>Measure a ‚Äúdistance‚Äù between features based on which neurons appeared in their activation patterns. They find that features with similar concept are close to each other. This demonstrates internal organization of concepts in AI model correspond to human notions of similarity.</li>
<li>By artificially amplifying or suppressing features, they see how Claude‚Äôs responses change. This shows that features can be used to change how a model acts.</li>
<li>For the purpose of AI safety, they find features corresponding to the capabilities with misuse potential (code backdoors, developing bio-weapons), different forms of biases (gender discrimination, racist claims about crime), and potentially problematic AI behavior (power-seeking, manipulation, secrecy)</li>
<li>For previous concern about sycophancy, they also find a feature associated with sycophantic praise.</li>
</ol>
<p>This study proposed a good approach to ensure AI safety: use the technique described here to monitor AI systems for dangerous behaviors and to debias outcomes.</p>
<blockquote>
<p><em>To qualify as a ‚ÄúCopilot+ PC‚Äù a computer needs distinct CPUs, GPUs, and NPUs (neural processing units) capable of &gt;40 trillion operations per second (TOPS), and a minimum of 16 GB RAM and a 256 GB SSD.</em> </p>
<p><em>All of those analysts who assumed Wal-Mart would squish Amazon in e-commerce thanks to their own mastery of logistics were like all those who assumed Microsoft would win mobile because they won PCs. It turns out that logistics for retail are to logistics for e-commerce as operating systems for a PC are to operating systems for a phone. They look similar, and even have the same name, but require fundamentally different assumptions and priorities.</em></p>
<p><em>I then documented a few seminal decisions made to demote windows, including releasing Office on iPad as soon as he took over, explicitly re-orienting Microsoft around <a target="_blank" rel="noopener" href="https://stratechery.com/2013/services-not-devices/">services instead of devices</a>, isolating the Windows organization from the rest of the company, killing Windows Phone, and finally, in the decision that prompted that Article, splitting up Windows itself. Microsoft was finally, not just strategically but also organizationally, a services company centered on Azure and Office; yes, Windows existed, and still served a purpose, but it didn‚Äôt call the shots for the rest of Microsoft‚Äôs products.</em></p>
<p><em>That celebration, though, is not because Windows is differentiating the rest of Microsoft, but because the rest of Microsoft is now differentiating Windows. Nadella‚Äôs focus on AI and the company‚Äôs massive investments in compute are the real drivers of the business, and, going forward, are real potential drivers of Windows.</em></p>
<p><em>This is where the Walmart analogy is useful: McMillon needed to let e-commerce stand on its own and drive the development of a consumer-centric approach to commerce that depended on centralized tech-based solutions; only then could Walmart integrate its stores and online services into an omnichannel solution that makes the company the only realistic long-term rival to Amazon.</em></p>
<p><em>Nadella, similarly, needed to break up Windows and end Ballmer‚Äôs dreams of vertical domination so that the company could build a horizontal services business that, a few years later, could actually make Windows into a differentiated operating system that might, for the first time in years, actually drive new customer acquisition.</em></p>
<p><strong>‚Äï Windows Returns - Stratechery</strong> [<a target="_blank" rel="noopener" href="https://stratechery.com/2024/windows-returns/">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Chatbot Arena results are in: Llama 3 dominates the upper and mid cost-performance front (full analysis) ‚Äï Reddit</strong> [<a target="_blank" rel="noopener" href="https://www.reddit.com/r/LocalLLaMA/comments/1cakcfq/chatbot_arena_results_are_in_llama_3_dominates/">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Efficiently fine-tune Llama 3 with PyTorch FSDP and Q-Lora</strong> [<a target="_blank" rel="noopener" href="https://www.philschmid.de/fsdp-qlora-llama3">Link</a>]</p>
</blockquote>
<h3 id="YouTube-and-Podcasts"><a href="#YouTube-and-Podcasts" class="headerlink" title="YouTube and Podcasts"></a>YouTube and Podcasts</h3><blockquote>
<p><em>I don‚Äôt have an answer to peace in the Middle East, I wish I did, but I do have a very strong view that we are not going to get to peace when we are apologizing or denying crimes against humanity and crime mass rape of women. That‚Äôs not the path to peace, the path to peace is not saying this didn‚Äôt happen, the path to peace is saying this happened no matter what side of the fence you are on no matter what side of the world you are on, if you are the far right the far left, anywhere on the world, we are not going to let this happen again and we are going to get to peace to make sure.  - Sheryl Sandberg</em></p>
<p><strong>‚Äï  In conversation with Sheryl Sandberg, plus open-source AI gene editing explained - All-In Podcast</strong> [<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=OxbtNsenZJY&ab_channel=All-InPodcast">Link</a>]</p>
<p>U.N. to Study Reports of Sexual Violence in Israel During Oct. 7 Attack [<a target="_blank" rel="noopener" href="https://www.nytimes.com/2024/01/29/world/middleeast/israel-hamas-sexual-violence-un.html">Link</a>]</p>
<p>Western media concocts ‚Äòevidence‚Äô UN report on Oct 7 sex crimes failed to deliver [<a target="_blank" rel="noopener" href="https://thegrayzone.com/2024/03/07/media-concocts-un-hamas-rape-report/">Link</a>]</p>
</blockquote>
<p>It‚Äôs crazy that what is happening right now in some of the colleges is not to protest sexual violence as a tool of war by Hamas. This kind of ignorance or denial of sexual violence is horrible. People are so polarized to black and white that if something does not fit into their view, they are going to reject it. There are more than two sides to the Middle East story, one of them is sexual violence - mass rape, genital mutilation of men and women, women tied to trees naked bloody leg spread‚Ä¶</p>
<p>There is a long history of the involvement of women‚Äôs bodies in Wars. It‚Äôs only 30 years ago, people started to say rape is not a tool of War and should be prosecuted as a war crime against humanity. The feminist, human rights, and civil rights groups made this happen. Now it happened again in Gaza according to the report released by U.N., however there are a lot difficulties in proving and testifying the truth e.g. they couldn‚Äôt locate a single victim, or they don‚Äôt have the victim rights to take pictures. But victims are dead and they cannot speak up. Denying the fact of sexual violence is just unacceptable. And there is such a great <a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=zAr9oGSXgak&ab_channel=ScreamsBeforeSilence">documentary</a> shedding lights on the unspeakable sexual violence committed on Oct 7, 2023 that I think everyone should watch.</p>
<p>Good news is that the testimony of eyewitness meets the criteria of any international or global court. So crimes can be proven by any eyewitness for sure.</p>
<blockquote>
<p><strong>John Schulman - Reinforcement Learning from Human Feedback: Progress and Challenges</strong> [<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=hhiLw5Q_UFg&ab_channel=BerkeleyEECS">Link</a>]</p>
</blockquote>
<p>John Schulman is a research scientist and cofounder of OpenAI, focusing on Reinforcement Learning (RL) algorithms. He gave a talk on making AI more truthful on Apr 24, 2023 in UCB. The ideas and discussions are still helpful and insightful today.</p>
<p>In this talk, John discussed the issue of hallucination with large language models. He claims that behavior cloning or supervised learning is not enough to fix the hallucination problem, instead, reinforcement learning from human feedback (RLHF) can help improve the model‚Äôs truthfulness by 1) adjusting output distribution so model is allowed to express uncertainty, challenge premise, admit error, and 2) learning behavior boundaries. In his conceptual model, fine-tuning leads the model to hallucinate when it lacks knowledge. Retrieval and citing external sources can help improve verifiability. John discusses models that can browse the web to answer technical questions, citing relevant sources. </p>
<p>John mentioned three open problems in LLM: 1) how to train models to express uncertainty in natural language, 2) go beyond what human labelers can easily verify (‚Äúscalable oversight‚Äù), and 3) optimizing for true knowledge rather than human approval.</p>
<blockquote>
<p><strong>The 1-Year Old AI Startup That‚Äôs Rivaling OpenAI ‚Äî  Redpoint‚Äôs AI Podcast</strong> [<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=_N2KPEdh69s&ab_channel=UnsupervisedLearning:Redpoint%27sAIPodcast">Link</a>]</p>
</blockquote>
<p>A great interview with the CEO of Mistral Arthur Mensch on the topic of sovereignty and open models as a business strategy. Here are some highlighted points from Arthur:</p>
<ol>
<li>Open-source is going to solidify in the future. It is an infrastructure technology and at the end of the day it should be modifiable and owned by customers. Now Mistral has two offerings, open source one and commercial one, and the aim is to find out the business model to sustain the open source development.</li>
<li>The things that Mistral is best at 1) training model, and 2) specializing models. </li>
<li>The way they think about partnership strategy is to look at what enterprises would need, where they were operating, where the developers were operating, and figure out the channels that would facilitate adoption and spread. To be a multiplatform solution and to replicate the solution to different platforms is a strategy that Mistral is following.</li>
<li>There is still an efficiency upper bound to be pushed. Other than compute to spend on pre-training, there is still research to do on improving model efficiency and strength. On architecture side, we can be more efficient than plain Transformer which spends same amount of compute on every token. Mistral is making model faster. By making model faster, we open up a lot of applications that involve an LLM as a basic brick and then we can figure out how to do planning, explorations, etc. By increasing efficiency, we open up areas of research.</li>
<li>Meta has more GPUs than Mistral do. But Mistral has a good concentration of GPU (number of GPU per person). This is the way to be as efficient as possible to come up with creative ways of training models. Also unit economics need to be considered to make sure that $1 that you spend on training compute eventually accrues to more than $1 revenue.</li>
<li>Transformer is not an optimal architecture. It‚Äôs been out there for 7 years now. Everything is co-adapted to it such as training methods, debug methods, the algorithms, and hardware. It‚Äôs challenging to find a better one and also beat the baseline. But there are a lot of research on modification of attention to boost memory efficiencies and a lot of things can be done in that direction and similar directions.</li>
<li>About AI regulations and EU AI Act, Arthur states that it does not solve the actual problem of how to make AI safe. Because making AI safe is a hard problem (stochastic model), different from the way we evaluate software before. It‚Äôs more like a product problem rather than a regulation problem. We need to rethink continuous integration, verifications, etc and make sure everything is happening as it should be.</li>
<li>Mistral recently released Le Chat to help enterprise start incorporating AI. It gives an assistant that is contextualized on their enterprise data. It‚Äôs a tool to be closer to the end user to get feedback for the developer platform and also a tool to get the enterprise into GenAI.</li>
</ol>
<blockquote>
<p><strong>Open Source AI is AI we can Trust ‚Äî with Soumith Chintala of Meta AI</strong> [<a target="_blank" rel="noopener" href="https://www.latent.space/p/soumith">Link</a>]</p>
</blockquote>
<p>Synthetic data is the next rage of LLM. Soumith pointed out that synthetic data is where we as humans already have good symbolic models off, we need to impart that knowledge to neural networks, and we figured out the synthetic data is a vehicle to impart this knowledge to it. Related to synthetic data but in an unusual way, there is new research on distilling GPT-4 by creating synthetic data from GPT-4, creating mock textbooks inspired by Phi-2 and then fine tuning open source models like Lambda.  </p>
<p>Open source means different things to different people and we haven‚Äôt had a community norm definition yet at this very early stage of LLM. When being asked about open source, people in this field are used to highlight the definition of it in advance. In the open source topic, Soumith pointed out that the most beneficial value of open is it makes the distribution very wide and available with no friction so that people can do transformative things in a way that is very accessible. </p>
<blockquote>
<p><strong>Berkshire Hathaway 2024 Annual Meeting Movie: Tribute to Charlie Munger</strong> [<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=uGrf5PRFSJY&ab_channel=TheCapitalist">Link</a>]</p>
</blockquote>
<p>First year that the annual meeting movie is made public. First year that the annual meeting is without Charlie. Already started to miss his jokes.</p>
<blockquote>
<p><em>I think the reason why the car could have been completely reimagined by Apple is that they have a level of credibility and trust that I think probably no other company has, and absolutely no other tech company has. I think this was the third Steve Jobs story that I left out but in 2001, I launched a 99 cent download store and Steve Jobs just ran total circles around us, but the reason he was able to is he had all the credibility to go to the labels and get deals done for licensing music that nobody could get done before. I think that is an example of what Apple‚Äôs able to do which is to use their political capital to change the rules. So if the thing that we could all want is safer roads and autonomous vehicles, there are regions in every town and city that could be completely converted to level 5 autonomous zones. If I had to pick one company that had the credibility to go and change those rules, it‚Äôs them. Because they could demonstrate that there was a methodical safe approach to doing something. So the point is that even in these categories that could be totally reimagined, it‚Äôs not for a lack of imagination, again it just goes back to a complete lack of will. I understand because if you had 200B dollars of capital on your balance sheet, I think it‚Äôs probably easy to get fat and lazy. - Chamath Palihapitiya</em> </p>
<p><strong>‚Äï  In conversation with Sam Altman ‚Äî All-In Podcast</strong> [<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=nSM0xd8xHUM&ab_channel=All-InPodcast">Link</a>]</p>
</blockquote>
<blockquote>
<p><em>If you are a developer, the key thing to understand is where does model innovation end and your innovation begin, because if you get that wrong you will end up doing a bunch of stuff that the model will just obsolete in a few months. - David Sacks</em></p>
<p><em>The incentive for these folks is going to be push this stuff into the open source. Because if you solve a problem that‚Äôs operationally necessary for your business but it isn‚Äôt the core part of your business, what incentive do you have to really keep investing in this for the next 5 to 10 years to improve it. You are much better off release it in the open source, let the rest of the community take it over so that it‚Äôs available to everybody else, otherwise you are going to be stuck supporting it, and then if and when you ever wanted to switch out a model, GPT-4o, Claude, Llama, it‚Äôs going to be costly. The incentive to just push towards open source in this market if you will is so much meaningful than any other market. - Chamath Palihapitiya</em></p>
<p><em>I think the other thing that is probably true is a big measure at Google on the search page in terms of search engineer performance was the bounceback rate, meaning someone does a search, they go off to another site and they come back because they didn‚Äôt get the answer they wanted. Then one box launched which shows a short answer on the top, which basically keeps people from having a bad search experience, because they get the result right away. So a key metric is they are going to start to discover which vertical searches will provide the user a better experience than them jumping off to a third party page to get the same content. And then they will be able to monetize that content that they otherwise were not participating in the monetization of. So I think the real victim in all this is that long tale of content on the internet that probably gets cannibalized by the snippet one box experience within the search function. And then I do think that the revenue per search query in some of those categories actually has the potential to go up not down. You keep people on the page so you get more search volume there, you get more searches because of the examples you gave. And then when people do stay, you now have the ability to better monetize that particular search query, because you otherwise would have lost it to the third party content page. Keeping more of the experience integrated they could monetize the search per query higher and they are going to have more queries, and then they are going to have the quality of the queries go up. Going back to our earlier point about precision vs accuracy, my guess is there‚Äôs a lot of hedge fund type folks doing a lot of this Precision type of analysis trying to break apart search queries by vertical and try to figure out what the net effect will be of having better AI driven box and snippets. And my guess is that is why there is a lot of buying activity happening. I can tell you Meta and Amazon do not have an Isomorphic Lab and Waymo sitting inside their business, that suddenly pops to a couple hundred billion of market cap and Google does have a few of those. - David Friedberg</em></p>
<p><em>One thing I would say about big companies like Google or Microsoft is that the power of your monopoly determines how many mistakes you get to make. So think about Microsoft completely missed iPhone, remember they screwed up the whole smartphone era and it didn‚Äôt matter. Same thing here with Google, they completely screwed up AI. They invented the Transformer, completely missed LLMs. Then they had that fiasco where they have black George Washington. It doesn‚Äôt matter, they can make 10 mistakes but their monopoly is so strong, that they can finally get it right by copying the innovator, and they are probably going to be come 5T dollar company.  - David Sacks</em></p>
<p><strong>‚Äï  GPT-4o launches, Glue demo, Ohalo breakthrough, Druckenmiller‚Äôs bet, did Google kill Perplexity? ‚Äî All-In Podcast</strong> [<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=vDr1983LIuo&t=547s&ab_channel=All-InPodcast">Link</a>] </p>
</blockquote>
<p>Great conversations and insightful discussions as usual. Love it.</p>
<blockquote>
<p><em>When you are over earning so massively, the rational thing to do for other actors in the arena is to come and attack that margin, and give it to people for slightly cheaper slightly faster slightly better so you can take share. So I think what you‚Äôre seeing and what you will see even more now is this incentive for Silicon Valley who has been really reticent to put money into chips, really reticent to put money into hardware. They are going to get pulled into investing this space because there is no choice. - Chamath Palihapitiya</em></p>
<p><em>Why? It‚Äôs not that intel was a worse company, but it‚Äôs that everything else caught up. And the economic value went to things that sat above them in the stack, then it want to Cisco for a while right, then after Cisco, it went to the browser companies for a little bit, then it went to the app companies, then it went to the device companies, then it went to the mobile companies. So you see this natural tendency for value to push up the stack over time. For AI, we‚Äôve done the step one which is now you‚Äôve given all this value to NVIDIA and now we are going to see it being reallocated. - Chamath Palihapitiya</em></p>
<p><em>The reason why they are asking these questions is that if you go back to the doom dot come boom in 1999, you can see that Cisco had this incredible run. And if you overlay the stock price of Nvidia, it seems to be following that same trajectory. And what happened with Cisco is that when the doc come crash came in 2000, Cisco stock lost a huge part of its value. Obviously Cisco is still around today and it‚Äôs a valuable company, but it just hasn‚Äôt ever regained the type of market cap it had. The reason this happened is because Cisco got commoditized. So the success and market cap of that company attracted a whole bunch of new entrance and they copied Cisco‚Äôs products until they were total commodities. So the question is whether that happened to Nvidia. I think the difference here is that at the end of the day Network equipment which Cisco produced was pretty easy to copy, whereas if you look at Nvidia, these GPU cores are really complicated to make. So it‚Äôs a much more complicated product to copy. And then on top of that, they are already in the R&amp;D cycle for the next chip. So I think you can make the case that Nvidia has a much better moat than Cisco. - David Sacks</em></p>
<p><em>I think Nvidia is going to get pulled into competing directly with the hyperscalers. So if you were just selling chips, you probably wouldn‚Äôt, but these are big bulky actual machines, then all of a sudden you are like well why don‚Äôt I just create my own physical plant and just stack these things, and create racks and racks of these machines. It‚Äôs not a far stretch especially because Nvidia actually has the software interface that everybody uses which is CUDA. I think it‚Äôs likely that Nvidia goes on a full frontal assault against GCP and Amazon and Microsoft. That‚Äôs going to really complicate the relationship that those folks have with each other, but I think it‚Äôs inevitable because how do you defend an enormously large market cap, you are forced to go into businesses that are equally lucrative. Now if I look inside of compute and look at the adjacent categories, they are not going to all of a sudden start a competitor to TikTok or a social network, but if you look at the multi hundred billion revenue businesses that are adjacent to the markets that Nvidia enables, the most obvious ones are the hyperscalers. So they are going to be forced to compete otherwise their market cap will shrink and I don‚Äôt think they want that, and then it‚Äôs going to create a very complicated set of incentives for Microsoft and Google and Meta and Apple and all the rest. And that‚Äôs also going to be an accelerant, they are going to pump so much money to help all of these upstarts.  - Chamath Palihapitiya</em></p>
<p><em>Economy is bad without recognizing that it is an inflationary experience whereas economists use the definition of ‚Äúeconomic growth‚Äù being gross product, and so if gross product or gross revenue is going up they are like oh the economy is healthy we are growing. But the truth is we are funding that growth with leverage at the national level the federal level and at the household a domestic level. We are borrowing money to inflate the revenue numbers , and so the GDP goes up but the debt is going higher, and so the ability for folks to support themselves and buy things that they want to buy and continue to improve their condition in life has declined if things are getting worse‚Ä¶ The average American‚Äôs ability to improve their condition has largely been driven by their ability to borrow not by their earnings. - David Friedberg</em></p>
<p><strong>Scarlett Johansson vs OpenAI, Nvidia‚Äôs trillion-dollar problem, a vibecession, plastic in our balls</strong> [<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=c9HEjdjyVn4&ab_channel=All-InPodcast">Link</a>]</p>
</blockquote>
<p>It‚Äôs a fun session and it made my day :). Great discussions about Nvidia‚Äôs business, America‚Äôs negative economic sentiment, harm of plastics, etc.</p>
<blockquote>
<p><strong>Building with OpenAI What‚Äôs Ahead</strong> [<a target="_blank" rel="noopener" href="https://vimeo.com/949419199">Link</a>]</p>
</blockquote>
<h3 id="Papers-and-Reports"><a href="#Papers-and-Reports" class="headerlink" title="Papers and Reports"></a>Papers and Reports</h3><blockquote>
<p><strong>Large Language Models: A Survey</strong> [<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2402.06196">Link</a>]</p>
</blockquote>
<p>This is a must-read paper if you would like to have a comprehensive overview of SOTA LLMs, technical details, applications, datasets, benchmarks, challenges, and future directions.</p>
<blockquote>
<p><strong>Little Guide to Building Large Language Models in 2024 - HuggingFace</strong> [<a target="_blank" rel="noopener" href="https://docs.google.com/presentation/d/1IkzESdOwdmwvPxIELYJi8--K3EZ98_cL6c5ZcLKSyVg/edit?usp=sharing">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Are ChatGPT and GPT-4 General-Purpose Solvers for Financial Text Analytics? A Study on Several Typical Tasks</strong> [<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2305.05862">Link</a>]</p>
</blockquote>
<p>Bloomberg fine-tuned GPT-3.5 on their financial data only to find that GPT-4 8k, without specialized finance fine-tuning, beat it on almost all finance tasks. So there is really a moat? Number of parameters matters and data size matters, and they all require compute and money.</p>
<blockquote>
<p><strong>Jamba: A Hybrid Transformer-Mamba Language Model</strong> [<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2403.19887">Link</a>] [<a target="_blank" rel="noopener" href="https://www.ai21.com/blog/announcing-jamba">Link</a>]</p>
</blockquote>
<p><a target="_blank" rel="noopener" href="https://openreview.net/forum?id=AL1fq05o7H">Mamba paper</a> has been rejected while fruits are reaped fast: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2401.04081">MoE-Mamba</a>, <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2401.09417">Vision Mamba</a>, and Jamba. It‚Äôs funny to see the asymmetric impact in ML sometimes, e.g. FlashAttention has &lt;500 citations and is used everywhere. Github repos used by 10k+ has &lt;100 citations, etc.</p>
<blockquote>
<p><strong>KAN: Kolmogorov-Arnold Networks</strong> [<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.19756">Link</a>] [<a target="_blank" rel="noopener" href="https://github.com/KindXiaoming/pykan?tab=readme-ov-file#authors-note">authors-note</a>]</p>
</blockquote>
<p>This is a mathematically beautiful idea. The main difference between traditional MLP and KAN is that KAN has learnable activation function on weights, so all weights in KAN are non-linear. KAN outperforms MLP in accuracy and interpretability. Whether in the future KAN is able to replace MLP depends on whether there could be suitable learning algorithms like SGD, AdamW, etc and whether it will be GPU friendly.</p>
<blockquote>
<p><strong>The Platonic Representation Hypothesis</strong> [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2405.07987">Link</a>]</p>
</blockquote>
<p>Interesting paper to read if you like philosophy. This paper argues that there is a platonic representation as a result of convergence of AI models towards a shared statistical model of reality. They show that there is a growing similarity in data representation across different model architectures, training objectives, and data modalities, as the model size, data size, and task diversity are growing. They also proposed three hypothesis for the representation convergence: 1) The multitask scaling hypothesis, 2) The capacity hypothesis, and 3) The simplicity bias hypothesis. And it definitely worths reading the counterexamples and limitations. </p>
<blockquote>
<p><strong>Frontier Safety Framework - Google DeepMind</strong> [<a target="_blank" rel="noopener" href="https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/introducing-the-frontier-safety-framework/fsf-technical-report.pdf">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model</strong> [<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.04434">Link</a>] </p>
</blockquote>
<p>One main improvement: Multi-head latent attention via compressed latent KV requires smaller amount of KV cache per token but achieves stronger performance. Heads can be compressed differently (taking different portion of compressed latent states), and keys and values can be compressed differently.</p>
<blockquote>
<p><strong>What matters when building vision-language models</strong> [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2405.02246">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>The Unreasonable Ineffectiveness of the Deeper Layers</strong> [<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2403.17887">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>RecurrentGemma: Moving Past Transformers for Efficient Open Language Models</strong> [<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.07839">Link</a>]</p>
</blockquote>
<p>This paper published by Google DeepMind proposes language model called <a target="_blank" rel="noopener" href="https://ai.google.dev/gemma/docs/recurrentgemma">RecurrentGemma</a> that can match or exceed the performance of transformer-based models while being more memory efficient.</p>
<blockquote>
<p><strong>Towards Responsible Development of Generative AI for Education: An Evaluation-Driven Approach - Google‚Äôs Tech Report of LearnLM</strong> [<a target="_blank" rel="noopener" href="https://storage.googleapis.com/deepmind-media/LearnLM/LearnLM_paper.pdf">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Chameleon: Mixed-Modal Early-Fusion Foundation Models</strong> [<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.09818">Link</a>]</p>
</blockquote>
<p>This paper published by Meta proposed a mixed model which uses Transformer architecture under the covers but applies some innovations such as query-key normalization to fix the imbalance between the text and image tokens and other innovations as well.</p>
<blockquote>
<p><strong>Simple and Scalable Strategies to Continually Pre-train Large Language Models</strong> [[Link](<a target="_blank" rel="noopener" href="https://arxiv.org/">https://arxiv.org/</a> pdf&#x2F;2403.08763)]</p>
</blockquote>
<p>Tricks for successful continued pretraining:</p>
<ol>
<li>ÔªøÔªøÔªøRe-warming and re-decaying the learning rate.</li>
<li>ÔªøÔªøÔªøAdding a small portion (e.g., 5%) of the original pretraining data (D1) to the new dataset (D2) to prevent catastrophic forgetting.<br> Note that smaller fractions like 0.5% and 1% were also effective.</li>
</ol>
<p>Cautious about their validity on model with larger sizes.</p>
<blockquote>
<p><strong>Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study</strong> [<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.10719">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Algorithmic Progress in Language Models</strong> [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2403.05812">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Physics of Language Models: Part 3.3, Knowledge Capacity Scaling Laws</strong> [<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.05405">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Efficient Multimodal Large Language Models: A Survey</strong> [<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.10739v1">Link</a>]</p>
</blockquote>
<p>Good overview of multimodal LLMs.</p>
<blockquote>
<p><strong>Financial Statement Analysis with Large Language Models</strong> [<a target="_blank" rel="noopener" href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4835311">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>LoRA Learns Less and Forgets Less</strong> [<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.09673">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Lessons from the Trenches on Reproducible Evaluation of Language Models</strong> [<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.14782">Link</a>]</p>
</blockquote>
<p>Challenges and best practices in evaluating LLMs.</p>
<blockquote>
<p><strong>Agent Planning with World Knowledge Model</strong>  [<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.14205">Link</a>]</p>
</blockquote>
<h3 id="GitHub-Repo"><a href="#GitHub-Repo" class="headerlink" title="GitHub Repo"></a>GitHub Repo</h3><blockquote>
<p><strong>Google Research Tune Playbook - GitHub</strong> [<a target="_blank" rel="noopener" href="https://github.com/google-research/tuning_playbook">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>ML Engineering - GitHub</strong>  [<a target="_blank" rel="noopener" href="https://github.com/stas00/ml-engineering">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>LLM from Scratch</strong> [<a target="_blank" rel="noopener" href="https://github.com/rasbt/LLMs-from-scratch/tree/main">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Prompt Engineering Guide</strong> [<a target="_blank" rel="noopener" href="https://github.com/dair-ai/Prompt-Engineering-Guide">Link</a>] [<a target="_blank" rel="noopener" href="https://www.promptingguide.ai/">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>ChatML + chat templates + Mistral v3 7b full example</strong> [<a target="_blank" rel="noopener" href="https://colab.research.google.com/drive/15F1xyn8497_dUbxZP4zWmPZ3PJx1Oymv?usp=sharing">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Finetune pythia 70M</strong> [<a target="_blank" rel="noopener" href="https://colab.research.google.com/gist/virattt/af36fd12480827da3f8427169b3348cb/finetuning-pythia-70m.ipynb">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Llama3 Implemented from Scratch</strong> [<a target="_blank" rel="noopener" href="https://github.com/naklecha/llama3-from-scratch">Link</a>]</p>
</blockquote>
<h3 id="News"><a href="#News" class="headerlink" title="News"></a>News</h3><blockquote>
<p><strong>Intel Inside Ohio</strong> [<a target="_blank" rel="noopener" href="https://www.bloomberg.com/features/2024-intel-comeback-chipmaking/">Link</a>]</p>
<p>Intel Ohio One Campus Video Rendering [<a target="_blank" rel="noopener" href="https://vimeo.com/939198943">Link</a>]</p>
</blockquote>
<p>Intel Corp has committed $28B to build a ‚Äúmega fab‚Äù called Ohio One which could be the biggest chip factory on Earth. The Biden administration has agreed to provide Intel with $19.5B in loans and grants to support finance the project.</p>
<blockquote>
<p><strong>EveryONE Medicines: Designing Drugs for Rare Diseases, One at a Time</strong> [<a target="_blank" rel="noopener" href="https://www.wsj.com/articles/everyone-medicines-designing-drugs-for-rare-diseases-one-at-a-time-a6f98afc">Link</a>]</p>
</blockquote>
<p>Startup EveryONE Medicine aims to develop drugs designed based on genetic information for individual children who have rare, life-threatening neurological diseases. Since the number of patients with diseases caused by rare mutation is significant, the market share is large if EveryONE can scale its process. Although the cost won‚Äôt be the same as a standard drugmaker that runs large clinical trials, the challenge is safety without a standard clinical-testing protocol. To be responsible to patients, the initial drugs will have a temporary effect and a wide therapeutic window, so the potential toxicity will be minimized or stopped if there is.</p>
<blockquote>
<p><strong>Voyager 1‚Äôs Communication Malfunctions May Show the Spacecraft‚Äôs Age</strong> [<a target="_blank" rel="noopener" href="https://www.discovermagazine.com/the-sciences/voyager-1s-communication-malfunctions-may-show-the-spacecrafts-age">Link</a>]</p>
</blockquote>
<p>In Nov 2023, NASA‚Äôs over 46-year-old Voyager 1 spacecraft started sending nonsense to Earth. Voyager 1 was initially intended to study Jupiter and Saturn and was built to survive only 5 years of flight, however the trajectory was forged further and further into space and so the mission converted from a two-planet mission to an interstellar mission.</p>
<p>In Dec 2023, the mission team restarted the Flight Data Subsystem (FDS)  but failed to return the subsystem to functional state. On Mar 1 2023, they sent a command ‚Äúpoke‚Äù to the probe and received a response on Mar 3. On Mar 10, the mission team finally determined the response carried a readout of FDS memory. By comparing the readout with those received before the issue, the team confirmed that 3% of FDS memory was corrupted. On Apr 4, the team concluded the affected code was contained on a computer chip. To solve the problem, the team decided to divide these affected code into smaller sections and to insert those smaller sections into other operative places in the FDS memory. During Apr 18-20, the team sent out the orders to move some of the affected code and received responses with intelligible systems information. </p>
<blockquote>
<p><strong>Editing the Human Genome with AI</strong> [<a target="_blank" rel="noopener" href="https://www.profluent.bio/blog/editing-the-human-genome-with-ai">Link</a>]</p>
</blockquote>
<p>Berkeley based startup Profluent Bio used an AI based protein language model to create and train on an entirely new library of Cas proteins that do not exist in nature today and eventually find one called ‚ÄòOpenCRISPR-1‚Äô that is able to replace or improve the ones that are on the market today. The goal of this AI model is to learn what sequence of DNA generated what structure of protein that‚Äôs really good at gene editing. The new library of Cas proteins is created by simulation of trillions of letters. They made ‚ÄòOpenCRISPR-1‚Äô publicly available under an open source license so anyone can use this particular Cas protein.</p>
<blockquote>
<p><strong>Sony and Apollo in Talks to Acquire Paramount</strong> [<a target="_blank" rel="noopener" href="https://www.nytimes.com/2024/05/05/business/media/sony-apollo-paramount.html">Link</a>]</p>
</blockquote>
<p>Paramount‚Äôs stock declined 44% in 2022 and another 12% in 2023. It‚Äôs experiencing declining revenue as consumers abandon traditional pay-TV and it‚Äôs losing streaming business. Berkshire sold its entire Paramount shares in March 2023 and soon Sony Pictures and Apollo Globals Management reached out to Paramount board expressing interest of acquisition. Now Paramount decided to open negotiation with them after exclusive talks with Hollywood studio Skydance. This deal would break the Paramount and potentially transform the media landscape if successful. Otherwise an office of the CEO as the replacement of CEO Bob Bakish will be preparing a long term plan for the company.</p>
<blockquote>
<p><strong>AlphaFold 3 predicts the structure and interactions of all of life‚Äôs molecules</strong> [<a target="_blank" rel="noopener" href="https://blog.google/technology/ai/google-deepmind-isomorphic-alphafold-3-ai-model/">Link</a>]</p>
</blockquote>
<p>Previously, Google DeepMind AlphaFold project took 3D images of proteins and the DNA sequence that codes for those proteins and then they built a predictive model that predicted the 3D structure of protein base on DNA sequence. What is difference in AlphaFold 3 is that all small molecules are included. The way how small molecules are bind together with the protein is part of the predictive model. This is a breakthrough in that off target effect could be minimized by taking consideration of other molecules‚Äô interactions in the biochemistry environment. Google has a drug development subsidiary called Isomorphic Labs. They kept all of IP for AlphaFold 3. They published a web viewer for non-commercial scientists to do fundamental research but only Isomorphic Labs can make it for commercial use. </p>
<blockquote>
<p><strong>Introducing GPT-4o and making more capabilities available for free in ChatGPT</strong> [<a target="_blank" rel="noopener" href="https://openai.com/index/spring-update/">Link</a>]</p>
</blockquote>
<p>I missed the live announcement but watched the recording. GPT-4o is amazing.</p>
<p>One of the interesting technical difference made is tokenizer delta. GPT-4 and GPT-4-Turbo both had a tokenizer with a vocabulary of 100k tokens. GPT-4o has a tokenizer with 200k tokens to work better for native multimodality and multilingualism. The more tokens the more efficient in generating characters.</p>
<blockquote>
<p><em>‚ÄúOur goal is to make it effortless for people to go anywhere and get anything,‚Äù said Dara Khosrowshahi, CEO of Uber. ‚ÄúWe‚Äôre excited that this new strategic partnership with Instacart will bring the magic of Uber Eats to even more consumers, drive more business for restaurants, and create more earnings opportunities for couriers.‚Äù</em></p>
<h5 id="‚Äï-Uber-Eats-to-Power-Restaurant-Delivery-on-Instacart-Link"><a href="#‚Äï-Uber-Eats-to-Power-Restaurant-Delivery-on-Instacart-Link" class="headerlink" title="‚Äï  Uber Eats to Power Restaurant Delivery on Instacart [Link]"></a>‚Äï  Uber Eats to Power Restaurant Delivery on Instacart [<a target="_blank" rel="noopener" href="https://investor.uber.com/news-events/news/press-release-details/2024/Uber-Eats-to-Power-Restaurant-Delivery-on-Instacart/">Link</a>]</h5></blockquote>
<blockquote>
<p><strong>Project Astra: Our vision for the future of AI assistants</strong> [<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=nXVvvRhiGjI&ab_channel=Google">Link</a>]</p>
<p><strong>Google Keynote (Google I&#x2F;O 24‚Äô)</strong> [<a target="_blank" rel="noopener" href="https://io.google/2024/explore/a6eb8619-5c2e-4671-84cb-b938c27103be/">Link</a>]</p>
</blockquote>
<p>This developer conference is about Google‚Äôs AI related product updates. Highlighted features: 1) AI Overview for search 2) Ask Photos, 3) 2M context window, 4) Google Workspace, 5) NotebookLM, 6) Project Astra, 7) Imagen 3, 8) Music AI Sandbox, 9) Veo, 10) Trillium TPU, 11) Google Serach, 12) Asking Questions with Videos, 13) Gemini interacting with Gmail and data, 14) Gemini AI Teammate, 15) Gemini App, and upgrades, 16) Gemini Trip Planning.</p>
<blockquote>
<p><em>Leike went public with some reasons for his resignation on Friday morning. ‚ÄúI have been disagreeing with OpenAI leadership about the company‚Äôs core priorities for quite some time, until we finally reached a breaking point,‚Äù Leike wrote in a series of posts on X. ‚ÄúI believe much more of our bandwidth should be spent getting ready for the next generations of models, on security, monitoring, preparedness, safety, adversarial robustness, (super)alignment, confidentiality, societal impact, and related topics. These problems are quite hard to get right, and I am concerned we aren‚Äôt on a trajectory to get there.‚Äù</em></p>
<p><strong>‚Äï  OpenAI created a team to control ‚Äòsuperintelligent‚Äô AI ‚Äî then let it wither, source says</strong> [<a target="_blank" rel="noopener" href="https://techcrunch.com/2024/05/18/openai-created-a-team-to-control-superintelligent-ai-then-let-it-wither-source-says/">Link</a>]</p>
</blockquote>
<blockquote>
<p>Other News:</p>
<p><strong>Encampment Protesters Set Monday Deadline for Harvard to Begin Negotiations</strong> [<a target="_blank" rel="noopener" href="https://www.thecrimson.com/article/2024/5/3/yard-encampment-negotiation-deadline/">Link</a>]</p>
<p><strong>Israel Gaza war: History of the conflict explained</strong> [<a target="_blank" rel="noopener" href="https://www.bbc.com/news/newsbeat-44124396">Link</a>]</p>
<p><strong>Cyber Stuck: First Tesla Cybertruck On Nantucket Has A Rough Day</strong>  [<a target="_blank" rel="noopener" href="https://nantucketcurrent.com/news/cyber-stuck-first-tesla-cybertruck-on-nantucket-has-a-rough-day">Link</a>]</p>
<p><strong>Apple apologizes after ad backlash</strong> [<a target="_blank" rel="noopener" href="https://www.linkedin.com/news/story/apple-apologizes-after-ad-backlash-6024884/">Link</a>]</p>
<p><strong>Apple nears deal with OpenAI to put ChatGPT on iPhone: Report</strong> [<a target="_blank" rel="noopener" href="https://www.businesstoday.in/technology/news/story/apple-nears-deal-with-openai-to-put-chatgpt-on-iphone-report-429178-2024-05-11">Link</a>] [<a target="_blank" rel="noopener" href="https://www.bloomberg.com/news/articles/2024-05-11/apple-closes-in-on-deal-with-openai-to-put-chatgpt-on-iphone">Link</a>]</p>
<p><strong>Reddit announces another big data-sharing AI deal ‚Äî this time with OpenAI</strong> [<a target="_blank" rel="noopener" href="https://www.businessinsider.com/reddit-openai-deal-ai-data-partnership-2024-5">Link</a>]</p>
<p><strong>Apple Will Revamp Siri to Catch Up to Its Chatbot Competitors</strong> [<a target="_blank" rel="noopener" href="https://www.nytimes.com/2024/05/10/business/apple-siri-ai-chatgpt.html?unlocked_article_code=1.q00.NlEa.TZR9DORdS4C-&smid=url-share">Link</a>]</p>
<p><strong>OpenAI strikes deal to bring Reddit content to ChatGPT</strong> [<a target="_blank" rel="noopener" href="https://www.reuters.com/markets/deals/openai-strikes-deal-bring-reddit-content-chatgpt-2024-05-16/">Link</a>]</p>
</blockquote>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://jokerdii.github.io/digital-di/2024/05/11/Is-AI-a-Bubble/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/digital-di/images/avatar.gif">
      <meta itemprop="name" content="Di Zhen">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Di's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Di's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/digital-di/2024/05/11/Is-AI-a-Bubble/" class="post-title-link" itemprop="url">Is AI a Bubble?</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2024-05-11 23:54:35" itemprop="dateCreated datePublished" datetime="2024-05-11T23:54:35-04:00">2024-05-11</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-05-12 15:05:09" itemprop="dateModified" datetime="2024-05-12T15:05:09-04:00">2024-05-12</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p><em>Random words:</em></p>
<p><em>Feeling deep loneliness on New York City‚Äôs bustling streets. When I was in small town, I‚Äôve never said goodbye such frequently.</em></p>
<p>Back to the topic:</p>
<p>When we talk about investment, we talk about economic values. Current situation of AI is very similar to Cisco‚Äôs in 2000. Cisco as an internet company spread the capacity of the World Wide Web, but sooner people realized that there is no economic value in internet company, instead, opportunities are in e-commerce etc. AI is a tool very similar to web tech. Currently, with heightened expectations, people are allocating investments and capital expenditure in AI model development, however, end-user demand is unclear and revenue is relatively minimal. This situation makes AI look like a bubble from a very long term perspective.</p>
<p>Stepping closer to it, there is still room in the market to party. GPUs for training and inference are increasingly on demand. First round of beneficiaries are Cloud and Ad. Second round could be hardware or something else. Although it looks like a Capitalism‚Äôs scam which is getting more money to the big tech, as small open-source models are released, moats are expected to be  disintegrated and distributed. I‚Äôve seen more and more enterprises going to have Gen AI integrated to their business or operation now. Enterprise is going to be continuously transformed to be more efficient and productive, as well as human life with this long lasting attention on AI. This kind of long lasting attention and consistent innovation are something different from internet tech in 2000 and will probably create a momentum against bubble.</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://jokerdii.github.io/digital-di/2024/04/30/2024-April/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/digital-di/images/avatar.gif">
      <meta itemprop="name" content="Di Zhen">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Di's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Di's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/digital-di/2024/04/30/2024-April/" class="post-title-link" itemprop="url">2024 April</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2024-04-30 23:48:32" itemprop="dateCreated datePublished" datetime="2024-04-30T23:48:32-04:00">2024-04-30</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-05-25 10:10:02" itemprop="dateModified" datetime="2024-05-25T10:10:02-04:00">2024-05-25</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="Articles"><a href="#Articles" class="headerlink" title="Articles"></a><strong>Articles</strong></h3><blockquote>
<p><em>As Columbia Business School professor Rita McGrath points out, it‚Äôs about identifying ‚Äúthe strategic inflection points‚Äù at which the cost of waiting exceeds to cost acting ‚Äî in other words, identifying the most strategic point to enter a market or adopt a technology, balancing the risks and opportunities based on market readiness, technological maturity and organizational capacity.</em></p>
<p><em>This speaks to the growing adoption of agile, ‚Äúact, learn, build‚Äù approaches over traditional ‚Äúprove-plan-execute‚Äù orientations. The popularity of techniques like <a target="_blank" rel="noopener" href="https://hbr.org/1995/07/discovery-driven-planning">discovery-driven planning</a>, <a target="_blank" rel="noopener" href="https://hbr.org/2013/05/why-the-lean-start-up-changes-everything">the lean startup</a>, and other agile approaches and propagated this philosophy in which, rather than building bullet-proof business cases, one makes small steps, learning from them, and deciding whether to invest further.</em></p>
<p><strong>‚Äï  ‚Äú6 Strategic Concepts That Set High-Performing Companies Apart‚Äù, Harvard Business Review</strong> [<a target="_blank" rel="noopener" href="https://hbr.org/2024/03/6-strategic-concepts-that-set-high-performing-companies-apart">Article</a>]</p>
</blockquote>
<p>It‚Äôs a very good read. It provided real world business examples such as Nvidia‚Äôs partnership with ARM Holdings and Amazon‚Äôs Alexa offering for the strategic concept ‚Äúborrow someone‚Äôs road‚Äù, Microsoft‚Äôs decision to make Office available on Apple‚Äôs iOS devices in 2014 and Microsoft‚Äôs partnership with Adobe, Salesforce, and Google for the strategic concept ‚ÄúParter with a third party‚Äù, Deere &amp; Co‚Äôs decision on openly investing in precision agriculture technologies for the strategic concept ‚Äúreveal your strategy‚Äù, Mastercard‚Äôs ‚ÄúBeyond Cash‚Äù initiative in 2012 for the strategic concept ‚Äúbe good‚Äù, Ferrari‚Äôs strategic entry into the luxury SUV market for the strategic concept ‚Äúlet the competition go‚Äù, and Tesla‚Äôs modular approach to battery manufacturing for the strategic concept ‚Äúadopt small scale attacks‚Äù. </p>
<blockquote>
<p><em>Gig work is structured in a way that strengthens the alignment between customers and companies and deepens the divide between customers and workers, leading to systemic imbalances in its service triangle.</em> </p>
<p><em>Bridging the customer-worker divide can result in higher customer trust and platform commitment, both by the customer and the worker.</em> </p>
<p><em>To start, platforms need to increase transparency, reduce information asymmetry, and price their services clearly, allowing customers to better understand what they are paying for rather than only seeing an aggregated total at the end of the transaction. This, in turn, can help customers get used to the idea that if workers are to be paid fairly, gig work cannot be a free or low-cost service.</em></p>
<p><em>Gig workers might be working through an app, but they are not robots, and they deserve to be treated respectfully and thoughtfully. So tip well, rate appropriately, and work together to make the experience as smooth as possible both for yourself and for workers.</em></p>
<p><strong>‚Äï  ‚ÄúHow Gig Work Pits Customers Against Workers‚Äù, Harvard Business Review</strong> [<a target="_blank" rel="noopener" href="https://hbr.org/2024/04/how-gig-work-pits-customers-against-workers">Article</a>]</p>
</blockquote>
<p>This is a good article for better understanding how gig work structured differently than other business model, and what the key points are for better business performance and triangle relationships.</p>
<blockquote>
<p><em>TCP&#x2F;IP unlocked new economic value by dramatically lowering the cost of connections. Similarly, blockchain could dramatically reduce the cost of transactions. It has the potential to become the system of record for all transactions. If that happens, the economy will once again undergo a radical shift, as new, blockchain-based sources of influence and control emerge.</em></p>
<p><em>‚ÄúSmart contracts‚Äù may be the most transformative blockchain application at the moment. These automate payments and the transfer of currency or other assets as negotiated conditions are met. For example, a smart contract might send a payment to a supplier as soon as a shipment is delivered. A firm could signal via blockchain that a particular good has been receivedor the product could have GPS functionality, which would automatically log a location update that, in turn, triggered a payment. We‚Äôve already seen a few early experiments with such self-executing contracts in the areas of venture funding, banking, and digital rights management.</em></p>
<p><em>The implications are fascinating. Firms are built on contracts, from incorporation to buyer-supplier relationships to employee relations. If contracts are automated, then what will happen to traditional firm structures, processes, and intermediaries like lawyers and accountants? And what about managers? Their roles would all radically change. Before we get too excited here, though, let‚Äôs remember that we are decades away from the widespread adoption of smart contracts. They cannot be effective, for instance, without institutional buy-in. A tremendous degree of coordination and clarity on how smart contracts are designed, verified, implemented, and enforced will be required. We believe the institutions responsible for those daunting tasks will take a long time to evolve, And the technology challenges especially security are daunting.</em></p>
<p><strong>‚Äï  ‚ÄúThe Truth About Blockchain‚Äù, Harvard Business Review</strong> [<a target="_blank" rel="noopener" href="https://hbr.org/2017/01/the-truth-about-blockchain">Article</a>]</p>
</blockquote>
<p>This is the second Blockchain related article I have read from Harvard Business Review. Different authors have different perspectives. Unlike the previous article with a lot of concerns and cautions about Web3, this article seems more optimistic. It proposed a framework for adopting blockchain to revolutionize modern business, and a guidance to Blockchain investment. It points out that Blockchain has great potentials in boosting the efficiency and reducing the cost for all transactions and then explained the reason why the adoption of Blockchain would be slow by making a comparison with TCP&#x2F;IP, which took more than 30 years to reshape the economy by dramatically lowering the cost of connections. This is an interesting comparison: e-mail enabled bilateral messaging as the first application of TCP&#x2F;IP, while bitcoin enables bilateral financial transactions as the first application of Blockchain. It reminds me about what people (Jun Lei, Huateng Ma, Lei Ding, etc) were thinking about internet mindset and business model back in 2000s.</p>
<p>In the end, the authors proposed a four-quadrant framework for adopting Blockchain step by step. The four quadrants are created by two dimensions: novelty (equivalent to the amount of efforts required to ensure users understand the problem) and complexity (equivalent to the amount of coordination and collaboration required to produce values). With the increase of both dimensions, the adoption will require more institutional change. An example of ‚Äúlow novelty and low complexity‚Äù is simply adding bitcoin as an alternative transaction method. An example of ‚Äúlow novelty and high complexity‚Äù is building a new, fully formed cryptocurrency system which requires wide adoption from every monetary transaction party and consumers‚Äô complete understanding of cryptocurrency. An example of ‚Äúhigh novelty and low complexity‚Äù is building a local private network on which multiple organizations are connected via a distributed ledger. An example of ‚Äúhigh novelty and high complexity‚Äù is building ‚Äúsmart contracts‚Äù.</p>
<h3 id="News"><a href="#News" class="headerlink" title="News"></a><strong>News</strong></h3><blockquote>
<p><strong>Does Amazon‚Äôs cashless Just Walk Out technology rely on 1,000 workers in India?</strong> [<a target="_blank" rel="noopener" href="https://www.usatoday.com/story/money/shopping/2024/04/04/amazon-just-walk-out-indian-workers/73204975007/">Link</a>]</p>
<p><strong>Amazon insists Just Walk Out isn‚Äôt secretly run by workers watching you shop</strong> [<a target="_blank" rel="noopener" href="https://www.theverge.com/2024/4/17/24133029/amazon-just-walk-out-cashierless-ai-india">Link</a>]</p>
<p><strong>An update on Amazon‚Äôs plans for Just Walk Out and checkout-free technology</strong> [<a target="_blank" rel="noopener" href="https://www.aboutamazon.com/news/retail/amazon-just-walk-out-dash-cart-grocery-shopping-checkout-stores">Link</a>]</p>
</blockquote>
<p>It‚Äôs been reported that there are over 1000 Indian workers behind the cameras of Just Walk Out. It sounds dystopian and reminds me of ‚ÄúSnowpiercer‚Äù movie in 2013. In 2022, about 700 of every 1000 Just Walk Out sales had to be reviewed by Amazon‚Äôs team in India, according to The Information. Amazon spokesperson explained that the technology is made by AI (computer vision and deep learning) while it does rely on human moderators and data labelers. Amazon clarified that it‚Äôs not true that Just Walk Out relies on human reviewers. They said object detection and receipt generation are completely AI powered, so no human watching live videos. But human are responsible for labeling and annotation for data preparation, which also requires watching videos.</p>
<p>I guess the technology was not able to complete the task end-to-end by itself without supervision or it‚Äôs still on the developing stage? I believe it could be Amazon‚Äôs strategy to build and test Just Walk Out, Amazon Dash Cart, and Amazon One at the same time while improving AI system, since they are ‚Äújust getting started‚Äù. As Amazon found out that customers prefer Dash Cart in large stores, it has already expanded Dash Cart to all Amazon Fresh stores as well as third-party grocers. And customers prefer Just Walk Out in small stores, so it‚Äôs available now in 140+ thrid-party locations. Customers love Amazon One‚Äôs security and convenience regardless the scale of stores, so it‚Äôs now available at 500+ Whole Foods Market stores, some Amazon stores, and 150+ third-party locations.</p>
<blockquote>
<p><em>Data centres consume water directly to prevent information technology equipment from overheating. They also consume water indirectly from coal-powered electricity generation.</em></p>
<p><em>The report said that if 100 million users had a conversation with ChatGPT, the chatbot ‚Äúwould consume 50,000 cubic metres of water ‚Äì the same as 20 Olympic-sized swimming pools ‚Äì whereas the equivalent in Google searches would only consume one swimming pool‚Äù.</em></p>
<p><strong>‚Äï  China‚Äôs thirsty data centres, AI industy could use more water than size of South Korea‚Äôs population by 2030: report warns</strong> [<a target="_blank" rel="noopener" href="https://www.scmp.com/news/china/science/article/3259230/chinas-growing-data-centres-and-ai-industry-could-strain-scarce-water-resources-according-new-report?utm_source=copy-link&utm_campaign=3259230&utm_medium=share_widget">Link</a>]</p>
</blockquote>
<p>The rapid growth of AI could dramatically increase demand on water resources. AI eats tokens, consumes compute, and drinks water.</p>
<blockquote>
<p><strong>15 Graphs That Explain the State of AI in 2024</strong> [<a target="_blank" rel="noopener" href="https://spectrum.ieee.org/ai-index-2024">Link</a>]</p>
</blockquote>
<p>Stanford Institute for Human-Centered Artificial Intelligence (HAI) published 2024‚Äôs AI Index report [<a target="_blank" rel="noopener" href="https://aiindex.stanford.edu/wp-content/uploads/2024/04/HAI_AI-Index-Report-2024.pdf">Link</a>]. 502-page reading journey started.</p>
<blockquote>
<p><img src="/digital-di/./images/ins_emails1.JPG" alt="ins_emails1"></p>
<p><img src="/digital-di/./images/ins_emails2.JPG" alt="ins_emails2"></p>
<p><img src="/digital-di/./images/ins_emails3.JPG" alt="ins_emails3"></p>
<p><img src="/digital-di/./images/ins_emails4.JPG" alt="ins_emails4"></p>
<p><strong>‚Äï  Leaked emails reveal why Mark Zuckerberg bought Instagram</strong> [<a target="_blank" rel="noopener" href="https://www.cnbctv18.com/business/leaked-emails-reveal-why-mark-zuckerberg-bought-instagram-19399861.htm">Link</a>]</p>
</blockquote>
<p>Zuckerberg‚Äôs discussion of Instagram acquisition back in 2012 proved his corporate strategic foresights. He was aiming to buy the time and network effect, rather than simply neutralizing competitors or improving products. He bought Instagram for $1B, today it is worth $500B. It‚Äôs very impressive.</p>
<blockquote>
<p><strong>Introducing Meta Llama 3: The most capable openly available LLM to date</strong> [<a target="_blank" rel="noopener" href="https://ai.meta.com/blog/meta-llama-3/">Link</a>]</p>
<p><strong>Llama 3: Scaling open LLMs to AGI</strong> [<a target="_blank" rel="noopener" href="https://www.interconnects.ai/p/llama-3-and-scaling-open-llms?utm_source=profile&utm_medium=reader2">Link</a>]</p>
</blockquote>
<p>Meta released early versions of Llama 3. Pretrained and instruction-fine-tuned Llama3 with 8B and 70B parameters are now open-source. Its 405B version is still training.</p>
<p>Llama 3 introduces Grouped Query Attention (GQA), which reduces the computational complexity of processing large sequences by grouping attention queries. Llama 3 also had extensive pre-training involving over 15 trillion tokens, including a significant amount of content in different languages, enhancing its applicability across diverse linguistic contexts. Post-training techniques include finetuning and rejection sampling which refine the model‚Äôs ability to follow instructions and minimize error. </p>
<blockquote>
<p><strong>Cheaper, Better, Faster, Stronger - Continuing to push the frontier of AI and making it accessible to all.</strong> [<a target="_blank" rel="noopener" href="https://mistral.ai/news/mixtral-8x22b/">Link</a>]</p>
</blockquote>
<p>Mistral AI‚Äôs Mixtral 8x22B has a Sparse Mixture-of-Experts (SMoE) architecture, which maximize efficiency by activating only 44B out of 176B parameters. The model‚Äôs architecture ensures that only the most relevant ‚Äúexperts‚Äù are activated during specific tasks. The experts are individual neural networks as apart of SMoE model. They are trained to become proficient at particular sub-tasks out of the overall task. Since only a few experts are engaged for any given input, this design reduces computational complexity.</p>
<blockquote>
<p><strong>GPT-4 Turbo and GPT-4</strong> [<a target="_blank" rel="noopener" href="https://platform.openai.com/docs/models/gpt-4-turbo-and-gpt-4">Link</a>]</p>
</blockquote>
<p>GPT-4-Turbo has significantly enhanced its multimodal capabilities by incorporating AI vision technology. This model is able to analyze videos, images, and audios. Its tokenizer now has a larger 128000 token context window, which maximizes its memory.</p>
<blockquote>
<p><em>The race to lead A.I. has become a desperate hunt for the digital data needed to advance the technology. To obtain that data, tech companies including OpenAI, Google and Meta have cut corners, ignored corporate policies and debated bending the law, according to an examination by The New York Times.</em></p>
<p><em>Tech companies are so hungry for new data that some are developing ‚Äúsynthetic‚Äù information. This is not organic data created by humans, but text, images and code that A.I. models produce ‚Äî in other words, the systems learn from what they themselves generate.</em></p>
<p><strong>‚Äï  How Tech Giants Cut Corners to Harvest Data for A.I.</strong> [<a target="_blank" rel="noopener" href="https://www.nytimes.com/2024/04/06/technology/tech-giants-harvest-data-artificial-intelligence.html">Link</a>]</p>
</blockquote>
<p>OpenAI developed a speech recognition tool ‚ÄòWhisper‚Äô to transcribe the audio from YouTube videos, generating text data for AI system. Google employees know OpenAI had harvested YouTube videos for data but they didn‚Äôt stop OpenAI because Google had also used transcripts of YouTube videos for training AI models. Google‚Äôs rules about the legal usage of YouTube videos is vague and OpenAI‚Äôs employee were wading into a legal gray area.</p>
<p>As many tech companies such as Meta and OpenAI reached the stage of data shortage, OpenAI started to train AI models by using synthetic data synthesized by two different AI models, one produces the data, the other judges the information.</p>
<blockquote>
<p><strong>Grok-1.5 Vision Preview</strong> [<a target="_blank" rel="noopener" href="https://x.ai/blog/grok-1.5v">Link</a>]</p>
</blockquote>
<p>Musk released the preview of first multimodal model Grok-1.5V. It is able to understand both textual and visual information. One unique feature is that it adopts Rust, JAX, and Kubernetes to construct its distributed training architecture.</p>
<blockquote>
<p><em>One page of the Microsoft presentation highlights a variety of ‚Äúcommon‚Äù federal uses for OpenAI, including for defense. One bullet point under ‚ÄúAdvanced Computer Vision Training‚Äù reads: ‚ÄúBattle Management Systems: Using the DALL-E models to create images to train battle management systems.‚Äù Just as it sounds, a battle management system is a command-and-control software suite that provides military leaders with a situational overview of a combat scenario, allowing them to coordinate things like artillery fire, airstrike target identification, and troop movements. The reference to computer vision training suggests artificial images conjured by DALL-E could help Pentagon computers better ‚Äúsee‚Äù conditions on the battlefield, a particular boon for finding ‚Äî and annihilating ‚Äî targets.</em></p>
<p><em>OpenAI spokesperson Liz Bourgeous said OpenAI was not involved in the Microsoft pitch and that it had not sold any tools to the Department of Defense. ‚ÄúOpenAI‚Äôs policies prohibit the use of our tools to develop or use weapons, injure others or destroy property,‚Äù she wrote. ‚ÄúWe were not involved in this presentation and have not had conversations with U.S. defense agencies regarding the hypothetical use cases it describes.‚Äù</em></p>
<p><em>Microsoft told The Intercept that if the Pentagon used DALL-E or any other OpenAI tool through a contract with Microsoft, it would be subject to the usage policies of the latter company. Still, any use of OpenAI technology to help the Pentagon more effectively kill and destroy would be a dramatic turnaround for the company, which describes its mission as developing safety-focused artificial intelligence that can benefit all of humanity.</em></p>
<p><strong>‚Äï  Microsoft Pitched OpenAI‚Äôs DALL-E as Battlefield Tool for U.S. Military</strong>  [<a target="_blank" rel="noopener" href="https://theintercept.com/2024/04/10/microsoft-openai-dalle-ai-military-use/">Link</a>]</p>
</blockquote>
<p>Other than what has mentioned in the news, by cooperating with Department of Defense, AI can understand how human battle and defense, which is hard to learn from current textual and visual information from the internet. So it‚Äôs possible that this is the first step of AI troop.</p>
<blockquote>
<p><em>Microsoft scientists developed what they call a qubit virtualization system. This combines quantum error-correction techniques with strategies to determine which errors need to be fixed and the best way to fix them.</em></p>
<p><em>The company also developed a way to diagnose and correct qubit errors without disrupting them, a technique it calls ‚Äúactive syndrome extraction.‚Äù The act of measuring a quantum state such as superposition typically destroys it. To avoid this, active syndrome extraction instead learns details about the qubits that are related to noise, as opposed to their quantum states, Svore explains. The ability to account for this noise can permit longer and more complex quantum computations to proceed without failure, all without destroying the logical qubits.</em> </p>
<p><strong>‚Äï  Microsoft Tests New Path to Reliable Quantum Computers 1,000 physical qubits for each logical one? Try a dozen, says Redmond</strong> [<a target="_blank" rel="noopener" href="https://spectrum.ieee.org/microsoft-quantum-computer-quantinuum">Link</a>]</p>
</blockquote>
<blockquote>
<p><em>Think about it in the sense of another broad, diverse category like cars. When they were first invented, you just bought ‚Äúa car.‚Äù Then a little later, you could choose between a big car, a small car, and a tractor. Nowadays, there are hundreds of cars released every year, but you probably don‚Äôt need to be aware of even one in ten of them, because nine out of ten are not a car you need or even a car as you understand the term. Similarly, we‚Äôre moving from the big&#x2F;small&#x2F;tractor era of AI toward the proliferation era, and even AI specialists can‚Äôt keep up with and test all the models coming out.</em></p>
<p><strong>‚Äï  Too Many Models</strong> [<a target="_blank" rel="noopener" href="https://techcrunch.com/2024/04/19/too-many-models/">Link</a>]</p>
</blockquote>
<p>This week, the speed of releasing LLMs becomes about 10 per week. This article provides good explanation about why we don‚Äôt need to keep up with it or test all released models. Car is a good analogy to AI model nowadays. There are all kinds of brands and sizes, and designed for different purposes. Hundreds of cars are released every year, but you don‚Äôt need to know them. Majority of the models are not groundbreaking but whenever there is big step, you will be aware of it. </p>
<p>Although not necessary to catch up all the news, we at least need to be aware of the main future model features - where modern and future LLMs are heading to: 1) multimodality 2) recall capability 3) reasoning.</p>
<blockquote>
<p><strong>ByteDance Exploring Scenarios for Selling TikTok Without Algorithm</strong> [<a target="_blank" rel="noopener" href="https://www.theinformation.com/articles/bytedance-exploring-scenarios-for-selling-tiktok-without-algorithm">Link</a>]</p>
</blockquote>
<p>ByteDance is internally exploring scenarios for selling TikTok‚Äôs US business to non-tech industry without the algorithm if they exhausted all legal options to fight legislation of the ban. Can‚Äôt imagine who without car expertise is going to buy a car without engine. </p>
<blockquote>
<p><em>Developers and creators can take advantage of all these technologies to create mixed reality experiences. And they can reach their audiences and grow their businesses through the content discovery and monetization platforms built into Meta Horizon OS, including the Meta Quest Store, which we‚Äôll rename the Meta Horizon Store.</em> </p>
<p><strong>Introducing Our Open Mixed Reality Ecosystem</strong> [<a target="_blank" rel="noopener" href="https://about.fb.com/news/2024/04/introducing-our-open-mixed-reality-ecosystem/">Link</a>]</p>
</blockquote>
<p>Everyone knows how smart Zuck is in the idea of open-source.</p>
<blockquote>
<p>Other news:</p>
<p><strong>Elon Musk says Tesla will reveal its robotaxi on August 8th</strong> [<a target="_blank" rel="noopener" href="https://www.theverge.com/2024/4/5/24122384/tesla-robotaxi-reveal-date-elon-musk-august-8">Link</a>]</p>
<p><strong>SpaceX launches Starlink satellites on record 20th reflight of a Falcon 9 rocket first stage</strong> [<a target="_blank" rel="noopener" href="https://www.space.com/spacex-falcon-9-20th-launch-starlink-group-6-49">Link</a>]</p>
<p><strong>Deploy your Chatbot on Databricks AI with RAG, DBRX Instruct, Vector Search &amp; Databricks Foundation Models</strong> [<a target="_blank" rel="noopener" href="https://notebooks.databricks.com/demos/llm-rag-chatbot/index.html">Link</a>]</p>
<p><strong>Adobe‚Äôs ‚ÄòEthical‚Äô Firefly AI Was Trained on Midjourney Images</strong> [<a target="_blank" rel="noopener" href="https://www.bloomberg.com/news/articles/2024-04-12/adobe-s-ai-firefly-used-ai-generated-images-from-rivals-for-training">Link</a>]</p>
<p><strong>Exclusive: Microsoft‚Äôs OpenAI partnership could face EU antitrust probe, sources say</strong> [<a target="_blank" rel="noopener" href="https://www.reuters.com/technology/microsofts-openai-partnership-could-face-eu-antitrust-probe-sources-say-2024-04-18/">Link</a>]</p>
<p><strong>Meta AI adds Google Search results</strong> [<a target="_blank" rel="noopener" href="https://searchengineland.com/meta-ai-google-search-results-439825">Link</a>]</p>
<p><strong>Our next-generation Meta Training and Inference Accelerator</strong>  [<a target="_blank" rel="noopener" href="https://ai.meta.com/blog/next-generation-meta-training-inference-accelerator-AI-MTIA/">Link</a>]</p>
<p><strong>Meta‚Äôs new AI chips run faster than before</strong> [<a target="_blank" rel="noopener" href="https://www.theverge.com/2024/4/10/24125924/meta-mtia-ai-chips-algorithm-training">Link</a>]</p>
<p><strong>Anthropic-cookbook: a collection of notebooks &#x2F; recipes showcasing some fun and effective ways of using Claude</strong> [<a target="_blank" rel="noopener" href="https://github.com/anthropics/anthropic-cookbook">Link</a>]</p>
<p><strong>Amazon deploys 750,000+ robots to unlock AI opportunities</strong> [<a target="_blank" rel="noopener" href="https://aimagazine.com/machine-learning/amazon-deploys-750-000-robots-to-unlock-ai-opportunities">Link</a>]</p>
<p><strong>Apple‚Äôs four new open-source models could help make future AI more accurate</strong> [<a target="_blank" rel="noopener" href="https://appleinsider.com/articles/24/04/24/apples-four-new-open-source-models-could-help-make-future-ai-more-accurate">Link</a>]</p>
<p><strong>The Mystery of ‚ÄòJia Tan,‚Äô the XZ Backdoor Mastermind</strong> [<a target="_blank" rel="noopener" href="https://www.wired.com/story/jia-tan-xz-backdoor/">Link</a>]</p>
</blockquote>
<h3 id="YouTube"><a href="#YouTube" class="headerlink" title="YouTube"></a><strong>YouTube</strong></h3><blockquote>
<p> <em>If someone whom you don‚Äôt trust or an adversary gets something more powerful, then I think that that could be an issue. Probably the best way to mitigate that is to have good open source AI that becomes the standard and in a lot of ways can become the leader. It just ensures that it‚Äôs a much more even and balanced playing field.</em></p>
<p><strong>‚Äï  Mark Zuckerberg - Llama 3, $10B Models, Caesar Augustus, &amp; 1 GW Datacenters</strong> [<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=bc6uFV9CJGg">Link</a>]</p>
</blockquote>
<p>What I learned from this interview: The future of Meta AI would be a kind of AI general assistant product where you give it complicated tasks and then it goes away and does them. Meta will probably build bigger clusters. No one has built 1GW data center yet but building it could just be a matter of time. Open source can be both bad and good. People can use LLM to do harmful things, while what Mask worries more about is the concentration of AI, where there is an untrustworthy actor having the super strong AI. Open source software can make AI not getting stuck in one company but can be broadly deployed to a lot of different systems. People can set standards on how it works and AI can get checked and upgraded together. </p>
<blockquote>
<p><em>It is clear that inference was going to be a scaled problem. Everyone else had been looking at inference as you take one chip, you run a model on it, it runs whatever. But what happened with AlphaGo was we ported the software over, and even though we had 170 GPUs vs 48 TPUs, the 48 TPUs won 99 out of 100 games with the exact same software. What that meant was compute was going to result in better performance. And so the insight was - let‚Äôs build scaled inference.</em></p>
<p><em>(Nvidia) They have the ecosystem. It‚Äôs a double-sided market. If they have a kernel-based approach they already won. There‚Äôs no catching up. The other way that they are very good is vertical integration and forward integration. What happens is Nvidia over and over again decides that they want to move up the stack, and whatever the customers are doing, they start doing it.</em></p>
<p><em>Nvidia is incredible at training. And I think the design decision that they made including things like HBM, were really oriented around the world back then, which was everything is about training. There weren‚Äôt any real world application. None of you guys were really building anything in the wild where you needed super fast inference.</em></p>
<p><em>What we saw over and over again was you would spend 100% of your compute on training, you would get something that would work well enough to go into production, and then it would flip to about 5%-10% training and 90%-95% inference. But the amount of training would stay the same, the inference would grow massively. And so every time we would have a success at Google, all of a sudden, we would have a disaster, we called it the success disaster, where we can‚Äôt afford to get enough compute for inference.</em></p>
<p><em>HBM is this High Bandwidth Memory which is required to get performance, because the speed at which you can run these applications depends on how quickly you can read that into memory. There‚Äôs a finite supply, it‚Äôs only for data centers, so they can‚Äôt reach into the supply for mobile or other things, like you can with other parts. Also Nvidia is the largest buyer of super caps in the world and all sorts of other components. The 400 gigabit cables, they‚Äôve bought them all out. So if you want to compete, it doesn‚Äôt matter how good of a product you design, they‚Äôve bought out the entire supply chain for years.</em></p>
<p><em>The biggest difference between training and inference is when you are training, the number of tokens that you are training on is measured in month, like how many tokens can we train on this month. In inference, what matters is how many tokens you can generate per millisecond or a couple milliseconds.</em> </p>
<p><em>It‚Äôs fair to say that Nvida is the exemplar in training but really isn‚Äôt yet the equivalent scaled winner in inference.</em></p>
<p><em>In order to get the latency down, we had to design a completely new chip architecture, we had to design a completely new networking architecture, an entirely new system, an entirely new runtime, an entirely new compiler, and entirely new orchestration layer. We had to throw everything away and it had to be compatible with PyTorch and what other people actually developing in.</em></p>
<p><em>I think Facebook announced that by the end of this year, they are going to have the equivalent of 650000 H100s. By the end of this year, Grok will have deployed 100000 of our LPUs which do outperform the H100s on a throughput and on a latency basis. So we will probably get pretty close to the equivalent of Meta ourselves. By the end of next year, we are going to deploy 1.5M LPUs, for comparison, last year Nvidia deployed a total of 500000 H100s. So 1.5M means Grok will probably have more inference GenAI capacity than all of the hyperscalers and clouds service providers combined. So probably about 50% of the inference compute in the world.</em></p>
<p><em>I get asked a lot should we be afraid of AI and my answer to that is, if  you think back to Galileo, someone who got in a lot of trouble. The reason he got in trouble was he invented the telescope, popularized it, and made some claims that we were much smaller than everyone wanted to believe. The better the telescope got the more obvious it became that we were small. In a large sense, LLMs are the telescope for the mind, it‚Äôs become clear that intelligence is larger than we are and it makes us feel really really small and it‚Äôs scary. But what happened over time was as we realized the universe was larger than we thought and we got used to that, we started to realize how beautiful it was and our place in the universe. And I think that‚Äôs what‚Äôs going to happen. We‚Äôre going to realize intelligence is more vast than we ever imagined. And we are going to understand our place in it, and we are not going to be afraid of it.</em></p>
<p><strong>‚Äï  Conversation with Groq CEO Jonathan Ross</strong> [<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=UneoszmxRGg&ab_channel=SocialCapital">Link</a>]</p>
</blockquote>
<p>This is a very insightful conversation especially in the part of comparison of training and inference. The answer to the final question is fascinating to end the conversation. A great takeaway that ‚Äúintelligence is a telescope for the mind, in that we realize that we are small, while then also opportunity to see intelligence is vast and to not be afraid of it.‚Äù.</p>
<blockquote>
<p> <strong>Meta Announces Llama 3 at Weights &amp; Biases‚Äô Conference - Weights &amp; Biases</strong> [<a target="_blank" rel="noopener" href="https://youtu.be/r3DC_gjFCSA?si=z1RtN1FyyKiUOt16">Link</a>]</p>
</blockquote>
<blockquote>
<p><em>Economic value is getting disintegrated, there no value in foundational models economically. So then the question is who can build on top of them the fastest. Llama was announced last Thursday, 14 hours later Groq actually had that model deployed in the Groq Cloud, so that 100K+ developers could start building on it. That‚Äôs why that model is so popular. It puts the closed models on their heels. Because if you can‚Äôt both train and deploy iteratively and quickly enough, these open source alternatives will win, and as a result the economic potential that you have to monetize those models will not be there.  - Chamath Palihapitiya</em></p>
<p><em>By open-sourcing these models they limit competition because VCs are no longer going to plow half a billion dollars into a foundational model development company, so you limit the commercial interest and the commercial value of foundational models. - David Friedberg</em></p>
<p><em>AI is really two markets - training and inference. And inference is going to be 100 times bigger than training. And Nvidia is really good at training and very miscast at inference. The problem is that right now we need to see a capex build cycle for inference, and there are so many cheap and effective solutions, Groq being one of them but there are many others. And I think why the market reacted very negatively was that it did not seem that Facebook understood that distinction, that they were way overspending and trying to allocate a bunch of GPU capacity towards inference that didn‚Äôt make sense. - Chamath Palihapitiya</em></p>
<p><em>You want to find real durable moats not these like legal arrangement that try to protect your business through these types of contracts. One of the reasons why the industry moves so fast is best practices get shared very quickly, and one of the ways that happens is that everybody is moving around to different companies (average term of employment is 18-36 months). There are people who violate those rules (taking code to the new company etc), and that is definitely breaking the rules, but you are allowed to take with you anything in your head, and it is one of the ways that best practices sort of become more common. - David Sacks</em></p>
<p><strong>Meta‚Äôs scorched earth approach to AI, Tesla‚Äôs future, TikTok bill, FTC bans noncompetes, wealth tax - All-In Podcast</strong> [<a target="_blank" rel="noopener" href="https://youtu.be/1ZQ33OnGFWE?si=ymvk6H_9e5zda0Yc">Link</a>] </p>
</blockquote>
<blockquote>
<p><strong>Are LLMs Hitting A Wall, Microsoft &amp; Alphabet Save The Market, TikTok Ban - Big Technology Podcast</strong> [<a target="_blank" rel="noopener" href="https://podcasts.apple.com/us/podcast/big-technology-podcast/id1522960417?i=1000653713621">Link</a>]</p>
</blockquote>
<h3 id="Papers-and-Reports"><a href="#Papers-and-Reports" class="headerlink" title="Papers and Reports"></a><strong>Papers and Reports</strong></h3><blockquote>
<p><strong>ReALM: Reference Resolution As Language Modeling</strong> <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2403.20329">Link</a>]</p>
</blockquote>
<p>Apple proposed the ReALM model with 80M, 250M, 1B, and 3B parameters. It can be used on mobile devices and laptops. The task of ReALM is ‚ÄúGiven relevant entities and a task the user wants to perform, we wish to extract the entity (or entities) that are pertinent to the current user query. ‚Äú. The relevant entities can be on-screen entities, conversational entities, and background entities. The analysis shows that ReALM beats MARRs and has similar performance with GPT-4.</p>
<blockquote>
<p><strong>Bigger is not Always Better: Scaling Properties of Latent Diffusion Models</strong> [<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01367">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>CodeGemma: Open Code Models Based on Gemma</strong>  [<a target="_blank" rel="noopener" href="https://storage.googleapis.com/deepmind-media/gemma/codegemma_report.pdf">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention</strong> [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2404.07143.pdf">Link</a>]</p>
</blockquote>
<p>Google introduced the next generation transformer - infini-transformer. It‚Äôs able to take infinite length of input without the requirement of more memory or computation. Unlike vanilla attention mechanism in traditional transformer which reset their attention memory after each context window to manage new data, infini-attention retains a compressive memory and builds in both masked local attention and long-term linear attention mechanisms. The model compresses and reuses key-value states across all segments, allowing it to pull relevant information from any part of the document.</p>
<blockquote>
<p><em>AI agents are starting to transcend their digital origins and enter the physical world through devices like smartphones, smart glasses, and robots. These technologies are typically used by individuals who are not AI experts. To effectively assist them, Embodied AI (EAI) agents must possess a natural language interface and a type of ‚Äúcommon sense‚Äù rooted in human-like perception and understanding of the world.</em> </p>
<p><strong>OpenEQA: Embodies Question Answering in the Era of Foundation Models</strong> [<a target="_blank" rel="noopener" href="https://open-eqa.github.io/assets/pdfs/paper.pdf">Link</a>] [<a target="_blank" rel="noopener" href="https://open-eqa.github.io/">Link</a>]</p>
</blockquote>
<p>The OpenEQA introduced by Meta is the first open vocab benchmark dataset for the formulation of Embodied Question Answering (EQA) task of understanding environment either by memory or by active exploration, well enough to answer questions in natural language. Meta also provided an automatic LLM-powered evaluation protocol to evaluate the performance of SOTA models like GPT-4V and see whether it‚Äôs close to human-level performance.</p>
<p>OpenEQA looks like the very first step towards ‚Äúworld model‚Äù and I‚Äôm excited that it‚Äôs coming. The dataset contains over 1600 high-quality human generated questions drawn from over 180 real-world environments. If the future AI agent can answer N questions over N real-world environments, where N is approximately infinity, we can call it God intelligence. But we are probably not able to achieve that ‚Äúworld model‚Äù at least with my limited imagination, because it requires un-infinite compute resources and there can be ethical issues. However, if we take one step back, instead of creating ‚Äúworld model‚Äù, a ‚Äúhuman society model‚Äù or ‚Äútransformation model‚Äù, etc, sounds more possible. Limiting question to a specific pain point problem and limiting environment according to it would both save resources and contribute AI‚Äôs value to human society.</p>
<blockquote>
<p><strong>OpenELM: An Efficient Language Model Family with Open-source Training and Inference Framework</strong> [<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.14619">Link</a>] [<a target="_blank" rel="noopener" href="https://huggingface.co/apple/OpenELM">Link</a>]</p>
</blockquote>
<p>OpenELM is a small language model (SLM) tailored for on-device applications. The models range from 270M to 3B parameters, which are suitable for deployment on mobile devices and PCs. The key innovation is called ‚Äúlayer-wise scaling architecture‚Äù. It allocates fewer parameters to the initial transformer layers and gradually increases the number of parameters towards the final layers. This approach optimizes compute resources while remaining high accuracy. Inference of OpenELM can be run on Intel i9 workstation with RTX 4090 GPU and an M2 Max MacBook Pro.</p>
<blockquote>
<p><strong>Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone</strong> [<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.14219">Link</a>] [<a target="_blank" rel="noopener" href="https://huggingface.co/chat/models/microsoft/Phi-3-mini-4k-instruct">Link</a>]</p>
</blockquote>
<p>Microsoft launched Phi-3 family including mini (3.8B), small (7B), and medium (14B). These models are designed to run efficiently on both mobile devices and PCs. All models leverage a transformer decoder architecture. The performance is comparable to larger models such as Mixtral 8x7B and GPT3.5. It supports a default of 4K context length but is expandable to 128K through LongRope technology. The models are trained on web data and synthetic data, using two-phase approach which enhances both general knowledge and specialized skills (e.g. logical reasoning), and fine tuned in specific domains. Mini (3.8B) is especially optimized for mobile usage, requiring 1.8GB memory when compressed to 4-bits and processing 12+ tokens per second on mobile devices such as iPhone 14.</p>
<blockquote>
<p><strong>VASA-1: Lifelike Audio-Driven Talking Faces Generated in Real Time</strong> [<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.10667">Link</a>] [<a target="_blank" rel="noopener" href="https://www.microsoft.com/en-us/research/project/vasa-1/">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>2024 Generative AI Prediction Report from CB insights</strong> [<a target="_blank" rel="noopener" href="https://www.cbinsights.com/reports/CB-Insights_Generative-AI-Predictions-2024.pdf?utm_medium=email&_hsenc=p2ANqtz-9Ufhxz7mEW7sv4Kc0EtzE3m4UO4cBLN9U0p84fdXvKQQZPt1iee0UEjCqVCV5qhB6FfOI_gGQA4NoVqQbPYL8d8lc1BQ&_hsmi=228539474&utm_content=228539474&utm_source=hs_automation">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Stable Diffusion 3</strong> [<a target="_blank" rel="noopener" href="https://stability.ai/news/stable-diffusion-3">Link</a>]</p>
<p><strong>Stable Diffusion 3 API now available as Stable Assistant effort looms</strong> [<a target="_blank" rel="noopener" href="https://venturebeat.com/ai/stable-diffusion-3-api-now-available-as-stable-assist-effort-looms/">Link</a>]</p>
<p><strong>Stable Diffusion 3: Research Paper</strong> [<a target="_blank" rel="noopener" href="https://stability.ai/news/stable-diffusion-3-research-paper">Link</a>]</p>
</blockquote>
<h3 id="Substack"><a href="#Substack" class="headerlink" title="Substack"></a><strong>Substack</strong></h3><blockquote>
<p><em>You don‚Äôt get paid for working hard.</em></p>
<p><em>You get paid based on how hard you are to replace.</em></p>
<p><em>You get paid based on how much value you deliver.</em></p>
<p><em>Focus on being able to produce value and money will follow.</em></p>
<p><strong>‚Äï  Andrew Lokenauth</strong></p>
</blockquote>
<p>What he‚Äôs saying is so true - Don‚Äôt work so hard and end up losing yourself. </p>
<blockquote>
<p><em>There is a popular saying on Wall Street. While IPO means Initial Public Offering, it also means ‚ÄúIt‚Äôs Probably Overpriced‚Äù (coined by Ken Fisher).</em></p>
<p><em>I don‚Äôt invest in brand-new IPOs during the first six months. Why? Shares tend to underperform out of the gate for new public companies and often bottom around the tail end of the lock-up period, with anticipation of selling pressure from insiders.  It‚Äôs also critical to gain insights from the first few quarters to form an opinion about the management team.</em></p>
<ul>
<li><em>Do they forecast conservatively?</em></li>
<li><em>Do they consistently beat their guidance?</em></li>
</ul>
<p><em>If not, it might be a sign that they are running out of steam and may have embellished their prospects in the S-1. But we need several quarters to understand the dynamic at play.</em></p>
<p><strong>‚Äï Rubrik IPO: Key Takeaways - App Economy Insights</strong> [<a target="_blank" rel="noopener" href="https://www.appeconomyinsights.com/p/rubrik-ipo-key-takeaways">Link</a>]</p>
</blockquote>
<p>An analysis of Rubrik, a Microsoft-backed cybersecurity company going public. I‚Äôve got some opinions from the author in terms of company performance and strategic investment.</p>
<blockquote>
<p><strong>Intel Unleashes Enterprise AI with Gaudi 3 - AI Supremacy</strong> [<a target="_blank" rel="noopener" href="https://aisupremacy.substack.com/p/intel-unleashes-enterprise-ai-with">Link</a>]</p>
</blockquote>
<p>Intel is a huge beneficiary of Biden‚Äôs CHIPS Act. In late March 2024, Intel will receive up to $8.5 billion in grants and $11 billion in loans from the US government to produce cutting-edge semiconductors.</p>
<blockquote>
<p><strong>US Banks: Uncertain Year - App Economy Insights</strong> [<a target="_blank" rel="noopener" href="https://www.appeconomyinsights.com/p/us-banks-uncertain-year">Link</a>]</p>
</blockquote>
<blockquote>
<p><em>Formula 1‚Äôs recent surge in popularity and revenue isn‚Äôt simply a product of fast cars and daring drivers. The Netflix docuseries Drive to Survive, which premiered in March 2019 and is already in its sixth season, has played a transformative role in igniting global interest and fueling unprecedented growth for the sport.</em></p>
<p><em>The docuseries effectively humanized the sport, attracting new fans drawn to the high-stakes competition, team rivalries, and compelling personal narratives.</em></p>
<p><strong>‚Äï  Formula 1 Economics - App Economy Insights</strong> [<a target="_blank" rel="noopener" href="https://www.appeconomyinsights.com/p/formula-1-economics?utm_source=profile&utm_medium=reader2">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Netflix Engagement Machine - App Economy Insights</strong> [<a target="_blank" rel="noopener" href="https://www.appeconomyinsights.com/p/netflix-engagement-machine">Link</a>]</p>
</blockquote>
<p>Recent business highlights: 1) focus on drama and storylines around sports, 2) subscribers can download exclusive games on the App Store for free, since Nov 2021, and Netflix is exploring game monetization through in-app purchases or ads, 3) for Premium Subscription Video on Demand, churn plummets YoY, 4) the $6.99&#x2F;month ad-supported plan was launched in Nov 2023, memberships grew 65% QoQ and monetization is still lagging, 5) started limiting password sharing within one household. </p>
<blockquote>
<p><strong>Boeing vs Airbus - App Economy Insights</strong> [<a target="_blank" rel="noopener" href="https://www.appeconomyinsights.com/p/boeing-vs-airbus?utm_source=profile&utm_medium=reader2">Link</a>]</p>
</blockquote>
<p>Boeing 737 MAX‚Äôs two fatal crashes due to faulty software have eroded public trust. In addition to quality issues, Boeing is facing severe production delays. Airbus on the other hand has captured significant market share from Boeing. Airbus is heavily investing in technologies such as hydrogen-powered aircraft and sustainable aviation fuels. Airbus is also investing in the A321XLR and potential new widebody aircraft.</p>
<blockquote>
<p><strong>We disagree on what open-source AI should mean - Interconnects</strong> [<a target="_blank" rel="noopener" href="https://www.interconnects.ai/p/flavors-of-open-source-ai?utm_source=profile&utm_medium=reader2">Link</a>]</p>
</blockquote>
<blockquote>
<p><em>This is a general trend we have observed a couple of years ago. We called is <strong>Mosaic‚Äôs Law</strong> where a model of a certain capability will require 1&#x2F;4th the $ every year from hw&#x2F;sw&#x2F;algo advances. This means something that is $100m today -&gt; $25m next year -&gt; $6m in 2 yrs -&gt; $1.5m in 3 yrs.</em>  <em>‚Äï  Naveen Rao on X</em> [<a target="_blank" rel="noopener" href="https://twitter.com/NaveenGRao/status/1772969283011920189">Link</a>]</p>
<p><strong>DBRX: The new best open model and Databricks‚Äô ML strategy - Interconnects</strong> [<a target="_blank" rel="noopener" href="https://www.interconnects.ai/p/databricks-dbrx-open-llm?utm_source=profile&utm_medium=reader2">Link</a>]</p>
</blockquote>
<p>In the test of refusals, it shows that the inference system seems to contain an added filtering in the loop to refuse illegal requests.</p>
<blockquote>
<p><strong>Llama 3: Scaling open LLMs to AGI - Interconnects</strong> [<a target="_blank" rel="noopener" href="https://www.interconnects.ai/p/llama-3-and-scaling-open-llms?utm_source=profile&utm_medium=reader2">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Tesla: Robotaxi Pivot - App Economy Insights</strong> [<a target="_blank" rel="noopener" href="https://www.appeconomyinsights.com/p/tesla-robotaxi-pivot">Link</a>]</p>
</blockquote>
<p>Q1 FY24 is bad and probably the worst. This means Tesla is going to get better in the rest of the year. It sounds that Elon is more clear and focused on his plan. And promises are met though there are some delays [<a target="_blank" rel="noopener" href="https://www.tesla.com/blog/master-plan-part-deux">Master Plan, Part Deux</a>]. </p>
<p>Recent business highlights: 1) cancelling Model 2 and focusing on Robotaxis and next-gen platform (Redwood), 2) laying off 10%+, 3) FSD price cuts and EV (Model 3 and Model Y) price cuts, 4) Recall Cybertruck due to safety issues, 5) North American Charging Standard (NACS) is increasingly adopted by major automakers, 6) reached ~1.2B miles driven by FSD beta, 7) energy storage deployment increased sequentially.</p>
<p>What is competitive in the market: 1) competitive pressure from BYD, 2) OpenAI‚Äôs Figure 01 robot and Boston Dynamics‚Äôs next-gen Atlas are competing with Optimus.</p>
<blockquote>
<p><em>With its open-source AI model Llama, Meta learned that the company doesn‚Äôt have to have the best models ‚Äî but they need a lot of them. The content creation potential benefits Meta‚Äôs platforms, even if the models aren‚Äôt exclusively theirs.</em></p>
<p><em>Like Google with Android, Meta aims to build a platform to avoid being at the mercy of Apple or Google‚Äôs ecosystems. It‚Äôs a defensive strategy to protect their advertising business. The shift to a new vision-based computing experience is an opportunity to do so.</em></p>
<p><em>Meta has a head start in the VR developer community compared to Apple. A more open app model could solidify this advantage.</em></p>
<p><em>By now, Meta has a clear playbook for new products:</em></p>
<ol>
<li><em>Release an early version to a limited audience.</em></li>
<li><em>Gather feedback and start improving it.</em></li>
<li><em>Make it available to more people.</em></li>
<li><em>Scale and refine.</em></li>
<li><em>Monetize.</em></li>
</ol>
<p>He also shared some interesting nuggets:</p>
<ul>
<li>Roughly 30% of Facebook posts are AI-recommended.</li>
<li>Over 50% of Instagram content is AI-recommended.</li>
</ul>
<p><strong>‚Äï  Meta: The Anti-Apple - App Economy Insights</strong> [<a target="_blank" rel="noopener" href="https://www.appeconomyinsights.com/p/meta-the-anti-apple">Link</a>]</p>
</blockquote>
<p>Recent business highlights: 1) announced an open model for Horizon OS - which powers its VR headsets, 2) Meta AI is now powered by Llama 3, 3) whether not TikTok will still exist in US does not matter since the algorithm will not be sold, then it will benefit any competitor company such as Meta.</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://jokerdii.github.io/digital-di/2024/04/29/Zuck-s-strategies-behind-open-source/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/digital-di/images/avatar.gif">
      <meta itemprop="name" content="Di Zhen">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Di's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Di's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/digital-di/2024/04/29/Zuck-s-strategies-behind-open-source/" class="post-title-link" itemprop="url">Zuck's strategies behind open source</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2024-04-29 22:58:31" itemprop="dateCreated datePublished" datetime="2024-04-29T22:58:31-04:00">2024-04-29</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-05-15 22:48:00" itemprop="dateModified" datetime="2024-05-15T22:48:00-04:00">2024-05-15</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p><em>Random words:</em></p>
<p><em>Music teacher never answered my question: why should Triangle be included in a piece of music while there are already 10+ instruments and sounds loud in there? I accidentally got the answer from my dance teacher. She said: ‚Äúdifferent people have different hearing capabilities and thus different understanding of music, what dancers are doing is actually to interpret or reproduce music.‚Äù</em></p>
<p>Back to the topic:</p>
<p>There is always a lot to learn about strategic thinking from Zuck. Here are some of his smart strategies behind open source I‚Äôve learned:</p>
<ol>
<li>According to this <a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=bc6uFV9CJGg">interview</a>, Zuck‚Äôs point of open source is to avoid concentration of AI while he didn‚Äôt ignore the harmful consequences of open source saying that it‚Äôs our responsibility to do a good job of reducing harm. There are several benefits of open source, one is that people could figure out cheaper ways to develop models so it won‚Äôt cost too much resource. The other benefit is that they could enable more efficient developments and vertical use cases in a lot of different systems. Take Google and Apple for example, their mobile ecosystems restricted what developers could build or what features they could launch on them.</li>
</ol>
<ol start="2">
<li>For companies like Meta with well-established network effect, they really don‚Äôt need to have the best model. AI‚Äôs content creation potential benefits Meta‚Äôs platforms, even if the models are not exclusively theirs. This is the most reasonable reason from business perspective and was stated in an <a target="_blank" rel="noopener" href="https://s21.q4cdn.com/399680738/files/doc_financials/2023/q4/META-Q4-2023-Earnings-Call-Transcript.pdf">earnings call</a>. </li>
<li>By open-sourcing models, Meta started developer communities which can contribute to whatever the ecosystem Meta built and help solidify the advantage of it. Most recent example is the open model of Horizon OS which powers its VR headsets. It allows developers and creators to take advantage of these technologies to create MR experiences and grow business on it. Then Meta Quest Store can be quickly established.</li>
<li>Models themselves are not a moats. Moats are built through data and habits. Open source eventually makes economic value of foundational model disintegrated. There will be no value in foundational model economically and there is probably less point for VC to plow billions of dollars into a foundational model development startup. The potential economic values are in 100K+ developers iteratively and quickly training and deploying the open source models for specific business use cases. Inference will be way more important than training. So attention and money will be less concentrated to products like OpenAI GPT series and Nvidia training GPU but more on Cloud platforms with inference GPU for personal and business usage.</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://jokerdii.github.io/digital-di/2024/04/23/Scaling-law-is-not-working/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/digital-di/images/avatar.gif">
      <meta itemprop="name" content="Di Zhen">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Di's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Di's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/digital-di/2024/04/23/Scaling-law-is-not-working/" class="post-title-link" itemprop="url">Scaling law is not working?</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2024-04-23 21:37:16" itemprop="dateCreated datePublished" datetime="2024-04-23T21:37:16-04:00">2024-04-23</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-07-27 18:25:44" itemprop="dateModified" datetime="2024-07-27T18:25:44-04:00">2024-07-27</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>Looking at various LLM models developed in the past few months, we are absolutely pursuing LLM with larger number of parameters. This is counterintuitive to the concept of ‚Äòparsimony‚Äô in traditional machine learning, which means using a minimum number of parameters necessary to train the model therefore preventing overfitting problem on unseen data. But as we‚Äôve all seen that in computer vision and natural language processing, generative models with larger number of parameters tend to outperform their more parsimonious counterparts. There are probably two main hypotheses: 1) increasing the amount of data can improve the performance of LLM, then the huge size of dataset requires large number of parameters otherwise the model is going to be underfitting, and 2) large number of parameters allow the model to generate high-resolution pictures, provide more detailed answers, gain the capability of solving more intricate problems.</p>
<p>The paper ‚ÄúScaling Law for Neural Language Models‚Äù published in 2020 by OpenAI introduced the concept of scaling law - the performance of NLM is highly correlated with the scale of network, the size of dataset, and compute, but weakly correlated with model architecture (number of layers, depth, and width). There‚Äôs been a debate on this scaling law. OpenAI believes that parameter size is more important - for every x10 increase of compute, dataset size should be increased by x1.8, and parameter size should be increased by x5.5. However, DeepMind believes that they are equally important - for every x10 increase of compute, dataset size and parameter size should both be increased by x3.16.</p>
<p>The paper ‚ÄúBigger is not Always Better: Scaling Properties of Latent Diffusion Models‚Äù published by Google this month summarized an investigation of scaling law on Latent Diffusion Models (LDMs) specifically. It points out that ‚Äúwhen operating under a given inference budget, smaller models frequently outperform their larger equivalents in generating high-quality results‚Äù. It turns out that with the same compute capacity, smaller model with large number of training steps can outperform larger model with small number of training steps. The existence of trade-off here is due to the fact that total compute is proportional to the number of training steps times GFLOPS (number of rounds of one forward propagation and one back propagation). But with the same number of training steps, it‚Äôs still true that the larger model the better performance. As the expense is calculated roughly by the parameter size times dataset size, this finding opens up new pathways to enhance LDM‚Äôs generative capacity within limited inference budgets.</p>
<p>Reference:</p>
<ol>
<li><p>Scaling Laws for Neural Language Models [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2001.08361.pdf">Link</a>]</p>
</li>
<li><p>Bigger is not Always Better: Scaling Properties of Latent Diffusion Models [<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01367">Link</a>]</p>
</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://jokerdii.github.io/digital-di/2024/04/21/Concerns-about-AI/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/digital-di/images/avatar.gif">
      <meta itemprop="name" content="Di Zhen">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Di's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Di's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/digital-di/2024/04/21/Concerns-about-AI/" class="post-title-link" itemprop="url">Concerns about AI</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2024-04-21 11:27:58" itemprop="dateCreated datePublished" datetime="2024-04-21T11:27:58-04:00">2024-04-21</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-04-22 21:54:31" itemprop="dateModified" datetime="2024-04-22T21:54:31-04:00">2024-04-22</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>I have read a lot of news and articles, and watched a few interviews regarding AI safety these days. The future of AI is promising. People are working towards more powerful AI and personalized AI assistants or agents. At the same time, AI causes problems. A very obvious downside of AI is people could use AI to do harmful things, but I think we are good to work together and prevent that from different angles such as legal aspect or open source software. I worry more about things that are not easily avoided and cannot be seen at least in these years when AI is still immature - which is being too early to rely on AI and thus deviating from truth. For example, it is too soon for us to lose faith in jobs like teachers, historians, journalists, writers, etc, but I‚Äôm concerning we are already losing faith in those jobs because of the development of AI and some violations of copyrighted work. As we have seen that AI could have wrong understanding of facts, have biased opinions, and make things up that don‚Äôt exist, we lives could fight for the truth but the dead cannot speak for themselves. It would be pathetic if human live in hallucination in the future, and I don‚Äôt know if there‚Äôs any good practice to prevent it. It‚Äôs like Pandora‚Äôs box is opened and complications cannot be stopped. But we should at least think seriously about the potential impact of AI on society and human consciousness and possible unexpected consequences.</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://jokerdii.github.io/digital-di/2024/03/31/2024-March/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/digital-di/images/avatar.gif">
      <meta itemprop="name" content="Di Zhen">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Di's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Di's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/digital-di/2024/03/31/2024-March/" class="post-title-link" itemprop="url">2024 March</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2024-03-31 21:51:44" itemprop="dateCreated datePublished" datetime="2024-03-31T21:51:44-04:00">2024-03-31</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-09-02 11:47:29" itemprop="dateModified" datetime="2024-09-02T11:47:29-04:00">2024-09-02</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="Books"><a href="#Books" class="headerlink" title="Books"></a><strong>Books</strong></h3><blockquote>
<p><em>Humans do inspiration; machines do validation.</em> </p>
<p><em>Math is good at optimizing a known system; humans are good at finding a new one. Put another way, change favors local maxima; innovation favors global disruption.</em></p>
<p><strong>‚Äï  ‚ÄúLean Analytics, Use Data to Build a Better Startup Faster‚Äù</strong></p>
</blockquote>
<p>Sometimes people who are doing hard data work may forget to step back and look at the big picture. This is a common mistake because we can definitely go from scientific data analysis to actionable insight for making better business decision. But we need to have some additional thoughts about whether the decision is a global optima or it‚Äôs just local due to the limited sample, restricted project goal, or restricted team scope.</p>
<h3 id="Articles"><a href="#Articles" class="headerlink" title="Articles"></a><strong>Articles</strong></h3><blockquote>
<p><em>When I decided to end my voluntary immersion in the driver community, I could not shake the feeling that the depersonalization of app workers is a feature, not a bug, of an economic model born of and emboldened by transformations that are underway across the global economy. This includes increasingly prevalent work arrangements characterized by weak employer-worker relations (independent contracting), strong reliance on technology (algorithmic management, platform-mediated communication), and social isolation (no coworkers and limited customer interactions).</em></p>
<p><em>As forces continue to erode traditional forms of identity support, meaningful selfdefinition at work will increasingly rely on how we collectively use and misuse innovative technologies and business models.</em><br><em>For example, how can companies deploy algorithmic management in a way that doesn‚Äôt threaten and depersonalize workers? How can focusing on the narratives that underlie and animate identities help workers reimagine what they really want and deserve out of a career coming out of the pandemic and the Great Resignation? Will increasingly immersive and realistic digital environments like the metaverse function as identity playgrounds for workers in the future? How will Web3 broadly, and the emergence of novel forms of organizing specifically (e.g., decentralized autonomous organizations or DAOs), affect the careers, connections, and causes that are so important to workers? What role can social media platforms, online discussion forums, and other types of virtual water coolers play in helping independent workers craft and sustain a desirable work identity? In short, how can we retain the human element in the face of increasingly shrewd resource management tactics?‚Äù</em></p>
<p><strong>‚Äï ‚ÄúDehumanization is a Feature of Gig Work, Not a Bug‚Äù, Harvard Business Review, The Year in Tech 2024</strong> [<a target="_blank" rel="noopener" href="https://hbr.org/2022/06/dehumanization-is-a-feature-of-gig-work-not-a-bug">Article</a>]</p>
</blockquote>
<p>This reminds me that last year when I was on vacation in LA, I‚Äôve talked to a driver worked for both Lyft and Uber in LA. He complained Lyft‚Äôs route recommendation algorithm is shitty, not helpful at all, a waste of time, while Uber is better in comparison. At that time I realized how important it is to strengthen employer worker relations and gather feedback from workers or clients on the product improvement.<br>This is a great article where the author raises his concerns of dehumanization of workers in the future. While technology is advancing and economy is transforming, we don‚Äôt expect people to forget who they are in their daily basis work.</p>
<blockquote>
<p><em>Bringing a new technology to market presents a chicken-or-egg problem: The product needs a supply of complementary offerings, but the suppliers and complementors don‚Äôt exist yet, and no entrepreneur wants to throw their lot in with a technology that isn‚Äôt on the market yet.</em></p>
<p><em>There are two ways of ‚Äúsolving‚Äù this problem. First, you can time the market, and wait until the ecosystem matures‚Äî though you risk waiting a long time. Second, you can drive the market, or supply all the necessary inputs and complements yourself.</em></p>
<p><strong>‚Äï ‚ÄúDoes Elon Musk Have a Strategy?‚Äù, Harvard Business Review, The Year in Tech 2024</strong> [<a target="_blank" rel="noopener" href="https://hbr.org/2022/07/does-elon-musk-have-a-strategy">Article</a>]</p>
</blockquote>
<p>Here are two examples. To drive the market, Musk supplies both electric vehicles and charging stations. Zuckerberg proposed the concept of metaverse and changed his company‚Äôs name.</p>
<blockquote>
<p><em>This is where Musk‚Äôs Wall Street critics might say he‚Äôs weakest. Many of his businesses don‚Äôt articulate a clear logic, which is demonstrated by the unpredictable way these businesses ultimately reach solutions or products.</em></p>
<p><em>Musk has spelled out some of his prior logic in a set of ‚ÄúMaster Plans,‚Äù but most of the logical basis for exactly how he will succeed remains ambiguous. But this isn‚Äôt necessarily Musk‚Äôs fault or due to any negligence per se: When pursuing new technologies, particularly ones that open up a new market, there is no one who can anticipate the full set of possibilities of what that technology will be able to do (and what it will not be able to do).</em></p>
<p><strong>‚Äï ‚ÄúDoes Elon Musk Have a Strategy?‚Äù, Harvard Business Review, The Year in Tech 2024</strong> [<a target="_blank" rel="noopener" href="https://hbr.org/2022/07/does-elon-musk-have-a-strategy">Article</a>]</p>
</blockquote>
<p>Elon is interesting, but I have to say that we human need this type of person to leap to the future.</p>
<blockquote>
<p><em>What could he possibly want with Twitter? The thing is, over the last decade, the technological landscape has changed, and how and when to moderate speech has become a critical problem-and an existential problem for social media companies. In other words, moderating speech has looked more and more like the kind of big, complex strategic problem that captures Musk‚Äôs interest.</em></p>
<p><strong>‚Äï ‚ÄúDoes Elon Musk Have a Strategy?‚Äù, Harvard Business Review, The Year in Tech 2024</strong> [<a target="_blank" rel="noopener" href="https://hbr.org/2022/07/does-elon-musk-have-a-strategy">Article</a>]</p>
</blockquote>
<p>This is a great article which profiles Elon Musk specifically in his strategies and vision. It answered my question confusing me for two years: what was Musk thinking on buying Twitter? The answer is: Musk‚Äôs vision is not in pursuit of a specific type of solution but is in pursuit of a specific type of problem. If we go back to 2016, Igor as a CEO of Disney decided not to buy Twitter because he looked at Twitter as the solution: a global distribution platform, while concerned the quality of speech on it is a problem. Musk was looking for challenges and complexities while Igor was preventing them and looking for solutions.</p>
<h3 id="YouTube"><a href="#YouTube" class="headerlink" title="YouTube"></a><strong>YouTube</strong></h3><blockquote>
<p><em>‚ÄúOne of the things that I think OpenAI is doing that is the most important of everything that we are doing is putting powerful technology in the hands of people for free as a public good. We don‚Äôt run ads on our free version. We don‚Äôt monetize it in other ways. I think that kind of ‚Äòopen‚Äô is very important, and is a huge deal for how we fulfill the mission. ‚Äú‚Äï Sam</em></p>
<p><em>‚ÄúFor active learning, the thing is it truly needs a problem. It needs a problem that requires it. It is very hard to do research about the capability of active learning if you don‚Äôt have a task. You will come up with an artificial task, get good results, but not really convince anyone. ‚Äú, ‚ÄúActive learning will actually arrive with the problem that requires it to pop up.‚Äù‚Äï Ilya</em></p>
<p><em>‚ÄúTo build an AGI, I think it‚Äôs going to be Deep Learning plus some ideas, and self-play would be one of these ideas. Self-play has such properties that can surprise us in truly novel ways. Almost all self-play systems produce surprising behaviors that we didn‚Äôt expect. They are creating solutions to problems.‚Äù, ‚ÄúNot just random surprise but to find the surprising solution to a problem.‚Äù‚Äï Ilya</em></p>
<p><em>‚ÄúTransferring from simulation to the real world is definitely possible and it‚Äôs been exhibited many times by many different groups. It‚Äôs been especially successful in vision. Also OpenAI in the summer has demonstrated robot hand which was trained entirely in simulation. ‚Äú, ‚ÄúThe policy that it learned in simulation was trained to be very adaptive. So adaptive that when you transfer if could very quickly adapt to the physical world.‚Äù‚Äï Ilya</em></p>
<p><em>‚ÄúThe real world that I would imagine is one where humanity are like the board members of a company where the AGI is the CEO. The picture I would imagine is you have some kind of different entities, countries or cities, and the people who live there vote for what the AGI that represents them should do. You could have multiple AGIs, you would have an AGI for a city, for a country, and it would be trying to in effects take the democratic process to the next level.‚Äù ‚Äú(And the board can always fire the CEO), press the reset button, re-randomize the parameters.‚Äù‚Äï Ilya</em></p>
<p><em>‚ÄúIt‚Äôs definitely possible to build AI system which will want to be controlled by their humans.‚Äù, ‚ÄúIt will be possible to program an AGI to design it in such a way that it will have a similar deep drive that it will be delighted to fulfill, and the drive will be to help humans flourish.‚Äù‚Äï Ilya</em></p>
<p><em>‚ÄúI don‚Äôt know if most people are good. I think that when it really counts, people can be better than we think.‚Äù‚Äï Ilya</em></p>
<p><strong>Sam Altman: OpenAI, GPT-5, Sora, Board Saga, Elon Musk, Ilya, Power &amp; AGI | Lex Fridman Podcast</strong> [<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=jvqFAi7vkBc&ab_channel=LexFridman">Sam</a>] </p>
<p><strong>Ilya Sutskever: Deep Learning | Lex Fridman Podcast</strong> [<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=13CZPWmke6A&ab_channel=LexFridman">Ilya</a>]</p>
</blockquote>
<p>I watched Lex‚Äôs interview with Sam Altman uploaded on March 18, 2024, and an older interview with Ilya Sutskever happened 3 years ago. Elon‚Äôs lawsuit against OpenAI frustrated Sam but Sam is optimistic about the future and everything he is going to release in the next few months. Sam answered the questions ‚Äúwhat does open mean in OpenAI‚Äù that ‚Äòopen‚Äô mainly means putting powerful tech in the hands of people for free as a public good, but not necessarily mean open-source. He said there can be open-source models or closed-source models. About the transition between non-profit to capped for-profit, Sam said OpenAI is not setting a precedent for startup to mimic it but he suggested most startups should go for for-profit directly if they pursue profitability in the beginning.</p>
<p>Ilya‚Äôs interview is more interesting to me because he talked a lot about vision, tech, philosophy in Deep Learning. It‚Äôs impressive that he had such thoughts 3 years ago. </p>
<h3 id="News"><a href="#News" class="headerlink" title="News"></a><strong>News</strong></h3><blockquote>
<p><em>Speech is one kind of liability for companies using generative AI. The design of these systems can create other kinds of harms‚Äîby, say, introducing bias in hiring, giving bad advice, or simply making up information that might lead to financial damages for a person who trusts these systems.</em></p>
<p><em>Because AIs can be used in so many ways, in so many industries, it may take time to understand what their harms are in a variety of contexts, and how best to regulate them, says Schultz.</em></p>
<p><strong>‚Äï The AI Industry Is Steaming Toward A Legal Iceberg</strong> [<a target="_blank" rel="noopener" href="https://www.wsj.com/tech/ai/the-ai-industry-is-steaming-toward-a-legal-iceberg-5d9a6ac1">Link</a>]</p>
</blockquote>
<p>The Section 230 of Communications Decency Act of 1996 has protected internet platforms from being held liable for the things we say on them, but it doesn‚Äôt cover speech that a company‚Äôs  AI generates. It‚Äôs likely that in the future companies use AI will be liable for whatever it does. It could be a driver of pushing companies to take effort to avoid problematic AI output, and reduce ‚Äúhallucinations‚Äù (when GenAI makes stuff up). </p>
<p>A very obvious downside is people could use AI to do harmful things, but I think people are good to work together and prevent that from different angles such as legal aspect or open source software. I worry more about things that are potential or cannot be seen at least in these years when AI is still immature - which is being too early to rely on AI and deviating from truth. For example, it is too soon for people to lose faith in jobs like teachers, historians, journalists, writers, etc, but I‚Äôm concerning people are already losing faith in those jobs because of the development of AI and some violations of copyrighted work. As we have seen that AI could have wrong understanding of facts, have biased opinions, and make things up that don‚Äôt exist, we lives could fight for the truth but the dead cannot talk. </p>
<blockquote>
<p><strong>China‚Äôs latest EV is a ‚Äòconnected‚Äô car from smart phone and electronics maker Xiaomi</strong> [<a target="_blank" rel="noopener" href="https://apnews.com/article/xiaomi-electric-vehicle-ev-china-su7-13900c059ca3c530cf35ddd486328f99">Link</a>]</p>
</blockquote>
<p>Xiaomi started EV manufacturing since 2021 and launched its first EV ‚ÄúSU7‚Äù on March 28th 2024. It has the following reasons of success: 1) efficient technology manufacturing in a large scale. Though Xiaomi has no experience in auto field, it is a supply chain master, and has perfect partnership with various suppliers. 2) affordable price. SU7‚Äôs start price is 215900 yuan while Tesla‚Äôa model 3 is 245900 yuan. 3) customer experience oriented innovation. SU7 model can link to over 1000 Xiaomi devices as well as Apple‚Äôs devices. In addition, Xiaomi aims to connect its cars with its phones and home appliances in a ‚ÄúHuman x Car x Home‚Äù ecosystem.</p>
<blockquote>
<p><em>‚ÄúThe world is just now realizing how important high-speed inference is to generative Al,‚Äù said Madra. ‚ÄúAt Groa, we‚Äôre giving developers the speed, low latency, and efficiency they need to deliver on the generative Al promise. I have been a big fan of Groq since I first met Jonathan in 2016 and I am thrilled to join him and the Groq team in their quest to bring the fastest inference engine to the world.‚Äù</em></p>
<p><em>‚ÄúSeparating GroqCloud and Groq Systems into two business units will enable Groq to continue to innovate at a rapid clip, accelerate inference, and lead the Al chip race, while the legacy providers and other big names in Al are still trying to build a chip that can compete with our LPU,‚Äù added Ross.</em></p>
<p><strong>‚Äï Groq¬Æ Acquires Definitive Intelligence to Launch GroqCloud</strong> [<a target="_blank" rel="noopener" href="https://www.morningstar.com/news/pr-newswire/20240301sf50582/groq-acquires-definitive-intelligence-to-launch-groqcloud">Link</a>]</p>
</blockquote>
<p>Al chip startup Groq acquired Definitive Intelligence to launch GroqCloud business unit led by Definitive Intelligence‚Äôs CEO Sunny Madra. Groq is also forming a Groq Systems business unit by infusing engineering resources from Definitive Intelligence, which aims to greatly expanding its customer and developer ecosystem.</p>
<p>Groq‚Äôs founder Janathan Ross is the inventor of the Google Tensor Processing Unit (TPU), Google‚Äôs custom Al accelerator chip used to run models. Groq is creating a Language Processing Unit (LPU) inference engine, which is claimed to be able to run LLM at 10x speed. Now GroqCloud provides customers the Groq LPU inference engine via the self-serve playground.</p>
<blockquote>
<p><em>The House voted to advance a bill that could get TikTok banned in the U.S. on Wednesday. In a 352-65 vote, representatives passed the bipartisan bill that would force ByteDance, the parent company of TikTok, to either sell the video-sharing platform or prohibit it from becoming available in the U.S.</em></p>
<p><strong>‚Äï What to Know About the Bill That Could Get TikTok Banned in the U.S.</strong> [<a target="_blank" rel="noopener" href="https://time.com/6898845/tiktok-ban-bill-us-congress-what-to-know/">Link</a>]</p>
</blockquote>
<p>TikTok is considered as critical threats to US national security because it is owned by ByteDance and required to collaborate with the Chinese Communist Party (CCP). If the bill is passed then ByteDance has to either sell the platform within 180 days or face a ban. TikTok informed users that Congress is planning a total ban of TikTok and encouraged users to speak out against the ban. Shou Zi Chew said the ban would put more than 300000 American jobs at risk.</p>
<blockquote>
<p><em>San Francisco-based Anthropic introduced three new AI models ‚Äî Claude 3 Opus, Sonnet and Haiku. The literary names hint at the capabilities of each model, with Opus being the most powerful and Haiku the lightest and quickest. Opus and Sonnet are available to developers now, while Haiku will arrive in the coming weeks, the company said on Monday.</em></p>
<p><strong>‚Äï AI Startup Anthropic Launches New Models for Chatbot Claude</strong> [<a target="_blank" rel="noopener" href="https://www.bloomberg.com/news/articles/2024-03-04/ai-startup-anthropic-launches-new-models-for-chatbot-claude">Link</a>]</p>
</blockquote>
<blockquote>
<p><em>Waymo‚Äôs progress in California comes after General Motors-owned Cruise and Apple bowed out of the autonomous vehicle business in California, while Elon Musk‚Äôs Tesla has yet to develop an autonomous vehicle that can safely operate without a human driver at the controls.</em></p>
<p><strong>‚Äï Waymo approved by regulator to expand robotaxi service in Los Angeles, San Francisco Peninsula</strong> [<a target="_blank" rel="noopener" href="https://www.cnbc.com/2024/03/01/waymo-approved-to-expand-robotaxi-service-in-los-angeles-sf-peninsula.html">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Elon Musk requires ‚ÄúFSD‚Äù demo for every prospective Tesla buyer in North America</strong> [<a target="_blank" rel="noopener" href="https://www.cnbc.com/2024/03/25/elon-musk-requires-fsd-demo-for-every-prospective-tesla-buyer-in-north-america.html?__source=iosappshare%7Ccom.tencent.xin.sharetimeline">Link</a>]</p>
</blockquote>
<p>Full Self Driving era seems to start, but Tesla‚Äôs FSD system does not turn cars into autonomous vehicles, so drivers still need to be attentive to the road and ready to steer or brake at any time while using FSD or FSD Beta. Will FSD help with Tesla‚Äôs stock?</p>
<blockquote>
<p><strong>SpaceX Starship disintegrates after completing most of third test flight</strong> [<a target="_blank" rel="noopener" href="https://www.reuters.com/technology/space/spacex-hoping-launch-starship-farther-third-test-flight-2024-03-14/">Link</a>]</p>
</blockquote>
<p>SpaceX‚Äôs Starship rocket successfully completed a repeat of stage separation during initial ascent, open and close its payload door in orbit, the transfer of super-cooled rocket propellant from one tank to another during spaceflight. But it skipped Raptor engine re-ignition test, failed re-entry to the atmosphere, and flying the rocked back to Earth. Overall, completion of many of the objectives represented progress in the development of spacecraft for the business and SpaceX and NASA‚Äôs moon program.</p>
<blockquote>
<p><strong>Open Release of Grok-1</strong> [<a target="_blank" rel="noopener" href="https://x.ai/blog/grok-os">Link</a>]</p>
</blockquote>
<p>Musk founded xAI in March 2023 aiming to ‚Äúunderstand the true nature of the universe‚Äù. It released the weights and network architecture of 314B Grok-1 on March 17, 2024. It‚Äôs under the Apache 2.0 license meaning it allows for commercial use. The model can be found in <a target="_blank" rel="noopener" href="https://github.com/xai-org/grok">Github</a>.</p>
<blockquote>
<p><em>GB200 has a somewhat more modest seven times the performance of an H100, and Nvidia says it offers four times the training speed.</em></p>
<p><em>Nvidia is counting on companies to buy large quantities of these GPUs, of course, and is packaging them in larger designs, like the GB200 NVL72, which plugs 36 CPUs and 72 GPUs into a single liquid-cooled rack for a total of 720 petaflops of AI training performance or 1,440 petaflops (aka 1.4 exaflops) of inference. It has nearly two miles of cables inside, with 5,000 individual cables.</em></p>
<p><em>And of course, Nvidia is happy to offer companies the rest of the solution, too. Here‚Äôs the DGX Superpod for DGX GB200, which combines eight systems in one for a total of 288 CPUs, 576 GPUs, 240TB of memory, and 11.5 exaflops of FP4 computing.</em></p>
<p><em>Nvidia says its systems can scale to tens of thousands of the GB200 superchips, connected together with 800Gbps networking with its new Quantum-X800 InfiniBand (for up to 144 connections) or Spectrum-X800 ethernet (for up to 64 connections).</em></p>
<p><strong>‚Äï Nvidia reveals Blackwell B200 GPU, the ‚Äòworld‚Äôs most powerful chip‚Äô for AI</strong> [<a target="_blank" rel="noopener" href="https://www.theverge.com/2024/3/18/24105157/nvidia-blackwell-gpu-b200-ai">Link</a>] [<a target="_blank" rel="noopener" href="https://www.nvidia.com/gtc/keynote/">keynote</a>]</p>
</blockquote>
<p>Two B200 GPUs combined with one Grace CPU is a GB200 Blackwell Superchip. Two GB200 superchip is one Blackwell compute node. 18 Blackwell compute notes contain 36 CPU + 72 GPUs, becoming one larger virtual GPU - GB200 NVL72.</p>
<p>Nvidia also offers packages for companies such as DGX Superpod for DGX GB200 which combines 8 such GB200 NVL72. 8 GB200 NVL72 combined with xx becomes one GB200 NVL72 compute rack. And the AI factory or full data center in the future would consists about 56 GB200 NVL72 compute racks, which is in total around 32000 GPUs.</p>
<p>The Blackwell superchip will be 4 times faster and 25 times energy efficient than H100.</p>
<blockquote>
<p><strong>OpenAI is expected to release a ‚Äòmaterially better‚Äô GPT-5 for its chatbot mid-year, sources say</strong>[<a target="_blank" rel="noopener" href="https://www.businessinsider.com/openai-launch-better-gpt-5-chatbot-2024-3">Link</a>]</p>
</blockquote>
<p>On March 14 (local time), during a meeting with the Korean Silicon Valley correspondent group, CEO Altman mentioned, ‚ÄúI am not sure when GPT-5 will be released, but it will make significant progress as a model taking a leap forward in advanced reasoning capabilities. There are many questions about whether there are any limits to GPT, but I can confidently say ‚Äòno‚Äô.‚Äù He expressed confidence that if sufficient computing resources are invested, building AGI that surpasses human capabilities is entirely feasible.</p>
<blockquote>
<p> Other news:</p>
<p> <strong>Elon Musk sues OpenAI for abandoning its mission to benefit humanity</strong> [<a target="_blank" rel="noopener" href="https://www.theverge.com/2024/3/1/24087473/elon-musk-openai-lawsuit-nonprofit-mission">Link</a>]</p>
<p> <strong>A major AT&amp;T data leak posted to the dark web included passcodes, Social Security numbers</strong> [<a target="_blank" rel="noopener" href="https://www.businessinsider.com/att-data-leak-social-security-numbers-passcodes-email-2024-3">Link</a>]</p>
<p> <strong>Apple accused of monopolizing smartphone markets in US antitrust lawsuit</strong> [<a target="_blank" rel="noopener" href="https://www.reuters.com/legal/us-takes-apple-antitrust-lawsuit-2024-03-21/">Link</a>]</p>
<p> <strong>Amazon Invests $2.75 Billion in AI Startup Anthropic</strong> [<a target="_blank" rel="noopener" href="https://www.bloomberg.com/news/articles/2024-03-27/amazon-invests-additional-2-75-billion-in-ai-startup-anthropic">Link</a>]</p>
<p> <strong>Adam Neumann looks to buy back WeWork for more than $500M: sources</strong> [<a target="_blank" rel="noopener" href="https://nypost.com/2024/03/25/business/adam-neumann-looks-to-buy-back-wework-for-more-than-500m-report/">Link</a>]</p>
<p> <strong>NVIDIA Powers Japan‚Äôs ABCI-Q Supercomputer for Quantum Research</strong> [<a target="_blank" rel="noopener" href="https://nvidianews.nvidia.com/news/nvidia-powers-japans-abci-q-supercomputer-for-quantum-research">Link</a>]</p>
<p> <strong>Lilac Joins Databricks to Simplify Unstructured Data Evaluation for Generative AI</strong> [<a target="_blank" rel="noopener" href="https://www.databricks.com/blog/lilac-joins-databricks-simplify-unstructured-data-evaluation-generative-ai">Link</a>]</p>
</blockquote>
<h3 id="Papers-and-Reports"><a href="#Papers-and-Reports" class="headerlink" title="Papers and Reports"></a><strong>Papers and Reports</strong></h3><blockquote>
<p><strong>Scaling Instructable Agents Across Many Simulated Worlds</strong> [<a target="_blank" rel="noopener" href="https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/sima-generalist-ai-agent-for-3d-virtual-environments/Scaling%20Instructable%20Agents%20Across%20Many%20Simulated%20Worlds.pdf">Link</a>]</p>
</blockquote>
<p>Google DeepMind SIMA Team is working on the Scalable, Instructable, Multiworld Agent (SIMA) project. The goal is to develop an agent that follows instructions to complete tasks in any 3D environments. So far they are making progress on making AI agent understand the environment from computer screen, and use keyboard-and-mouse controls to interact with environment, follow language instructions, and play the video game to maximize the win-rate.</p>
<p>OpenAI has similar work called <a target="_blank" rel="noopener" href="https://openai.com/research/universe">OpenAI Universe</a>, which aims to train and validate AI agent on performing real world tasks. They started from video game environment as well. Although the goals of these two project sound similar, the minor difference is that OpenAI Universe intended to develop a platform where AI is able to interact with games, websites, and applications, while SIMA aims to develop an AI agent or maybe a robot to interact with the real world.</p>
<blockquote>
<p><strong>Announcing HyperGAI: a New Chapter in Multimodal Gen AI</strong> [<a target="_blank" rel="noopener" href="https://www.hypergai.com/blog/announcing-hypergai-a-new-chapter-in-multimodal-gen-ai">Link</a>]</p>
<p><strong>Introducing HPT: A Groundbreaking Family of Leading Multimodal LLMs</strong> [<a target="_blank" rel="noopener" href="https://www.hypergai.com/blog/introducing-hpt-a-family-of-leading-multimodal-llms">Link</a>]</p>
</blockquote>
<p>The startup HyperGAI aims to develop models for multimodal understanding and multimodal generation. They released HPT air and HPT pro. HPT pro outperforms GPT-4V and Gemini Pro on the MMbench and SEED-Image benchmark.</p>
<blockquote>
<p><strong>Mora: Enabling Generalist Video Generation via A Multi-Agent Framework</strong> [<a target="_blank" rel="noopener" href="https://arxiv.org/html/2403.13248v1">Link</a>]</p>
</blockquote>
<p>Sora is the first video generation model, however it is not open-source. Lehigh University and Microsoft Research developed Mora to address the gap of no other video generation models to parallel with Sora in performance. Mora introduces an innovative multi-agent framework. As a result, Mora marks a considerable advancement in video generation from text prompts. The evaluation shows that Mora competes with Sora on most of the tasks, but not as refined as Sora in tasks such as changes in the video content, and video connectivity. </p>
<h3 id="Substack"><a href="#Substack" class="headerlink" title="Substack"></a>Substack</h3><blockquote>
<p> <strong>Streaming Giants Earnings - App Economy Insights</strong> [<a target="_blank" rel="noopener" href="https://www.appeconomyinsights.com/p/streaming-giants-earnings-ee0">Link</a>]</p>
</blockquote>
<blockquote>
<p><em>CrowdStrike has repeated in its investor presentations how it wants to be the leading ‚ÄòSecurity Cloud‚Äô and emulate other category-defining cloud platforms:</em></p>
<ul>
<li><em>Workday (HR Cloud).</em></li>
<li><em>Salesforce (CRM Cloud).</em></li>
<li><em>ServiceNow (Service Management Cloud).</em></li>
</ul>
<p><em>Public cloud software companies are overwhelmingly unprofitable businesses. However, in FY24, Salesforce (CRM) demonstrated that margins can expand quickly once the focus turns to the bottom line (see visual). And when the entire business is driven by recurring subscription revenue and highly predictable unit economics, you are looking at a finely-tuned cash flow machine.</em></p>
<p><strong>‚Äï CrowdStrike: AI-Powered Security - App Economy Insights</strong> [<a target="_blank" rel="noopener" href="https://www.appeconomyinsights.com/p/crowdstrike-ai-powered-security">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>GRANOLAS: Europe‚Äôs Darlings - App Economy Insights</strong> [<a target="_blank" rel="noopener" href="https://www.appeconomyinsights.com/p/granolas-europes-darlings">Link</a>]</p>
</blockquote>
<blockquote>
<p><em>Oracle became TikTok‚Äôs cloud provider in 2020 for US users. With the risk of a TikTok ban in America, we‚Äôll look at the potential revenue impact.</em></p>
<p><em>Catz believes this growth (of OCI revenue) is driven by:</em></p>
<ul>
<li><em>Price Performance.</em></li>
<li><em>Full-stack technology for mission-critical workloads.</em></li>
<li><em>AI capabilities focused on business outcomes.</em></li>
<li><em>Deployment flexibility.</em></li>
<li><em>Multi-cloud offerings.</em></li>
</ul>
<p><strong>‚Äï Oracle: Cloud &amp; AI Focus - App Economy Insights</strong> [<a target="_blank" rel="noopener" href="https://www.appeconomyinsights.com/p/oracle-cloud-and-ai-focus">Link</a>]</p>
</blockquote>
<p>Oracle services for enterprise software and cloud solutions: 1) cloud suite (cloud applications and services), 2) data analytics, 3) autonomous database, 4) enterprise resource planning (ERP) to improve operational efficiencies and integrated solutions to streamline complex business functions.</p>
<p>Key news highlights: 1) Oracle acquired Cerner in June 2022 which is a leading provider of electronic health records (EHR) and other healthcare IT solutions used by hospitals and health systems. Oracle is expanding cloud services including the upcoming launch of Ambulatory Clinic Cloud Application Suite for Cerner customers. 2) The adoption of Oracle Cloud Infrastructure (OCI) are across different segments: cloud natives customers such as Zoom, Uber, ByteDance looking for high price performance and integrated security and privacy, AI&#x2F;ML customers looking for key differentiation, compute performance, and networking design, generative AI customers looking for control, data security, privacy, and governance. 3) TikTok is probably an essential component of the growth of OCI Gen2 infrastructure cloud services. 4) Oracle signed big Generation 2 Cloud infrastructure contract with Nvidia. 5) Oracle is a critical customer in Sovereign AI. It‚Äôs starting to win business per country for sovereign cloud, especially the cloud companies in Japan.</p>
<blockquote>
<p><strong>NVIDIA ‚ÄòAI Woodstock‚Äô - App Economy Insights</strong> [<a target="_blank" rel="noopener" href="https://www.appeconomyinsights.com/p/nvidia-ai-woodstock">Link</a>]</p>
</blockquote>
<blockquote>
<p><em>We spent $700,000 on [our five-second Super Bowl ad] in total and yet earned over 60 million social media impressions.‚Äù</em> [<a target="_blank" rel="noopener" href="https://youtu.be/QPjgXWm7F6c">link</a>]</p>
<p><strong>‚Äï Duolingo: Gamified Learning - App Economy Insights</strong> [<a target="_blank" rel="noopener" href="https://www.appeconomyinsights.com/p/duolingo-gamified-learning">Link</a>]</p>
</blockquote>
<p>Duolingo launched the Duolingo Max subscription tier ($168&#x2F;year), with Gen AI features enabling a more conversational and listening approach. Duolingo has leveraged AI in two areas: 1) using AI to create content, which allows it to experiment faster, 2) using AI to power spoken conversation with characters.</p>
<p>What is coming: Duolingo launched Math and Music courses into its app in 2023.</p>
<h3 id="Articles-1"><a href="#Articles-1" class="headerlink" title="Articles"></a><strong>Articles</strong></h3><blockquote>
<p><strong>OpenAI and Elon Musk</strong> [<a target="_blank" rel="noopener" href="https://openai.com/blog/openai-elon-musk">Link</a>]</p>
</blockquote>
<p>Read arguments between OpenAI and Elon. Learned that Elon once believed there is 0% probability for OpenAI to succeed and wanted OpenAI to become for-profit so it can be merged to Tesla and being controlled by Elon himself.</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://jokerdii.github.io/digital-di/2024/02/28/2024-February/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/digital-di/images/avatar.gif">
      <meta itemprop="name" content="Di Zhen">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Di's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Di's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/digital-di/2024/02/28/2024-February/" class="post-title-link" itemprop="url">2024 February</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2024-02-28 00:20:54" itemprop="dateCreated datePublished" datetime="2024-02-28T00:20:54-05:00">2024-02-28</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-05-25 10:12:21" itemprop="dateModified" datetime="2024-05-25T10:12:21-04:00">2024-05-25</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="Podcasts"><a href="#Podcasts" class="headerlink" title="Podcasts"></a><strong>Podcasts</strong></h3><blockquote>
<p><em>We know from our past experiences that big things start small. The biggest oak starts from an acorn. If you want to do anything new, you‚Äôve got to be willing to let that acorn grow into a little sapling and then into a small tree and maybe one day it will be a big business on its own.</em></p>
<p><em>He was a free thinker whose ideas would often run against the conventional wisdom of any community in which he operated.</em></p>
<p><em>I‚Äôve always actually found something to be very true, which is most people don‚Äôt get those experiences because they never ask. I have never found anybody who didn‚Äôt want to help me when I‚Äôve asked them for help. I have never found anyone who said no or hung up the phone when I called. I just asked. And when people ask me, I try to be as responsive, to pay back that debt of gratitude. Most people never pick up the phone and call. Most people never ask. That is what separates the people that do things from the people that just dream about them. You‚Äôve got to act and you‚Äôve got to be willing to fail. You‚Äôve got to be ready to crash and burn with people on the phone, with starting a company, with whatever. If you‚Äôre afraid of failing, you won‚Äôt get very far.</em></p>
<p><em>His company and its computer into something aspirational. He links this machine made a few months earlier, a few months ago by some disheveled California misfits to Rolls Royce, the 73 year old paragon of sophisticated industrial manufacturing and elite consumer taste. He even calls Apple a world leader, an absolutely unprovable claim that rockets the little company into the same league as IBM, which was then the industry‚Äôs giant. He was an extraordinary speaker and he wielded that tool to great effect.</em></p>
<p><em>People that are learning machines and they refuse to quit are incredibly hard to beat.</em></p>
<p><em>When you have something that‚Äôs working, you do not talk about it. You shut up because the more you talk about it, the more broadcasting you do about it, the more it encourages competition.</em></p>
<p><em>The only purpose for me in building a company is so that the company can make products. One is a means to the other. Over a period of time, you realize that building a very strong company and a very strong foundation of talent and culture in a company is essential to keep making great products. The company‚Äôs one of the most amazing inventions of humans, this abstract construct that‚Äôs incredibly powerful. Even so, for me, it‚Äôs about the products. It‚Äôs about working together with really fun, smart, creative people and making wonderful things. It is not about the money. What a company is, then, is a group of people who can make more than just the next big thing. It is a talent. It is a capability. It is a culture. It is a point of view. And it is a way of working together to make the next thing and the next one and the next one.</em></p>
<p><em>In that case, Steve would check it out, and the information he‚Äôd glean would go into the learning machine that was his brain. Sometimes, that‚Äôs where it would sit and nothing would happen. Sometimes, on the other hand, he‚Äôd concoct a way to combine it with something else that he‚Äôd seen or perhaps to twist it in a way to benefit an entirely different project altogether. This was one of his great talents, the ability to synthesize separate developments and technologies into something previously unimaginable.</em></p>
<p><em>I felt I had let the previous generations of entrepreneurs down, that I had dropped the baton as it was being passed to me. I met with David Packard and Bob Noyce, and tried to apologize for screwing up so badly. I was a very public failure and even thought about running away from the Valley. But something slowly began to dawn on me. I still love what I did. The turn of events at Apple had not changed that one bit. I had been rejected, but I was still in love, and so I decided to start over.</em></p>
<p>**‚Äï  Founders #265 Becoming Steve Jobs: The Evolution of a Reckless Upstart into a Visionary Leader ** [<a target="_blank" rel="noopener" href="https://podcasts.apple.com/us/podcast/founders/id1141877104?i=1000577804385">Link</a>]</p>
</blockquote>
<blockquote>
<p><em>It helps if you can be satisfied with an inner scorecard, I would also say it‚Äôs probably the only ‚Äì the single only way to have a happy life.</em></p>
<p><em>I wanted money. It could make me independent then I could do with what I wanted to do with my life. And the biggest thing I wanted to do was work for myself. I didn‚Äôt want other people directing me. The idea of doing what I wanted to do every day was very important to me.</em></p>
<p><em>I like to work by myself where I could spend my time thinking about things I wanted to think about. Washington was upsetting at first, but I was in my own world all the time. I could be sitting in a room thinking or could be writing around flinging things and thinking.</em></p>
<p><em>Walt Disney seldom dabbled. Everyone who knew him remarked on his intensity. When something intrigued him, he focused himself entirely on it as if it were the only thing that mattered.</em></p>
<p><em>Intensity is the price of excellence.</em></p>
<p><em>People ask me where they should go to work, and I always tell them to go work for whom they most admire.</em></p>
<p><em>That‚Äôs like saving sex for your old age, do what you love and work for whom you admire the most, and you‚Äôve given yourself the best chance in life you can.</em></p>
<p><em>You‚Äôll get very rich if you thought of yourself as having a card with only 20 punches in a lifetime, and every financial decision used up one punch. You will resist the temptation to dabble. You make more good decisions, and you would make more big decisions.</em></p>
<p><em>Instead, he said, basically, when you get to my age, you‚Äôll really measure your success in life by how many of the people you want to have love you actually do love you. I know people who have a lot of money, and they get testimonial dinners and they get hospital wings named after them. But the truth is that nobody in the world loves them. If you get to my age and life and nobody thinks well of you, I don‚Äôt care how big your bank account is.</em></p>
<p><em>Your life is a disaster. That‚Äôs the ultimate test of how you‚Äôve lived your life. The trouble with love is you can‚Äôt buy it. You can buy sex. You can buy testimonial dinners. You can buy pamphlets that say how wonderful you are, but the only way to get love is to be lovable. It is very irritating if you have a lot of money. You‚Äôd like to think you could write a check. I‚Äôll buy $1 million worth of love, please, but it doesn‚Äôt work that way.</em></p>
<p><em>The more you give love away, the more you get.</em></p>
<p><strong>‚Äï  Founders #100 Warren Buffett [The Snowball]</strong> [<a target="_blank" rel="noopener" href="https://podcasts.apple.com/us/podcast/founders/id1141877104?i=1000515205659">Link</a>]</p>
</blockquote>
<blockquote>
<p><em>The biggest threat to dynastic family continuity was enrichment and success.</em></p>
<p><em>Almost all of the dynasties started as outsiders.</em></p>
<p><em>Those on the margins often come to control the center.</em></p>
<p><em>Great industrial leaders are always fanatically committed to their jobs. They are not lazy or amateurs.</em></p>
<p><em>A man always has two reasons for the things he does, a good one and the real one.</em></p>
<p><em>Do it yourself, insist on quality, make something that will benefit society, and pick a mission that is bigger than yourself.</em></p>
<p><em>It is impossible to create an innovative product, unless you do it yourself, pay attention to every detail, and then to test it exhaustively. Never entrust your creation of a product to others, for that will inevitably lead to failure and cause you deep regret.</em></p>
<p><strong>‚Äï  Founders #307 The World‚Äôs Great Family Dynasties: Rockefeller, Rothschild, Morgan, &amp; Toyada</strong> [<a target="_blank" rel="noopener" href="https://podcasts.apple.com/us/podcast/founders/id1141877104?i=1000616702114">Link</a>]</p>
</blockquote>
<blockquote>
<p><em>Amazon‚Äôs single-threaded leadership: ‚ÄúThe basic premise is that for each project, there is a single leader whose focus is that project and that project alone. And that leader oversees teams of people whose attention is similarly focused on that one project.‚Äù</em></p>
<p><em>Similar idea in Peter Thiel‚Äôs book Zero to One: ‚ÄúThe best thing I did as a manager at PayPal was to make every person in the company responsible for doing just one thing. Every employee‚Äôs one thing was unique, and everyone knew I would evaluate him only on that one thing. I had started doing this just to simplify the task of managing people, but then I noticed a deeper result. Defining roles reduced conflict.‚Äù</em></p>
<p><em>‚ÄúWhen your dependencies keep growing, it‚Äôs only natural to try speeding things up by improving your communication. We finally realize that all of this cross-team communication didn‚Äôt really need refinement at all. It needed to be eliminated. It wasn‚Äôt just that we had the wrong solution in mind. Rather, we‚Äôve been trying to solve the wrong problem altogether.‚Äù</em></p>
<p><em>Jeff‚Äôs vision was that we needed to focus on loosely coupled interaction via machines through well-defined APIs rather than via humans through e-mails and meetings. This would free each team to act autonomously and move faster.</em></p>
<p><em>From his 2016 shareholder letter, Jeff suggested that most decisions should probably be made with somewhere around 70% of the information you wish you had. If you wait for 90%, in most cases, you‚Äôre probably being slow. Plus, either way, you need to be good at quickly recognizing and correcting bad decisions. If you‚Äôre good at course correcting, being wrong, may be less costly than you think, whereas being slow is going to be expensive for sure.</em></p>
<p><em>‚ÄúThe best way to fail and inventing something is by making it somebody‚Äôs part-time job. And so the problem that they were trying to solve and the vision they had was how to move faster and remove dependencies, but what they also realized once this was in place, that ownership and accountability are much easier to establish under the single-threaded leader model.‚Äù</em></p>
<p><em>‚ÄúMost large organizations embrace the idea of invention but are not willing to suffer the string of failed experiments necessary to get there.‚Äù ‚ÄúLong-term thinking levers are existing abilities and lets us do new things we couldn‚Äôt otherwise contemplate. Long-term orientation interacts well with customer obsession. If we can identify a customer need and if we can further develop conviction that the need is meaningful and durable, our approach permits us to work patiently for multiple years to deliver a solution.‚Äù</em></p>
<p><em>Invention works well where differentiation matters. Differentiation with customers is often one of the key reasons to invent.</em></p>
<p><em>Working backwards exposes skill sets that your company needs but does not have yet. So the longer that your company works backwards, the more skills it develops and the more skills it develops, the more valuable it becomes over time.</em></p>
<p><em>Founders force the issue. Not outsourcing means it‚Äôs going to be more expensive, going to spend a lot of or money. It‚Äôs going to take longer to get a product out there. But at the end of that, if we are successful, we have a set of skills that we lacked beforehand, then we can go out and do this over and over again.</em></p>
<p><strong>‚Äï  Founders #321 Working with Jeff Bezos</strong> [<a target="_blank" rel="noopener" href="https://podcasts.apple.com/us/podcast/founders/id1141877104?i=1000628642785">Link</a>]</p>
</blockquote>
<blockquote>
<p><em>‚ÄúMy passion has been to build an enduring company where people were motivated to make great products. Everything else was secondary. Sure, it was great to make a profit because that‚Äôs what allowed you to make great products. But the products, not the profits, were the motivation. Sculley flipped these priorities to where the goal was to make money.‚Äù</em></p>
<p><em>‚ÄúIt‚Äôs a subtle difference, but it ends up meaning everything, the people you hire, who gets promoted, what you discuss in meetings. Some people say, give the customer what they want, but that‚Äôs not my approach. Our job is to figure out what they‚Äôre going to want before they do. I think Henry Ford once said, ‚ÄòIf I asked customers what they wanted, they would have told me, a faster horse.‚Äô People don‚Äôt know what they want until you show it to them. That‚Äôs why I never rely on market research. Our task is to read things that are not yet on the page. Edwin Land of Polaroid talked about the intersection of the humanities and science. I like that intersection. There‚Äôs something magical about that place.‚Äù</em></p>
<p> <em>‚ÄúThere are a lot of people innovating, and that‚Äôs not the main distinction of my career. The reason Apple resonates with people is that there‚Äôs a deep current of humanity in our innovation. I think great artists and great engineers are similar in that they both have a desire to express themselves. In fact, some of the best people working on the original Mac were poets and musicians on the side.‚Äù</em> </p>
<p><em>‚ÄúIn the ‚Äò70s, computers became a way for people to express their creativity. Great artists like Leonardo da Vinci and Michelangelo were also great at science. Michelangelo knew a lot about how to quarry stone, not just how to be a sculptor. At different times in the past, there were companies that exemplified Silicon Valley. It was Hewlett-Packard for a long time. Then in the semiconductor era, it was Fairchild and Intel. I think that it was Apple for a while, and then that faded. And then today, I think it‚Äôs Apple and Google and a little more so Apple. I think Apple has stood the test of time. It‚Äôs been around for a while, but it‚Äôs still at the cutting edge of what‚Äôs going on.‚Äù</em> </p>
<p><em>‚ÄúIt‚Äôs easy to throw stones at Microsoft, and yet I appreciate what they did and how hard it was. They were very good at the business side of things. They were never as ambitious product-wise as they should have been. Bill likes to portray himself as a man of the product, but he‚Äôs really not. He‚Äôs a businessperson. Winning business was more important than making great products. He ended up the wealthiest guy around. And if that was his goal, then he achieved it. But it‚Äôs never been my goal. And I wonder in the end if it was his goal.‚Äù</em> </p>
<p><em>‚ÄúI admire him for the company he built. It‚Äôs impressive, and I enjoyed working with him. He‚Äôs bright and actually has a good sense of humor. But Microsoft never had the humanities and liberal arts in its DNA. Even when they saw the Mac, they couldn‚Äôt copy it well. They totally didn‚Äôt get it. I have my own theory about why decline happens at companies. The company does a great job, innovates and becomes a monopoly or close to it in some field. And then the quality of the product becomes less important. The company starts valuing great salesmen because they‚Äôre the ones who can move the needle on revenues, not the product engineers and designers.‚Äù</em></p>
<p><em>‚ÄúSo the salespeople end up running the company. When the sales guys run the company, the product guys don‚Äôt matter so much, and a lot of them just turn off. It happened at Apple when Sculley came in, which was my fault. Apple was lucky, and it rebounded. I hate it when people call themselves entrepreneurs when what they‚Äôre really trying to do is launch a startup and then sell or go public so they can cash in and move on. They‚Äôre unwilling to do the work it takes to build a real company, which is the hardest work in business. That is how you really make a contribution and add to the legacy of those who went before.‚Äù</em></p>
<p><em>‚ÄúYou build a company that will stand for something a generation or two from now. That‚Äôs what Walt Disney did and Hewlett and Packard and the people who built Intel. They created a company to last, not just to make money. That‚Äôs what I want Apple to be. I don‚Äôt think I run roughshod over people. But if something sucks, I tell people to their face. It is my job to be honest. I know what I‚Äôm talking about, and I usually turn out to be right. That‚Äôs the culture I try to create. We are brutally honest with each other, and anyone can tell me they think I‚Äôm full of s***, and I can tell them the same.‚Äù</em></p>
<p><em>‚ÄúAnd we‚Äôve had some rip-roaring arguments where we were yelling at each other and it‚Äôs some of the best times I‚Äôve ever had. I feel totally comfortable saying, ‚ÄòRon, that story looks like s,‚Äô in front of everyone else. Or I might say, ‚ÄòGod, we really fed up the engineering on this,‚Äô in front of the person that‚Äôs responsible. That‚Äôs the ante for being in the room. You‚Äôve got to be able to be super honest. Maybe there‚Äôs a better way, a gentlemen‚Äôs club, where we all wear ties and speak in soft language and velvet code words. But I don‚Äôt know that way because I‚Äôm middle class from California.‚Äù</em> </p>
<p><em>‚ÄúI was hard on people sometimes, probably harder than I needed to be. I remember the time when my son was six years old, coming home, I had just fired somebody that day. And I imagined what it was like for that person to tell his family and his young son that he had lost his job. It was hard, but somebody has got to do it. I figured that it was always my job to make sure that the team was excellent. And if I didn‚Äôt do it, nobody was going to do it. You always have to keep pushing to innovate.‚Äù</em></p>
<p><em>‚ÄúBob Dylan could have sung protest songs forever and probably made a lot of money, but he didn‚Äôt. He had to move on. And when he did, by going electric in 1965, he alienated a lot of people. His 1966 Europe tour was his greatest. He would come on and do a set of acoustic guitars and the audience loved him. Then he would do an electric set and the audience booed. There was one point where he was about to sing Like a Rolling Stone and someone from the audience yells, ‚ÄúJudas,‚Äù and Dylan says, ‚ÄòPlay it f***ing loud,‚Äô and they did. The Beatles were the same way. They kept evolving, moving, refining their art. That is what I‚Äôve always tried to do. Keep moving. Otherwise, as Dylan says, ‚ÄòIf you‚Äôre not busy being born, you‚Äôre busy dying.‚Äô‚Äù</em></p>
<p><em>‚ÄúWhat drove me? I think most creative people want to express appreciation for being able to take advantage of the work that‚Äôs been done by others before us. I didn‚Äôt invent the language or mathematics I use. I make little of my own food, none of my own clothes. Everything I do depends on other members of our species and the shoulders that we stand on. And a lot of us want to contribute something back to our species and to add something to that flow. It‚Äôs about trying to express something in the only way that most of us know how. We try to use the talents we do have to express our deep feelings, to show our appreciation of all the contributions that came before us, and to add something to that flow. That is what has driven me.‚Äù</em></p>
<p><em>‚ÄúHe was not a model boss or human being, tightly packaged for emulation. Driven by demons, he would drive those around him to fury and despair. But his personality and passions and products were all interrelated. His tale is thus both instructive and cautionary, filled with lessons about innovation, character, leadership, and values.‚Äù</em></p>
<p><em>‚ÄúI don‚Äôt focus too much on being pragmatic. Logical thinking has its place but really go on intuition and emotion. I began to realize that an intuitive understanding and consciousness was far more significant than abstract thinking and intellectual logical analysis.‚Äù</em></p>
<p><em>‚ÄúWhatever he was interested in, he would generally carry to an irrational extreme.‚Äù</em></p>
<p><em>Charlie Munger says, ‚ÄúIn business, we often find that the winning system goes almost ridiculously far in maximizing or minimizing one or a few variables.‚Äù</em></p>
<p><em>‚ÄúHe made me do something I didn‚Äôt think I could do. It was the brighter side of what would become known as his reality distortion field. If you trust him, you can do things,‚Äù Holmes said. ‚ÄúIf he decided that something should happen, then he‚Äôs just going to make it happen.‚Äù</em></p>
<p><em>‚ÄúI taught him that if you act like you can do something, then it will work. I told him, pretend to be completely in control and people will assume that you are.‚Äù</em></p>
<p><em>‚ÄúJobs had a bravado that helped him get things done, occasionally by manipulating people. He could be charismatic, even mesmerizing, but also cold and brutal. Jobs was awed by Wozniak‚Äôs engineering wizardry and Wozniak was awed by Jobs‚Äô business strive. I never wanted to deal with people and step on toes. But Steve could call up people he didn‚Äôt know and make them do things.‚Äù</em></p>
<p><em>‚ÄúIn order to do a good job of those things that we decide to do, we must eliminate all the unimportant opportunities.‚Äù</em></p>
<p><em>‚ÄúThe world is a very malleable place. If you know what you want and you go forward with maximum energy and drive and passion, the world will often reconfigure itself around you much more quickly and easily than you would think.‚Äù</em></p>
<p><em>The reality distortion field was a confounding combination of a charismatic rhetorical style, indomitable will and an eagerness to bend any fact to fit the purpose at hand.</em></p>
<p><em>‚ÄúJobs is a strong world elitist artist, who doesn‚Äôt want his creations mutated inauspiciously by unworthy programmers. It would be as if someone off the street added some brush strokes to a Picasso painting or changed the lyrics to a Bob Dylan song.‚Äù</em></p>
<p><em>‚ÄúIf you want to live your life in a creative way, you have to not look back too much. You have to be willing to take whatever you‚Äôve done and whoever you were and throw them away. The more the outside world tries to reinforce an image of you, the harder it is to continue to be an artist, which is why a lot of times artists have to say, ‚ÄòBye, I have to go. I‚Äôm going crazy, and I‚Äôm getting out of here.‚Äô And then they go hybrid somewhere. Maybe later, they reemerge a little differently.‚Äù</em></p>
<p><strong>‚Äï  Founders #214 Steve Jobs: The Exclusive Biography</strong> [<a target="_blank" rel="noopener" href="https://podcasts.apple.com/us/podcast/founders/id1141877104?i=1000552824020">Link</a>]</p>
</blockquote>
<h3 id="Articles"><a href="#Articles" class="headerlink" title="Articles"></a><strong>Articles</strong></h3><blockquote>
<p><em>The qubit in superposition has some probability of being 1 or 0, but it represents neither state, just like our quarter flipping into the air is neither heads nor tails, but some probability of both. A quantum computer can use a collection of qubits in superpositions to play with different possible paths through a calculation. If done correctly, the pointers to incorrect paths cancel out, leaving the correct answer when the qubits are read out as Os and 1s.</em></p>
<p><em>Grover‚Äôs algorithm, a famous quantum search algorithm, could find you in a phone book of 100 million names with just 10,000 operations. If a classical search algorithm just spooled through all the listings to find you, it would require 50 million operations, on average.</em></p>
<p><em>Qubits have to be carefully shielded, and operated at very cold temperatures-sometimes only fractions of a degree above absolute zero. A major area of research involves developing algorithms for a quantum computer to correct its own errors, caused by glitching qubits.</em></p>
<p><em>Some researchers, most notably at Microsoft, hope to sidestep this challenge by developing a type of qubit out of clusters of electrons known as a topological qubit. Physicists predict topological qubits to be more robust to environmental noise and thus less error-prone, but so far they‚Äôve struggled to make even one.</em></p>
<p><em>Teams in both the public and private sector are betting so, as Google, IBM, Intel, and Microsoft have all expanded their teams working on the technology, with a growing swarm of startups such as Xanadu and QuEra in hot pursuit. The US, China, and the European Union each have new programs measured in the billions of dollars to stimulate quantum R&amp;D. Some startups, such as Rigetti and lonQ, have even begun trading publicly on the stock market by merging with a so-called special-purpose acquisition company, or SPAC-a trick to quickly gain access to cash.</em></p>
<p><em>Chemistry simulations may be the first practical use for these prototype machines, as researchers are figuring out how to make their qubits interact like electrons in a molecule. Daimler and Volkswagen have both started investigating quantum computing as a way to improve battery chemistry for electric vehicles. Microsoft says other uses could include designing new catalysts to make industrial processes less energy intensive, or even pulling carbon dioxide out of the atmosphere to mitigate climate change. Tech companies like Google are also betting that quantum computers can make artificial intelligence more powerful.</em></p>
<p><em>Big Tech companies argue that programmers need to get ready now. Google, IBM, and Microsoft have all released open source tools to help coders familiarize themselves with writing programs for quantum hardware. IBM offers online access to some of its quantum processors, so anyone can experiment with them. Launched in 2019, Amazon Web Services offers a service that connects users to startup-built quantum computers made of various qubit types over the cloud. In 2020, the US government launched an initiative to develop a K-12 curriculum relating to quantum computing. That same year, the University of New South Wales in Australia offered the world‚Äôs first bachelor‚Äôs degree in quantum engineering.</em></p>
<p><strong>‚Äï  Wired Guide to Quantum Computing</strong> [<a target="_blank" rel="noopener" href="https://www.wired.com/story/wired-guide-to-quantum-computing/">Link</a>]</p>
</blockquote>
<p>This article is pretty comprehensive in describing quantum computing mechanism and techniques. One interesting fact is that quantum computers are on the verge of breaking into bank accounts and breaking encryption and cryptography. Shor‚Äôs algorithm has been proven mathematically that if you had a large enough quantum computer, you could find the prime factor of large numbers - the basis of RSA encryption, the most commonly used thing on the internet. Although we are far away from being able to have a quantum computer big enough to execute Shor‚Äôs algorithm on that scale, cryptography research has already been preparing for quantum computers‚Äô code-breaking capabilities.</p>
<h3 id="News"><a href="#News" class="headerlink" title="News"></a><strong>News</strong></h3><blockquote>
<p><em>Neuralink‚Äôs brain-computer interface, or BCI, would allow people to control a computer or mobile device wirelessly ‚Äújust by thinking about it,‚Äù according to the company‚Äôs website.</em> </p>
<p><em>The goal of the new technology is to allow paralyzed people the ability to control a computer cursor or keyboard using just their thoughts.</em> </p>
<p><em>Beyond helping paralyzed patients regain some mobility and communicate without typing, Neuralink‚Äôs longer-term goals include helping restore full mobility and sight.</em></p>
<p><strong>‚Äï  First human to receive Neuralink brain implant is ‚Äòrecovering well,‚Äô Elon Musk says</strong> [<a target="_blank" rel="noopener" href="https://www.usatoday.com/story/tech/news/2024/01/30/neuralink-implant-elon-musk/72404741007/">Link</a>]</p>
</blockquote>
<blockquote>
<p><em>Biderman notes that the leak is likely harmful in terms of reducing trust between companies like Meta and the academics they share their research with. ‚ÄúIf we don‚Äôt respect people‚Äôs good faith attempts to disseminate technology in ways that are consistent with their legal and ethical obligations, that‚Äôs only going to create a more adversarial relationship between the public and researchers and make it harder for people to release things,‚Äù she notes.</em></p>
<p><strong>‚Äï  Meta‚Äôs powerful AI language model has leaked online ‚Äî what happens now?</strong> [<a target="_blank" rel="noopener" href="https://www.theverge.com/2023/3/8/23629362/meta-ai-language-model-llama-leak-online-misuse">Link</a>]</p>
</blockquote>
<p>Meta is taking the lead of open-source LLM by releasing the AI language model LLaMA. Some say open source is necessary to ensure AI safety and faster LLM progress. Others argue that there will be more personalized spam and phishing due to the fact of the model has already leaked on 4chan, and a wave of malicious use of AI. There are pros and cons of open sourcing LLM, just like last year OpenAI open sourced Stable Diffusion which has a lot of bad potential influences. But while every is making AI models private, there has to be someone who makes it public, then everyone goes public. The good and necessary thing is that open source software can help decentralize AI power.</p>
<blockquote>
<p><em>The OpenAI chief executive officer is in talks with investors including the United Arab Emirates government to raise funds for a wildly ambitious tech initiative that would boost the world‚Äôs chip-building capacity, expand its ability to power AI, among other things, and cost several trillion dollars, according to people familiar with the matter. The project could require raising as much as $5 trillion to $7 trillion, one of the people said.</em> </p>
<p><strong>‚Äï  Sam Altman Seeks Trillions of Dollars to Reshape Business of Chips and AI</strong> [<a target="_blank" rel="noopener" href="https://www.wsj.com/tech/ai/sam-altman-seeks-trillions-of-dollars-to-reshape-business-of-chips-and-ai-89ab3db0">Link</a>]</p>
</blockquote>
<blockquote>
<p><em>‚ÄúSora has a deep understanding of language, enabling it to accurately interpret prompts and generate compelling characters that express vibrant emotions,‚Äù OpenAI writes in a blog post. ‚ÄúThe model understands not only what the user has asked for in the prompt, but also how those things exist in the physical world.‚Äù</em></p>
<p><em>‚Äú[Sora] may struggle with accurately simulating the physics of a complex scene, and may not understand specific instances of cause and effect. For example, a person might take a bite out of a cookie, but afterward, the cookie may not have a bite mark. The model may also confuse spatial details of a prompt, for example, mixing up left and right, and may struggle with precise descriptions of events that take place over time, like following a specific camera trajectory.‚Äù</em></p>
<p><strong>‚Äï  OpenAI‚Äôs newest model Sora can generate videos ‚Äî and they look decent</strong> [<a target="_blank" rel="noopener" href="https://techcrunch.com/2024/02/15/openais-newest-model-can-generate-videos-and-they-look-decent/">Link</a>]</p>
<p><em>The predictor in this Joint Embedding Predictive Architecture serves as an early physical world model: You don‚Äôt have to see everything that‚Äôs happening in the frame, and it can tell you conceptually what‚Äôs happening there.</em> </p>
<p><strong>‚Äï  V-JEPA: The next step toward Yann LeCun‚Äôs vision of advanced machine intelligence (AMI)</strong> <a target="_blank" rel="noopener" href="https://ai.meta.com/blog/v-jepa-yann-lecun-ai-model-video-joint-embedding-predictive-architecture/">Link</a>]</p>
</blockquote>
<p>OpenAI released amazing technology again! Compared to other release language models, Sora seems to start to have the capability of understanding physical world, but OpenAI acknowledged that that might not be true. In the meantime, Meta developed V-JEPA, which is not focusing on linking language to videos, but learning the cause and effect from videos and gaining the capability of understand and reason the object-object interactions in the physical world. </p>
<blockquote>
<p><strong>Our next-generation model: Gemini 1.5</strong> [<a target="_blank" rel="noopener" href="https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/">Link</a>]</p>
</blockquote>
<p>Google‚Äôs Gemini 1.5 Pro employs a Mixture-of-Experts (MoE) architecture which helps the model to process large datasets by activating relevant neural network segments. It‚Äôs capable of managing up  to 1M tokens - equivalent to 700000 words, one hour of video, or 11 hours of audio. What‚Äôs exciting is that it leverages a transformer-based architecture with a specifically designed long context window, which allows it to remember and process vast amounts of information. It‚Äôs able to achieve tasks like summarizing lectures from lengthy videos. It‚Äôs really able to retrieve ‚Äòneedles‚Äô from a ‚Äòhaystack‚Äô of millions of tokens across different structures of data sources with accuracy of 99%.</p>
<blockquote>
<p>Other news:</p>
<p><strong>Nvidia Is Now More Valuable Than Amazon And Google</strong> [<a target="_blank" rel="noopener" href="https://www.forbes.com/sites/dereksaul/2024/02/12/nvidia-is-now-more-valuable-than-amazon-and-google/?sh=21b365b54554">Link</a>]</p>
<p><strong>Nvidia Hits $2 Trillion Valuation on Insatiable AI Chip Demand</strong> [<a target="_blank" rel="noopener" href="https://www.wsj.com/tech/ai/nvidia-stock-market-cap-2-trillion-b1c839c8">Link</a>]</p>
<p><strong>Elon Musk Says Neuralink‚Äôs First Brain Chip Patient Can Control Computer Mouse By Thought</strong> [<a target="_blank" rel="noopener" href="https://www.forbes.com/sites/roberthart/2024/02/20/elon-musk-says-neuralinks-first-brain-chip-patient-can-control-computer-mouse-by-thought/">Link</a>]</p>
<p><strong>Capital One to Acquire Discover, Creating a Consumer Lending Colossus</strong> [<a target="_blank" rel="noopener" href="https://www.nytimes.com/2024/02/19/business/capital-one-discover-merger.html">Link</a>]</p>
<p><strong>White House touts $11 billion US semiconductor R&amp;D program</strong> [<a target="_blank" rel="noopener" href="https://www.reuters.com/technology/us-announces-over-5-bln-investments-semiconductor-related-research-development-2024-02-09/">Link</a>]</p>
<p><strong>Meta to deploy custom-designed Artemis AI processor alongside commercial GPUs</strong> [<a target="_blank" rel="noopener" href="https://www.tomshardware.com/tech-industry/meta-to-deploy-custom-designed-artemis-ai-processor-alongside-commercial-gpus">Link</a>]</p>
</blockquote>
<h3 id="Substack"><a href="#Substack" class="headerlink" title="Substack"></a><strong>Substack</strong></h3><blockquote>
<p><strong>Alphabet Cloud Rebound - App Economy Insights</strong>  [<a target="_blank" rel="noopener" href="https://www.appeconomyinsights.com/p/alphabet-cloud-rebounds">Link</a>]</p>
</blockquote>
<p>Google Cloud (GCP and Workspace) revenue growth reaccelerated by 4 percentage points, while AWS and Azure show softer momentum. Key business highlights: 1) Gemini in search for faster Search Generative Experience (SGE), 2) Conversational AI tool Bard now powered by Gemini Pro and will be powered by Gemini Ultra, 3) YouTube now has over 100M subscribers across Music and Premium, 4) Cloud driven by AI - Vertex AI platform and Duet AI agents, leads to expand relationships with many leading brands (e.g. Hugging Face, McDonald‚Äôs, Motorola Mobility, Verizon. ), 5) Waymo reached over 1M fully autonomous ride-hailing trips, 6) Isomorphic Labs partnered with Eli Lilly and Novartis to apply AI to treat diseases.</p>
<p>AI specific business highlights: 1) Google is transforming searching behavior of customers: Search Generative Experience (SGE) is introducing a dynamic AI enhanced search experience, 2) Gemini includes Gemini Nano, Gemini Pro, and Gemini Ultra. Gemini Nano is optimized for on-device tasks and already available on Pixel 8 phone. Gemini Pro is currently in early preview through Cloud and specific apps. Gemini Ultra will be released later in 2024, 3) the conversational AI - Bard - might be exclusive to Tensor-powered Pixel phones and will be accessible through voice commands or double-tapping device side buttons. Bard will also be integrated with apps (e.g. Gmail, Maps, Drive) and Camera on Android phones.</p>
<blockquote>
<p><strong>Amazon: Ads Take the Cake - App Economy Insights</strong> [<a target="_blank" rel="noopener" href="https://www.appeconomyinsights.com/p/amazon-ads-take-the-cake">Link</a>]</p>
</blockquote>
<p>Key updates on Amazon business: 1) Infrastructure: Amazon has developed customized ML chips e.g. Trainium for training and Inferentia for inference. Additionally, it offers Graviton for generalized CPU chips, and launched Trainium2 with four times training performance. 2) Model: Bedrock is the LLM as a Service, allowing customers to run foundational models, customize them and create agents for automated tasks and workflows. 3) Apps: Amazon Q is a workplace-focused generative AI chatbot. It‚Äôs designed for business to assist with summarizing docs and answering internal questions. It‚Äôs built with high security and privacy, and integrated with Slack, Gmail, etc. </p>
<p>What else to watch: 1) Cloud: Gen AI benefits Amazon (AWS) as well as existing market leaders in cloud infrastructure. 2) Project Kuiper is an initiative to increase global broadband access through a constellation of 3,236 satellites in low Earth orbit (LEO). Amazon is on track of launching it in the first half of 2024 and will start beta testing in the second half of the year. 3) Prime Video (with ads) remains a large and profitable business. 4) Investment in live sports as a critical customer acquisition strategy. </p>
<p>What is coming: 1) Rufus - a Gen AI-powered shopping assistant with conversational AI capabilities 2) amazon‚Äôs advertising revenue is catching up Meta and Google, with Prime Video a probable accelerator. </p>
<blockquote>
<p><strong>Meta: The Zuck ‚ÄòPlaybook‚Äô - App Economy Insights</strong> [<a target="_blank" rel="noopener" href="https://www.appeconomyinsights.com/p/meta-the-zuck-playbook">Link</a>]</p>
</blockquote>
<p>The Zuck Playbook: 1) Massive compute investment, 2) open-source strategy, 3) future-focused research, 4) data and feedback utilization, 5) experimentation culture, 6) growth before monetization.</p>
<p>Meta‚Äôs business segments: 1) Family of Apps (Facebook, Instagram, Messenger, and WhatsApp), 2) Reality Labs (virtual reality hardware and supporting software).</p>
<p>Key business highlights: 1) 1B+ revenue in Q4 2023 for the first time with Quest, and Quest 3 is off to a strong start, 2) established the right feedback loops with Stories and Reels to test new features and products, 3) Ray-Ban Meta smart glasses is off to a strong start. 4) Reels and Threads are growing. 5) Llama 3 and AGI. Zuck is aiming to position Meta as a leader in the field  of AI without necessarily monopolizing control over it.</p>
<blockquote>
<p><strong>Microsoft: AI at Scale - App Economy Insights</strong> [<a target="_blank" rel="noopener" href="https://www.appeconomyinsights.com/p/microsoft-ai-at-scale">Link</a>]</p>
</blockquote>
<p>Key business highlights: 1) AI‚Äôs impact on Azure‚Äôs growth: most essential revenue growth drivers are Azure OpenAI and OpenAI APIs, 2) small language models: Orca 2 leverages Meta‚Äôs Llama 2 base models, fine tuned with synthetic data, and Phi 2 is a transformer-based SLM designed for cloud and edge deployment, 3) new custom AI chips: Microsoft‚Äôs first custom chips - Maia and Cobalt. Maia 100 GPU is tailored for AI workloads, Cobalt 100 powers general cloud services, 4) rebranding Bing Chat as Copilot. 5) introducing a new key (Copilot key) on keyboard to Windows 11 PCs.</p>
<p>What‚Äôs else in Microsoft‚Äôs portfolio: 1) Azure AI services gain more new customers, 2) Github Copilot revenue accelerated, 3) Microsoft 365 Copilot show faster customer adoption, 4) LinkedIn, 5) Search - not gaining market share in Search, 6) Gaming: with acquisition of Activision Blizzard, hundreds of millions of gamers are added in to the ecosystem. Innovation of cloud gaming improves player experience. 7) Paid Office 365 commercial seats.</p>
<blockquote>
<p><em>The Digital Markets Act (DMA) is a European Union regulation designed to promote fair competition and innovation in the digital sector by preventing large tech companies from monopolizing the market. It aims to ensure consumers have more choices and access to diverse digital services by regulating the practices of platforms acting as digital ‚Äúgatekeepers.‚Äù</em></p>
<p><strong>‚Äï  Apple App Store Shenanigans - App Economy Insights</strong> [<a target="_blank" rel="noopener" href="https://www.appeconomyinsights.com/p/apple-app-store-shenanigans">Link</a>]</p>
</blockquote>
<p>Recent news highlights: 1) as DMA compliance, Apple will allow for third-party stores and payment systems to App Store in Europe, so the developers can avoid 30% fee from Apple, 2) EU fines Apple ‚Ç¨500M for unfairly competing with Spotify by restricting it from linking out to its own website for subscriptions. These anticompetitive practices by favoring its services over rivals have a bad impact on Apple‚Äôs reputation. 3) revenue from China is continuously declining.</p>
<p>What is coming: 1) Vision Pro has 600+ native apps and games, and is supported by mixed streaming (Disney+, Prime Video). But Netflix and YouTube have held back. TikTok launched a native Vision Pro App tailored for an immersive viewing experience. 2) AI.</p>
<blockquote>
<p><em>In a context where the crux of the thesis is the durability of the demand for NVIDIA‚Äôs AI solutions, inference will likely become more crucial to future-proof the business.</em></p>
<p><em>Jensen Huang previously described a ‚Äògenerative AI wave‚Äô from one category to the next:</em></p>
<ol>
<li><em>‚Üí Startups and CSPs.</em></li>
<li><em>‚Üí Consumer Internet.</em></li>
<li><em>‚Üí Software platforms.</em></li>
<li><em>‚Üí Enterprise and government.</em></li>
</ol>
<p><em>Huang continues to see three massive tailwinds:</em></p>
<ol>
<li><em>Transition from general-purpose to accelerated computing</em></li>
<li><em>Generative AI.</em></li>
<li><em>A whole new industry (think ChatGPT, Midjourney, or Gemini).</em></li>
</ol>
<p><em>History tells us that highly profitable industries tend to attract more competition, leading to mean reversion for the best performers.</em></p>
<p><strong>‚Äï  Nvidia at Tipping Point - App Economy Insights</strong> [<a target="_blank" rel="noopener" href="https://www.appeconomyinsights.com/p/nvidia-ai-tipping-point">Link</a>]</p>
</blockquote>
<p>AI system operate through two core stages: training and inference. Nvidia dominates the Training segment with its robust GPU but faces stiffer competition in the Inference segment with Intel, Qualcomm, etc.</p>
<p>Nvidia‚Äôs equity portfolio: Arm Holdings (ARM), Recursion Pharmaceuticals (RXRX), SoundHound AI (SOUN), TuSimple (TSPH), Nano-X Imaging (NNOX), showing Nvidia‚Äôs expansive approach to AI.</p>
<p>Microsoft developed custom AI chips Maia and Cobalt to lessen reliance on Nvidia and benefit OpenAI. This shows a desire for self-reliance across Nvidia‚Äôs largest customers, which could challenge the company‚Äôs dominance in AI accelerators.</p>
<p>Key business highlights: 1) Nvidia has three major customer categories: Cloud Service Providers (CSPs) for all hyperscalers (Amazon, Microsoft, Google), consumer internet companies such as Meta who invested in 350000 H100s from Nvidia, and enterprise such as Adobe, Databricks, and Snowflake who are adding AI copilots to their platforms. 2) Sovereign AI. </p>
<blockquote>
<p><strong>Reddit IPO: Key Takeaways - App Economy Insights</strong> [<a target="_blank" rel="noopener" href="https://www.appeconomyinsights.com/p/reddit-ipo-key-takeaways">Link</a>]</p>
</blockquote>
<h3 id="YouTube"><a href="#YouTube" class="headerlink" title="YouTube"></a>YouTube</h3><blockquote>
<p><strong>Apple Vision Pro Vs. Meta Quest: The Ultimate Showdown</strong> [<a target="_blank" rel="noopener" href="https://youtu.be/u2rAScMGwo0">Link</a>]</p>
<p><strong>Apple Vision Pro review: magic, until it‚Äôs not</strong> [<a target="_blank" rel="noopener" href="https://youtu.be/hdwaWxY11jQ">Link</a>] [<a target="_blank" rel="noopener" href="https://www.theverge.com/24054862/apple-vision-pro-review-vr-ar-headset-features-price">Link</a>]</p>
</blockquote>
<p>Apple Vision Pro launched on Feb 2, 2024 at $3,499. It‚Äôs interesting that Meta and Apple are starting on opposite ends of the spectrum. Meta quest has the right price and will try to improve the technology overtime. Apple Vision Pro has the right technology and will try to lower the price overtime. With a price of 3499, Apple is targeting the high end of the market, not aiming for a mass market product in the first iteration. Instead their sights are set on the early adaptors. It‚Äôs a pattern that most Apple products take several years to achieve the mass production. The first iteration of iPhone in 2007 was a soft launch. iPhone didn‚Äôt crack 10M units per quarter until the iPhone 4 in late 2010. Now Apple sells about 200M iPhones every year. So it‚Äôs highly possible that mass adoption of technologically improved mixed-reality headsets with more affordable pricing is coming in a decade.</p>
<blockquote>
<p><strong>Microsoft Game Day Commercial | Copilot: Your everyday AI companion</strong> [<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=SaCVSUbYpVc&ab_channel=Microsoft">Link</a>]</p>
</blockquote>
<p>Microsoft‚Äôs first Super Bowl Commercial to highlight its transformation into an AI-centric company, with a focus on Copilot‚Äôs ability of simplifying coding and digital art creation, etc.</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://jokerdii.github.io/digital-di/2024/01/29/2024-January/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/digital-di/images/avatar.gif">
      <meta itemprop="name" content="Di Zhen">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Di's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Di's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/digital-di/2024/01/29/2024-January/" class="post-title-link" itemprop="url">2024 January</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2024-01-29 00:20:44" itemprop="dateCreated datePublished" datetime="2024-01-29T00:20:44-05:00">2024-01-29</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-05-25 10:11:54" itemprop="dateModified" datetime="2024-05-25T10:11:54-04:00">2024-05-25</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="Podcast"><a href="#Podcast" class="headerlink" title="Podcast"></a><strong>Podcast</strong></h3><blockquote>
<p><em>Never ever think about something else when you should be thinking about the power of incentives.</em></p>
<p><em>Fanaticism and scale combined can be very powerful.</em></p>
<p><em>Invert always invert, you could innovate by doing the exact opposite of your competitors.</em></p>
<p><em>Once you get on the ball, stay on the ball. And once you start down, it is mighty hard to turn around.</em></p>
<p><em>Success in my mind comes from having a successful business, one that is a good place to work, one that offers opportunity for people, and one that you could be proud of to own.</em></p>
<p><em>Whatever you do, you must do it with gusto, you must do it in volume. It is a case of repeat, repeat, repeat.</em></p>
<p><em>Extreme success is likely to be caused by some combination of the following factors. Number one, extreme maximization or minimization of one or two variables. Number two, adding success factors so that a bigger combination drives success, often in a nonlinear fashion. Number three, an extreme of good performance over many factors. And finally, four, catching and riding some sort of wave.</em></p>
<p><strong>‚Äï  Founders #330 Les Schwab</strong> [<a target="_blank" rel="noopener" href="https://podcasts.apple.com/us/podcast/founders/id1141877104?i=1000638192567">Link</a>]</p>
</blockquote>
<blockquote>
<p><em>Learning is not memorizing information, learning is changing your behavior.</em></p>
<p><em>Troubles from time to time should be expected. They are an inescapable part in life. So why let them bother you, just handle them and then move on.</em></p>
<p><em>You are not changing human nature things will just keep repeating forever.</em></p>
<p><em>You need to do your best to avoid problems and the way you do that is you go for great. It‚Äôs hard to do but it makes your life easier if you go for great. Great businesses are rare, great people are rare. But it worth the time to find. Great businesses threw off way less problems than average or low quality businesses, just like great people cause way less problems in life than average or low quality people.</em></p>
<p><strong>‚Äï  Founders #329 Charlie Munger</strong> [<a target="_blank" rel="noopener" href="https://podcasts.apple.com/us/podcast/founders/id1141877104?i=1000637562249">Link</a>]</p>
</blockquote>
<blockquote>
<p><em>Opportunity is a strange beast. It frequently appears after a loss.</em></p>
<p><em>When you read biographies of people who‚Äôve done great work, it is remarkable how much luck is involved. They discover what to work on as a result of chance meeting, or by reading a book, they happen to pick up. So you need to make yourself a big target for luck. And the way to do that is to be curious.</em></p>
<p><em>It‚Äôs the impossibility of making something new out of something old. In a trade where novelty is all important, I decided that I was not meant by nature to raise corpses from the dead.</em></p>
<p><em>I think of my work as a femoral architecture dedicated to the beauty of the female body.</em></p>
<p><em>The entrepreneur only ever experiences two states, that of euphoria and terror.</em></p>
<p><em>My life, in fact, revolves around the preparation of a collection with its torments and happiness. I know that in spite of all the delights of a vacation, it will seem an intolerable gap. My thoughts stay with my dresses. It is now that I like to sit down in front of my dresses, gaze at them a last time altogether and thank them from the bottom of my heart.</em></p>
<p><strong>‚Äï  Founders #331 Christian Dior</strong> [<a target="_blank" rel="noopener" href="https://podcasts.apple.com/us/podcast/founders/id1141877104?i=1000638880814">Link</a>]</p>
</blockquote>
<blockquote>
<p><em>The goal is to not have the longest train, but to arrive at the station first using the least fuel.</em></p>
<p><em>Find your edge, don‚Äôt diversify, and never repeat what works.</em></p>
<p><em>The formula that allowed Murphy to overtake Paley was deceptively simple: number one, focus on industries with attractive economic characteristics; number two, selectively use leverage to buy occasional large properties; number three, improve operations; number four, pay down debt; and number five, repeat this loop.</em></p>
<p><em>The behavior of peer companies will be mindlessly imitated.</em></p>
<p><em>The business of business is a lot of little decisions every day mixed up with a very few big decisions.</em></p>
<p><em>Stay in the game long enough to get lucky.</em></p>
<p><em>The outsider CEOs shared an unconventional approach, one that emphasized flat organizations and dehydrated corporate staffs.</em></p>
<p><em>Decentralization is the cornerstone of our philosophy. Our goal is to hire the best people we can and give them the responsibility and authority they need to perform their jobs. We expect our managers to be forever cost conscious and to recognize and exploit sales potential.</em></p>
<p><em>Headquarters staff was anorexic. No vice presidents in functional areas like marketing, strategic planning, or human resources. No corporate counsel and no public relations department either. In the Capital Cities culture, the publishers and station managers had the power and the prestige internally, and they almost never heard from New York if they were hitting their numbers.</em></p>
<p><em>The company‚Äôs guiding human resource philosophy: Hire the best people you can and leave them alone.</em></p>
<p><em>Murphy delegates to the point of Anarchy. Frugality was also central to the Ethos.</em></p>
<p><em>Murphy and Burke realized early on that while you couldn‚Äôt control your revenues, you can control your costs. They believed that the best defense against the revenue lumpiness inherent in advertising-supported businesses was a constant vigilance on costs, which became deeply embedded in the company culture.</em></p>
<p><strong>‚Äï  Founders #328 Tom Murphy</strong> [<a target="_blank" rel="noopener" href="https://podcasts.apple.com/us/podcast/founders/id1141877104?i=1000635617173">Link</a>]</p>
</blockquote>
<blockquote>
<p><em>Life is like a big lake. All the boys get in the water at one end and start swimming. Not all of them will swim across. But one of them I assure will and that is Gr√≥f.</em></p>
<p><strong>‚Äï  Founders #159 Andy Grove</strong> [<a target="_blank" rel="noopener" href="https://podcasts.apple.com/us/podcast/founders/id1141877104?i=1000503106174">Link</a>]</p>
</blockquote>
<blockquote>
<p><em>The money will come as a byproduct of building great products and building a great organization, but you absolutely cannot put that first or you‚Äôre dooming yourself.</em></p>
<p><em>I was worth about $1 million when I was 23. I was worth $10 million when I was 24, and I was worth over $100 million when I was 25. And it wasn‚Äôt that important because I never did it for the money.</em></p>
<p><em>I‚Äôm looking for a fixer-upper with a solid foundation. I am willing to tear down walls, build bridges, and light fires. I have great experience, lots of energy, a bit of that vision thing, and I‚Äôm not afraid to start from the beginning.</em></p>
<p><em>Apple is about people who think outside the box, people who want to use computers to help them change the world, to help them create things that make a difference and not just get a job done.</em></p>
<p><em>As technology becomes more and more complex, Apple‚Äôs core strength of knowing how to make very sophisticated technology, comprehensible to mere mortals is in even greater demand.</em></p>
<p><em>Be a yardstick of quality. Some people are not used to an environment where excellence is expected.</em></p>
<p><em>Design is a funny word. Some people think design means how it looks. But of course, if you dig deeper, it‚Äôs really how it works. The design of the Mac wasn‚Äôt what it looked like, although, that was part of it. Primarily, it was how it worked. To design something really well, you have to get it. You have to really grok what it‚Äôs all about. It takes a passionate commitment to really, thoroughly understand something, to chew it up, not just quickly swallow it.</em></p>
<p><em>Simplicity is complexity resolved. Once you get into the problem, you see that it‚Äôs complicated, and then you come up with all these convoluted solutions. That‚Äôs where most people stop. And the solutions tend to work for a while, but the really great person will keep going.</em></p>
<p><em>I always considered a part of my job was to keep the quality level of people in the organizations I work with very high. That‚Äôs what I consider one of the few things that actually can contribute individually to, to really try to instill in the organization the goal of only having A players.</em></p>
<p><em>The people who are doing the work are the moving force behind the Macintosh. My job is to create a space for them to clear out the rest of the organization and keep it at bay.</em></p>
<p><em>I‚Äôve always been attracted to the more revolutionary changes. I don‚Äôt know why. Because they‚Äôre harder. They‚Äôre just much more stressful emotionally. And you usually go through a period where everyone tells you that you‚Äôve completely failed.</em></p>
<p><em>‚ÄúI could see what the Polaroid camera should be. It was just as real to me as it was ‚Äì as if it was sitting in front of me before I had ever built one.‚Äù</em></p>
<p><em>And Steve said, ‚ÄúYes. That‚Äôs exactly the way I saw the Macintosh. If I ask someone who had only use a personal calculator, what a Macintosh should be, they couldn‚Äôt have told me. There was no way to do consumer research on it. I had to go and create it and then show it to the people and say, now what do you think?‚Äù</em></p>
<p><em>Both of them had this ability to, well, not invent products, but discover products. Both of‚Äù ‚Äì this is wild, man. ‚ÄúBoth of them said these products had always existed. It‚Äôs just that no one has ever seen them before. We were the ones who discovered them. The polaroid camera had always existed, and the Macintosh had always existed. It was a matter of discovery. Steve had huge admiration for Dr. Land. He was fascinated by him.</em></p>
<p><em>‚ÄúJobs had said several times that he thinks technological creativity and artistic creativity are two sides of the same coin. When asked about the differences between art and technology, he said, ‚ÄòI‚Äôve never believed that they‚Äôre separate.‚Äô Leonard da Vinci was a great artist and a great scientist. Michelangelo knew a tremendous amount about how to cut stones at a quarry, not just how to make a sculpture, right?‚Äù</em></p>
<p><em>‚ÄúI don‚Äôt believe that the best people in any of these fields see themselves as one branch of a forked tree. I just don‚Äôt see that. People bring these ideas together a lot. Dr. Land at Polaroid said, ‚ÄòI want Polaroid to stand at the intersection of art and science, and I‚Äôve never forgotten that.‚Äô‚Äù</em></p>
<p><em>In 30 years since founding Apple, Jobs has remained remarkably consistent. The demand for excellence, the pursuit of great design, the instinct for marketing, the insistence on each ‚Äì on ease of use and compatibility, all have been there from the get-go.</em></p>
<p><em>‚ÄúThe things that Jobs cares about, design, ease of use, good advertising, are right in the sweet spot of the new computer industry. Apple is the only company left in this industry that designs the whole thing,‚Äù Jobs said.</em></p>
<p><em>‚ÄúHardware, software, developer relations, marketing. It turns out that, in my opinion, that is Apple‚Äôs greatest strategieec advantage. It is Apple‚Äôs core strategic advantage. If you believe that there‚Äôs still room for innovation in this industry, which I do, because Apple can then innovate faster than anyone else. The great thing is that Apple‚Äôs DNA hasn‚Äôt changed,‚Äù Jobs said. ‚ÄúThe place where Apple has been standing for the last two decades is exactly where computer technology and the consumer electronics markets are converging. So it‚Äôs not like we‚Äôre having to cross the river to go somewhere else. The other side of the river is coming to us.‚Äù</em></p>
<p><strong>‚Äï  Founders #204 Steve Jobs</strong> [<a target="_blank" rel="noopener" href="https://podcasts.apple.com/us/podcast/founders/id1141877104?i=1000535289598">Link</a>]</p>
</blockquote>
<h3 id="Articles"><a href="#Articles" class="headerlink" title="Articles"></a><strong>Articles</strong></h3><blockquote>
<p><em>Plans should be measured in decades, execution should be measured in weeks.</em></p>
<p><strong>‚Äï  Sam Altman‚Äôs Blog</strong> [<a target="_blank" rel="noopener" href="https://blog.samaltman.com/what-i-wish-someone-had-told-me">Blog</a>]</p>
</blockquote>
<h3 id="News"><a href="#News" class="headerlink" title="News"></a><strong>News</strong></h3><blockquote>
<p><em>A topological qubit is a system that encodes data into the properties of pairs of non-Abelian anyons by physically swapping them around with each other in space. Non-Abelian anyons is desirable component for holding and manipulating information in quantum computers because of its resilience which is rooted in math from the field of topology - the study of spatial relationships and geometry that hold true even when shapes are distorted.</em></p>
<p><strong>‚Äï  The Holy Grail of Quantum Computing, Weird</strong> [<a target="_blank" rel="noopener" href="https://www.wired.com/story/the-holy-grail-of-quantum-computing/">Link</a>]</p>
</blockquote>
<p>Researcher from Google and Quantinuum demonstrated a mechanism needed for a component called a topological qubit, which should promise a means to maintain and manipulate information encoded into quantum states more robustly than existing hardware designs. Topological qubits store and work with digital information in non-Abelian anyons, which retain a sort of ‚Äúmemory‚Äù of their past movement that enables the representation of binary data. While physicists previous proved the existence of non-Abelian anyons, Quantinuum‚Äôs and Google‚Äôs work are the first to demonstrate their signature feature, memory of movement. However, people disagreed that topological qubit has been created because the object was too fragile for practical use and cannot reliably manipulate information to achieve practical quantum computing. Delivering a practical topological qubit will require all kinds of studies of non-Abelian anyons and the math underpinning their quirky behavior. Technological breakthrough can be expected after incremental progress.</p>
<p>Google Quantum Al‚Äôs paper published in May 2023 is <a target="_blank" rel="noopener" href="https://www.nature.com/articles/s41586-023-05954-4">here</a> and no I‚Äôm not going to read this theoretical physics paper :)</p>
<blockquote>
<p><em>Apple‚Äôs rivals, such as Samsung, are gearing up to launch a new kind of ‚ÄúAI smartphone‚Äù next year. Counterpoint estimated more than 100 million AI-focused smartphones would be shipped in 2024, with 40 percent of new devices offering such capabilities by 2027.</em></p>
<p><em>Optimizing LLMs to run on battery-powered devices has been a growing focus for AI researchers. Academic papers are not a direct indicator of how Apple intends to add new features to its products, but they offer a rare glimpse into its secretive research labs and the company‚Äôs latest technical breakthroughs.</em></p>
<p><em>‚ÄúOur work not only provides a solution to a current computational bottleneck but also sets a precedent for future research,‚Äù wrote Apple‚Äôs researchers in the conclusion to their paper. ‚ÄúWe believe as LLMs continue to grow in size and complexity, approaches like this work will be essential for harnessing their full potential in a wide range of devices and applications.‚Äù</em></p>
<p><strong>‚Äï  Apple wants AI to run directly on it hardware instead of in the cloud</strong> [<a target="_blank" rel="noopener" href="https://arstechnica.com/apple/2023/12/apple-wants-ai-to-run-directly-on-its-hardware-instead-of-in-the-cloud/?utm_social-type=owned&utm_source=twitter&utm_medium=social&utm_brand=ars">Link</a>]</p>
</blockquote>
<p>Apple is developing solutions to running LLM or other AI models directly on a customer‚Äôs iPhone. The published paper is titled ‚Äú<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2312.11514.pdf">LLM in a Flash</a>‚Äú. Apple‚Äôs focus is different from Microsoft‚Äôs and Google‚Äôs focus of developing Chatbots and GenAI over the Internet from cloud computing platform. I think the smartphone market would be revived and customer experience would be largely changed in the future with this potential vision of personalized and mobile AI agents &#x2F; assistants. If this personalized little AI agent can learn everything about a person from childhood to adult, can we eventually get a perfect adult mind? It reminds me of what Alan Turing said about AI: </p>
<p><em>‚ÄúInstead of trying to produce a program to simulate the adult mind, why not rather try to produce one which simulates the child‚Äôs? If this were then subjected to an appropriate course of education one would obtain the adult brain.‚Äù</em></p>
<blockquote>
<p><em>Since the beginning of 2017, China has chalked up more than 18 million EV sales, nearly half the world‚Äôs total and over four times more than the US, according to BloombergNEF data. By 2026, the research group projects that over 50% of all new passenger vehicle sales in China will be electric, compared to a little over a quarter in the US.</em></p>
<p><em>The growth of the network was both a result of state planning and private enterprise. Giant state-owned companies like State Grid Corp. of China were given mandates to roll out chargers, while private companies like Qingdao TGOOD Electric Co. jumped at the chance to build charging posts‚Äîin part to lay early claim to the best locations. Baidu‚Äôs mapping software‚Äîthe Chinese equivalent of Google Maps‚Äîhas them all integrated, delivering constant reminders of where to go. Payment is typically via an app or the ubiquitous WeChat platform.</em></p>
<p><em>Demand for new lithium-ion batteries is expected to increase about five-fold between 2023 and 2033, according to Julia Harty, an energy transition analyst at FastMarkets. Meeting that will require recycling as well as mining.</em></p>
<p><strong>‚Äï  Electric Cars Are Driving China Toward the End of the Age of Oil</strong> [<a target="_blank" rel="noopener" href="https://www.bloomberg.com/graphics/2023-china-ev-roadtrip-oil-turning-point/?cmpid=socialflow-twitter-business">Link</a>]</p>
</blockquote>
<p>I haven‚Äôt been back to China for 3 years, and last time (2021) there are just a few EVs. Maybe a few years later there are only a few gasoline cars left on the roads. The transition from gasoline cars to electrical vehicles won‚Äôt be fast but the peak of sales for gasoline cards is coming soon in China. </p>
<blockquote>
<p><em><a target="_blank" rel="noopener" href="https://archive.ph/o/s3Sfs/https://nytco-assets.nytimes.com/2023/12/NYT_Complaint_Dec2023.pdf">The lawsuit, filed in Federal District Court in Manhattan</a>, contends that millions of articles published by The Times were used to train automated chatbots that now compete with the news outlet as a source of reliable information.</em></p>
<p><em>In its complaint, The Times said it approached Microsoft and OpenAI in April to raise concerns about the use of its intellectual property and explore ‚Äúan amicable resolution,‚Äù possibly involving a commercial agreement and ‚Äútechnological guardrails‚Äù around generative A.I. products.</em></p>
<p><em>The lawsuit could test the emerging legal contours of generative A.I. technologies ‚Äî so called for the text, images and other content they can create after learning from large data sets ‚Äî and could carry major implications for the news industry.</em> </p>
<p><em>‚ÄúIf The Times and other news organizations cannot produce and protect their independent journalism, there will be a vacuum that no computer or artificial intelligence can fill,‚Äù the complaint reads. It adds, ‚ÄúLess journalism will be produced, and the cost to society will be enormous.‚Äù</em></p>
<p><strong>‚Äï  The Times Sues OpenAI and Microsoft Over A.I. Use of Copyrighted Work</strong> [<a target="_blank" rel="noopener" href="https://www.nytimes.com/2023/12/27/business/media/new-york-times-open-ai-microsoft-lawsuit.html">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Microsoft, OpenAI hit with new lawsuit by authors over AI training</strong> [<a target="_blank" rel="noopener" href="https://www.reuters.com/legal/microsoft-openai-hit-with-new-lawsuit-by-authors-over-ai-training-2024-01-05/">Link</a>]</p>
</blockquote>
<p>There is always a group of people who are positively working on changing the world while another group of people who are suspicious and concerned. Every lawsuit in AI field allows us to hold on and reflect whether we are doing the right things and how to fix the problems along the way of innovation and development. It is a good point that if intellectual property of journalism is not well-protect while AI is still in its immature stage with a lot of mistakes in text learning and generation, then less journalism will be produced, and less truths will be revealed and documented. The damage to the society is enormous.</p>
<blockquote>
<p><strong>DOJ close to filing massive antitrust suit against Apple over iPhone dominance: report</strong> [<a target="_blank" rel="noopener" href="https://nypost.com/2024/01/05/business/doj-close-to-filing-massive-antitrust-suit-against-apple-report/">Link</a>]</p>
</blockquote>
<p>Apple is under extensive investigation of DOJ for potential antitrust violations. Google currently faces multiple antitrust cases as well.</p>
<blockquote>
<p><em>Per the IRS, for-profit entities and not-for-profit entities are fundamentally at odds with each other, so in order to combine the two competing concepts, OpenAl came up with a novel structure which allowed the non-profit to control the direction of a for-profit entity while providing the investors a ‚Äúcapped‚Äù upside of 100x. This culminated in a $1Bn investment from Microsoft, marking the beginning of a key strategic relationship, but complicating the company‚Äôs organizational structure and incentives.</em></p>
<p><strong>‚Äï  Quick Essay: A Short History of OpenAI</strong> [<a target="_blank" rel="noopener" href="https://chamath.substack.com/p/a-short-history-of-openai">Link</a>]</p>
<p><em>‚ÄúI deeply regret my participation in the board‚Äôs actions,‚Äù Sutskever, a longtime Al researcher and cofounder of OpenAl, posted on X. ‚ÄúI never intended to harm OpenAl. I love everything we‚Äôve built together and l will do everything I can to reunite the company.‚Äù</em> </p>
<p><strong>‚Äï  4 days from fired to re-hired: A timeline of Sam Altman‚Äôs ouster from OpenAI</strong> [<a target="_blank" rel="noopener" href="https://abcnews.go.com/Business/sam-altman-reaches-deal-return-ceo-openai/story?id=105091534">Link</a>]</p>
<p>More news articles: [<a target="_blank" rel="noopener" href="https://www.forbes.com/sites/timabansal/2024/01/16/openai-or-anthropic-which-will-keep-you-more-safe/?sh=2766fc485122">Link</a>] [<a target="_blank" rel="noopener" href="https://abcnews.go.com/Business/sam-altman-reaches-deal-return-ceo-openai/story?id=105091534">Link</a>] [<a target="_blank" rel="noopener" href="https://www.forbes.com/sites/timabansal/2024/01/16/openai-or-anthropic-which-will-keep-you-more-safe/?sh=70f6ecd75122">Link</a>] [<a target="_blank" rel="noopener" href="https://www.forbes.com/sites/timabansal/2023/10/13/does-openais-non-profit-ownership-structure-actually-matter/?sh=2938c7027d18">Link</a>]</p>
</blockquote>
<p>This article (Quick Essay: A Short History of OpenAI) reviewed the history of OpenAl from 2015 when it‚Äôs founded to 2023 when Sam was once fired. A crucial development happened in 2018 when the company first introduced the foundational architecture of GPT in a paper ‚ÄúImproving Language Understanding by Generative Pre-Training‚Äù. This leads to the flagship product of the company - ChatGPT, in Nov 2022. In 2019 OpenAl transited from nonprofit to a ‚Äúcapped-profit‚Äù model. This novel convoluted corporate structure led to conflicting motivations and incentives within the company and it latter raised board‚Äôs concern about company‚Äôs commitment to safety.</p>
<p>Sam Altman was fired on Nov 17, 2023. On Nov 19, 2023, OpenAl hired former Twitch CEO Emmett Shear as its interim CEO. Meanwhile Microsoft CEO Satya Nadella announced that they would hired Sam to lead a new Al department. On Nov 20, 2023, Nearly all 800 OpenAl employees signed a letter calling for the resignation of the company‚Äôs board and the return of Altman as CEO. On Nov 21, 2023, Sam returned as CEO of OpenAl.</p>
<p>One story in 2020: Two of the lead Al developers Amodei and his sister Daniela left OpenAl in late 2020 to launch Anthropic over concerns the company was moving too quickly to commercialize its technology. Anthropic was founded aiming to develop more safer and trustworthy model and it has billions invested from Google, Amazon, Salesforce, Zoom, etc.</p>
<p>Since Sam was rehired, the questions about neglecting Al safety has been quieted, and new board members appear to be more focused on profitability. There is no doubt of OpenAl‚Äôs capability of profitably scaling ChatGPT, but it should raise doubts about whether OpenAl is still committing to its purpose in the future. </p>
<p>Sutskever was recruited to OpenAl from Google in 2015 by Elon Musk, who describes him as ‚Äúthe linchpin for OpenAl being successful‚Äù. A tweet from Greg Brockman confirms that Ilya was a key figure in Altman‚Äôs removal. But Sutskever also a signee calling Sam back to CEO. He later said he deeply regret his participation in the board‚Äôs actions.</p>
<blockquote>
<p><strong>National Artificial Intelligence Research Resource Pilot</strong> [<a target="_blank" rel="noopener" href="https://new.nsf.gov/focus-areas/artificial-intelligence/nairr">Link</a>]</p>
</blockquote>
<p>Training AI models costs huge amount of money. There is growing divide between industry and academia in AI. Thanks to this pilot programs stepping towards democratizing AI access.</p>
<blockquote>
<p>Other news:</p>
<p><strong>AI Hallucinations Are a Boon to Creatives</strong> [<a target="_blank" rel="noopener" href="https://www.bloomberg.com/news/articles/2024-01-04/the-ai-hallucinations-plaguing-chatbots-can-have-utility">Link</a>]</p>
<p><strong>Altman Seeks to Raise Billions for Network of AI Chip Factories</strong> [<a target="_blank" rel="noopener" href="https://www.bloomberg.com/news/articles/2024-01-19/altman-seeks-to-raise-billions-for-network-of-ai-chip-factories">Link</a>]</p>
</blockquote>
<h3 id="SubStack"><a href="#SubStack" class="headerlink" title="SubStack"></a>SubStack</h3><blockquote>
<p><strong>Tesla: AI &amp; Robotics - App Economy Insights</strong> [<a target="_blank" rel="noopener" href="https://open.substack.com/pub/appeconomyinsights/p/tesla-ai-and-robotics">Link</a>]</p>
</blockquote>
<p>What Tesla is experiencing: 1) price cuts, 2) prioritizing volume and fleet growth, 3) continued improvement in the cost of goods sold per vehicle. What negatively impact gross margin are 1) price cuts, 2) Cybertruck production ramp, 3) AI, 4) other product expenses. What positively impact gross margin are 1) lower cost per vehicle, 2) delivery growth, 3) gross margin improvement for non-auto segments. What to watch of Tesla‚Äôs business: 1) Model Y Triumph, 2) supercharging the EV market (North American Charging Standard (NACS)), 3) market share hit 4% in North America, 4) Autopilot and Full Self-Driving (FSD) beta software (V12), 5) energy storage deployments (15 gigawatt hours of batteries delivered), 5) Optimus humanoid robot, 6) next generation platform ‚ÄúRedwood‚Äù, 7) Dojo supercomputer.</p>
<blockquote>
<p><em>The Netherlands has restricted the export of ASML‚Äôs cutting-edge extreme ultraviolet (EUV) lithography machines to China, a decision influenced by international diplomatic pressures, notably from the US.</em></p>
<p><strong>‚Äï  ASML: Advanced Chip Monopoly - App Economy Insights</strong> [<a target="_blank" rel="noopener" href="https://open.substack.com/pub/appeconomyinsights/p/asml-advanced-chip-monopoly">Link</a>]</p>
</blockquote>
<p>ASML is the sole producer of advanced EUV lithorgraphy machines at the center of global chip supply chain. Since the restriction of export of EUV lithography machines to China, it is now also at the mercy of the US-China trade war. There is a list of risk ASML is facing: IP theft and security breaches, cybersecurity costs, China‚Äôs ambitions of developing its semiconductor sector, semiconductor industry volatility (shortage and gluts), etc. </p>
<h3 id="Paper-and-Reports"><a href="#Paper-and-Reports" class="headerlink" title="Paper and Reports"></a>Paper and Reports</h3><blockquote>
<p><strong>The economic potential of generative AI: The next productivity frontier</strong> [[Link](<a target="_blank" rel="noopener" href="https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/the-economic-potential-of-generative-ai-the-next-productivity-frontier">https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/the-economic-potential-of-generative-ai-the-next-productivity-frontier</a><br> Sent from McKinsey Insights, available in App Store: <a target="_blank" rel="noopener" href="https://itunes.apple.com/us/app/mckinsey-insights/id674902075?mt=8">https://itunes.apple.com/us/app/mckinsey-insights/id674902075?mt=8</a> and Play Store: <a target="_blank" rel="noopener" href="https://play.google.com/store/apps/details?id=com.mckinsey.mckinseyinsights)%5D">https://play.google.com/store/apps/details?id=com.mckinsey.mckinseyinsights)]</a></p>
<p><strong>McKinsey Technology Trends Outlook 2023</strong> [[Link](<a target="_blank" rel="noopener" href="https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/the-top-trends-in-tech">https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/the-top-trends-in-tech</a><br> Sent from McKinsey Insights, available in App Store: <a target="_blank" rel="noopener" href="https://itunes.apple.com/us/app/mckinsey-insights/id674902075?mt=8">https://itunes.apple.com/us/app/mckinsey-insights/id674902075?mt=8</a> and Play Store: <a target="_blank" rel="noopener" href="https://play.google.com/store/apps/details?id=com.mckinsey.mckinseyinsights)%5D">https://play.google.com/store/apps/details?id=com.mckinsey.mckinseyinsights)]</a></p>
</blockquote>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://jokerdii.github.io/digital-di/2024/01/02/hello-world/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/digital-di/images/avatar.gif">
      <meta itemprop="name" content="Di Zhen">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Di's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Di's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/digital-di/2024/01/02/hello-world/" class="post-title-link" itemprop="url">Hello World</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2024-01-02 13:28:31" itemprop="dateCreated datePublished" datetime="2024-01-02T13:28:31-05:00">2024-01-02</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-05-16 18:11:47" itemprop="dateModified" datetime="2024-05-16T18:11:47-04:00">2024-05-16</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>Welcome to <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a target="_blank" rel="noopener" href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a target="_blank" rel="noopener" href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a target="_blank" rel="noopener" href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <a class="extend prev" rel="prev" title="Previous page" aria-label="Previous page" href="/digital-di/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/digital-di/">1</a><span class="page-number current">2</span><a class="page-number" href="/digital-di/page/3/">3</a><a class="extend next" rel="next" title="Next page" aria-label="Next page" href="/digital-di/page/3/"><i class="fa fa-angle-right"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2024</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Di Zhen</span>
  </div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/digital-di/js/comments.js"></script><script src="/digital-di/js/utils.js"></script><script src="/digital-di/js/motion.js"></script><script src="/digital-di/js/schemes/muse.js"></script><script src="/digital-di/js/next-boot.js"></script>

  






  





</body>
</html>
