<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.1.1">

  <link rel="apple-touch-icon" sizes="180x180" href="/digital-di/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/digital-di/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/digital-di/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/digital-di/images/logo.svg" color="#222">

<link rel="stylesheet" href="/digital-di/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css" integrity="sha256-wiz7ZSCn/btzhjKDQBms9Hx4sSeUYsDrTLg7roPstac=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"jokerdii.github.io","root":"/digital-di/","images":"/digital-di/images","scheme":"Muse","darkmode":false,"version":"8.19.2","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/digital-di/js/config.js"></script>

    <meta name="description" content="Substack To me, the best model going forward is going to be based on the weighted performance per parameter and training token count. Ultimately, a model keeps getting better the longer you train it.">
<meta property="og:type" content="article">
<meta property="og:title" content="2024 May">
<meta property="og:url" content="https://jokerdii.github.io/digital-di/2024/05/04/2024-May/index.html">
<meta property="og:site_name" content="Di&#39;s Blog">
<meta property="og:description" content="Substack To me, the best model going forward is going to be based on the weighted performance per parameter and training token count. Ultimately, a model keeps getting better the longer you train it.">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2024-05-04T13:08:51.000Z">
<meta property="article:modified_time" content="2024-05-16T02:47:09.019Z">
<meta property="article:author" content="Di Zhen">
<meta property="article:tag" content="readings">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://jokerdii.github.io/digital-di/2024/05/04/2024-May/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"https://jokerdii.github.io/digital-di/2024/05/04/2024-May/","path":"2024/05/04/2024-May/","title":"2024 May"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>2024 May | Di's Blog</title>
  








  <noscript>
    <link rel="stylesheet" href="/digital-di/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
    </div>
  </div>

  <div class="site-meta">

    <a href="/digital-di/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Di's Blog</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
    </div>
  </div>
</div>







</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#Substack"><span class="nav-number">1.</span> <span class="nav-text">Substack</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Articles"><span class="nav-number">2.</span> <span class="nav-text">Articles</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#YouTube"><span class="nav-number">3.</span> <span class="nav-text">YouTube</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Papers-and-Reports"><span class="nav-number">4.</span> <span class="nav-text">Papers and Reports</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#News"><span class="nav-number">5.</span> <span class="nav-text">News</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Di Zhen</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/digital-di/archives/">
          <span class="site-state-item-count">11</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="https://jokerdii.github.io/digital-di/2024/05/04/2024-May/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/digital-di/images/avatar.gif">
      <meta itemprop="name" content="Di Zhen">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Di's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="2024 May | Di's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          2024 May
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2024-05-04 09:08:51" itemprop="dateCreated datePublished" datetime="2024-05-04T09:08:51-04:00">2024-05-04</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-05-15 22:47:09" itemprop="dateModified" datetime="2024-05-15T22:47:09-04:00">2024-05-15</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><h3 id="Substack"><a href="#Substack" class="headerlink" title="Substack"></a>Substack</h3><blockquote>
<p><em>To me, the best model going forward is going to be based on the <strong>weighted performance per parameter and training token count.</strong> Ultimately, a model keeps getting better the longer you train it. Most open model providers could train longer, but it hasn’t been worth their time. We’re starting to see that change.</em></p>
<p><em><strong>The most important models will represent improvements in capability density</strong>, rather than shifting the frontier.</em></p>
<p><em>In some ways, it’s easier to make the model better by training longer compared to anything else, if you have the data.</em></p>
<p><em>The core difference between open and closed LLMs on these charts is <strong>how undertrained open LLMs often are</strong>. The only open model confirmed to be trained on a lot of tokens is DBRX.</em> </p>
<p><strong>―  The End of the “Best Open LLM”</strong> [<a target="_blank" rel="noopener" href="https://www.interconnects.ai/p/compute-efficient-open-llms?utm_source=profile&utm_medium=reader2">Link</a>]</p>
</blockquote>
<p>Good analysis of the direction of open LLM development in 2023 and 2024. In 2023, models were progressing in MMLU by leveraging more compute budgets to handle scaled active parameters and training tokens. In 2024, the progressing direction is slightly changed to be orthogonal to previous - which is improving on MMLU while keeping compute budgets constant.</p>
<blockquote>
<p><em>The companies that have users interacting with their models consistently have moats through data and habits. The models themselves are not a moat, as I discussed at the end of last year when I tried to <a target="_blank" rel="noopener" href="https://www.interconnects.ai/p/ml-moats">predict machine learning moats</a>, but there are things in the modern large language model (LLM) space that open-source will really struggle to replicate. Concretely, that difference is access to quality and diverse training prompts for fine-tuning. While I want open-source to win out for personal philosophical and financial factors, this obviously is not a walk in the park for the open-source community. It’ll be a siege of a castle with, you guessed it, a moat. We’ll see if the moat holds.</em></p>
<p><strong>―  Model commoditization and product moats</strong> [<a target="_blank" rel="noopener" href="https://www.interconnects.ai/p/gpt4-commoditization-and-moats">Link</a>]</p>
</blockquote>
<blockquote>
<p><em>The goal of promoting scientific understanding for the betterment of society has a long history. Recently I was pointed to the essay <a target="_blank" rel="noopener" href="https://www.ias.edu/sites/default/files/library/UsefulnessHarpers.pdf">The Usefulness of Useless Knowledge</a> by Abraham Flexner in 1939 which argued how basic scientific research without clear areas for profit will eventually turn into societally improving technologies. If we want LLMs to benefit everyone, my argument is that we need far more than just computer scientists and big-tech-approved social scientists working on these models. We need to continue to promote openness to support this basic feedback loop that has helped society flourish over the last few centuries.</em></p>
<p><em>The word openness has replaced the phrase open-source among most leaders in the open AI movement. It’s the easiest way to get across what your goals are, but it is not better in indicating how you’re actually supporting the open ecosystem. The three words that underpin the one messy word are <strong>disclosure</strong> (the details), <strong>accessibility</strong> (the interfaces and infrastructure), and <strong>availability</strong> (the distribution).</em></p>
<p><strong>―  We disagree on what open-source AI should mean</strong> [<a target="_blank" rel="noopener" href="https://www.interconnects.ai/p/flavors-of-open-source-ai?utm_source=profile&utm_medium=reader2">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Google: “A Positive Moment”</strong> [<a target="_blank" rel="noopener" href="https://www.appeconomyinsights.com/p/alphabet-a-positive-moment?utm_source=profile&utm_medium=reader2">Link</a>]</p>
</blockquote>
<p>The report of Google Search’s death is exaggerated so far. In fact, search advertising has grown faster at Google than at Microsoft. User searching behavior is harder to change than people expected. Also, Google is leading the development of AI powered tools for Search: 1) “circle to search” is feature allowing a search from an image, text, or video without switching apps. 2) “Point your camera, ask a question” is a feature allowing for multisearch with both images and text for complex questions given an image to the tool. Overall, SGE (Search Generative Experience) is revolutionizing search experience (“10 blue links”) by  introducing a dynamic AI-enhanced experience. So far from I observed AI powers Google Search rather than weakens it.</p>
<blockquote>
<p><strong>Amazon: Wild Margin Expansion</strong> [<a target="_blank" rel="noopener" href="https://www.appeconomyinsights.com/p/amazon-wild-margin-expansion?utm_source=profile&utm_medium=reader2">Link</a>]</p>
</blockquote>
<p>Amazon’s margin expansion: AWS hit $100 B run rate with a 38% operating margin; Ads is surging; delivery costs have been reduced.</p>
<blockquote>
<p><em>The biggest risk is not correctly projecting demand for end-user AI consumption, which would threaten the utilization of the capacity and capital investments made by tech firms today. This would leave them exposed at the height of the valuation bubble, if and when it bursts, just like Cisco’s growth story that<a target="_blank" rel="noopener" href="https://www.wsj.com/amp/articles/SB973215201314032825"> began to unravel in 2000</a>.</em> <em>After all, history may not repeat, but it often rhymes.</em></p>
<p><em>At the Upfront Ventures confab mentioned earlier, Brian Singerman, a partner at Peter Thiel’s Founders Fund, was asked about contrarian areas worth investing in given the current landscape. <a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=QBzhRVRIf2Q&t=927s">His response</a>: “Anything not AI”.</em></p>
<p><strong>―  AI’s Bubble Talk Takes a Bite Out Of The Euphoria</strong> [<a target="_blank" rel="noopener" href="https://open.substack.com/pub/aisupremacy/p/ais-bubble-talk-takes-a-bite-out?r=333z5o&utm_medium=ios">Link</a>]</p>
</blockquote>
<p>When we talk about investment, we talk about economic values. Current situation of AI is very similar to Cisco’s in 2000. Cisco as an internet company spread the capacity of the World Wide Web, but sooner people realized that there is no economic value in internet company, instead, opportunities are in e-commerce etc. AI is a tool very similar to web tech. Currently, with heightened expectations, people are allocating investments and capital expenditure in AI model development, however, end-user demand is unclear and revenue is relatively minimal. This situation makes AI look like a bubble from a very long term perspective.</p>
<blockquote>
<p><em>Steve Jobs famously said that Apple stands at the intersection of technology and liberal arts. Apple is supposed to enhance and improve our lives in the physical realm, not to replace cherished physical objects indiscriminately.</em></p>
<p><strong>―  Apple’s Dystopian iPad Video</strong> [<a target="_blank" rel="noopener" href="https://newsletter.rationalwalk.com/p/apples-dystopian-ipad-video?r=333z5o&utm_medium=ios&triedRedirect=true">Link</a>]</p>
</blockquote>
<blockquote>
<p><em>Key pillars of the new strategy (on gaming):</em></p>
<ul>
<li><em>Expanding PC and cloud gaming options.</em></li>
<li><em>Powerful consoles (still a core part of the vision).</em></li>
<li><em>Game Pass subscriptions as the primary access point.</em></li>
<li><em>Actively bringing Xbox games to rival platforms (PS5, Switch).</em></li>
<li><em>Exploring mobile gaming with the potential for handheld hardware.</em></li>
</ul>
<p><em>Microsoft’s “every screen is an Xbox” approach is a gamble and may take a long time to pay off. But the industry is bound to be device-agnostic over time as it shifts to the cloud and offers cross-play and cross-progression. It’s a matter of when not if.</em></p>
<p><strong>―  Microsoft: AI Inflection</strong> [<a target="_blank" rel="noopener" href="https://www.appeconomyinsights.com/p/microsoft-ai-inflection?r=333z5o&utm_medium=ios&triedRedirect=true">Link</a>]</p>
</blockquote>
<p>Highlights: Azure’s growth accelerated sequentially thanks to AI services and was the fastest-growing of the big three (Amazon AWS, Google Cloud, Microsoft Azure). On Search, Microsoft is losing market share to Alphabet. Capex on AI grows roughly 80% YoY. On gaming, it’s diversifying approaches from selling consoles. Copilot and the Office succeed with Enterprise customers.</p>
<blockquote>
<p><strong>Streaming Wars Visualized</strong> [<a target="_blank" rel="noopener" href="https://open.substack.com/pub/appeconomyinsights/p/streaming-wars-visualized?r=333z5o&utm_medium=ios">Link</a>]</p>
<p><strong>This Week in Visuals</strong> [<a target="_blank" rel="noopener" href="https://www.appeconomyinsights.com/p/this-week-in-visuals?lli=1&utm_source=profile&utm_medium=reader2">Link</a>]</p>
</blockquote>
<h3 id="Articles"><a href="#Articles" class="headerlink" title="Articles"></a>Articles</h3><blockquote>
<p><strong>Musings on building a Generative AI product</strong> [<a target="_blank" rel="noopener" href="https://www.linkedin.com/blog/engineering/generative-ai/musings-on-building-a-generative-ai-product">Link</a>]</p>
</blockquote>
<p>This is a very good read about developing Gen AI product for business by using pre-trained LLM. This article elaborates how this product is designed, how each part works specifically, what works and what does not work, what has been improving, and what has been struggling. Some takeaways for me are </p>
<ol>
<li><p>Supervised fine tuning step was done by embedding-based retrieval (EBR) powered by an in-memory database to inject response examples into prompts. </p>
</li>
<li><p>An organizational structure was designed to ensure communication consistency: one horizontal engineering pod for global templates and styles, and several vertical engineering pods for specific tasks such as summarization, job fit assessment, interview tips, etc.</p>
</li>
<li><p>Tricky work: </p>
<ol>
<li><p>Developing end to end automatic evaluation pipeline.</p>
</li>
<li><p>Skills in dynamically discover and invoke APIs &#x2F; agents.</p>
<p>This requires input and output to be ‘LLM friendly’ - JSON or YAML schemes. </p>
</li>
<li><p>Supervised fine tuning by injected responses of internal database.</p>
<p>As evaluation becoming more sophisticated, prompt engineering needs to be improved to reach high quality&#x2F;evaluation scores. The difficulty is that quality scores shoot up fast then plateau so it’s hard to reach a very high score in the late improvement stage. This makes prompt engineering more like an art rather than science. </p>
</li>
<li><p>Tradeoff of capacity and latency</p>
<p>Chain of Thoughts can improve quality and accuracy of responses, but increase latency. TimeToFirstToken (TTFT) &amp; TimeBetweenTokens (TBT) are important to utilization but need to be bounded to limit latency. Besides, they also intend to implement end to end streaming and async non-blocking pipeline.</p>
</li>
</ol>
</li>
</ol>
<blockquote>
<p><em>The concept of open source was devised to ensure developers could use, study, modify, and share software without restrictions. But AI works in fundamentally different ways, and key concepts don’t translate from software to AI neatly, says Maffulli.</em></p>
<p><em>But depending on your goal, dabbling with an AI model could require access to the trained model, its training data, the code used to preprocess this data, the code governing the training process, the underlying architecture of the model, or a host of other, more subtle details.</em></p>
<p><em>Which ingredients you need to meaningfully study and modify models remains open to interpretation.</em> </p>
<p><em>both Llama 2 and Gemma come with licenses that restrict what users can do with the models. That’s anathema to open-source principles: one of the key clauses of the Open Source Definition outlaws the imposition of any restrictions based on use cases.</em></p>
<p><em>All the major AI companies have simply released pretrained models, without the data sets on which they were trained. For people pushing for a stricter definition of open-source AI, Maffulli says, this seriously constrains efforts to modify and study models, automatically disqualifying them as open source.</em></p>
<p><strong>―  The tech industry can’t agree on what open-source AI means. That’s a problem. ― MIT Technology Review</strong> [<a target="_blank" rel="noopener" href="https://www.technologyreview.com/2024/03/25/1090111/tech-industry-open-source-ai-definition-problem/">Link</a>]</p>
</blockquote>
<p>This article argues that the definitions of open-source AI are problematic. ‘Open’ models either have restriction on usage or don’t release details of training data. This does not fit traditional definition of ‘open source’. However, people argue that for the special case of AI, we need different definition of open source. As long as the definition remains vague, it’s problematic, because big tech will define open-source AI to be what suits it. </p>
<blockquote>
<p><strong>Everything I know about the XZ backdoor</strong> [<a target="_blank" rel="noopener" href="https://boehs.org/node/everything-i-know-about-the-xz-backdoor">Link</a>]</p>
<p>Some great high-level technical overview of XZ backdoor [<a target="_blank" rel="noopener" href="https://gynvael.coldwind.pl/?lang=en&id=782">Link</a>] [<a target="_blank" rel="noopener" href="https://gist.github.com/thesamesam/223949d5a074ebc3dce9ee78baad9e27">Link</a>] [<a target="_blank" rel="noopener" href="https://gist.github.com/thesamesam/223949d5a074ebc3dce9ee78baad9e27">Link</a>] [<a target="_blank" rel="noopener" href="https://infosec.exchange/@fr0gger/112189232773640259">Infographic</a>] [<a target="_blank" rel="noopener" href="https://research.swtch.com/xz-script">Link</a>] [<a target="_blank" rel="noopener" href="https://bsky.app/profile/filippo.abyssdomain.expert/post/3kowjkx2njy2b">Link</a>]</p>
</blockquote>
<p>A backdoor in xz-utils (used for lossless compression) was recently revealed by Andres Freund (Principle SDE at Microsoft). The backdoor only shows up when a few specific criteria are met at least: 1) running a distro that uses glibc, 2) have version 5.6.0 or 5.6.1 xz installed or liblzma installed. There is a malicious script called <code>build-to-host.m4</code> which checks for various conditions like the architecture of the machine. If those conditions check, the payload is injected into the source tree. The intention of payload is still under investigation. Lasse Collin, one of the maintainer of the repo, has posted an <a target="_blank" rel="noopener" href="https://tukaani.org/xz-backdoor/">update</a> and is working on carefully analyzing the situation. The author Evan Boehs in the article present a timeline of the attack and online investigators’ discoveries of Jia Tan identity (from IP address, LinkedIn, <a target="_blank" rel="noopener" href="https://rheaeve.substack.com/p/xz-backdoor-times-damned-times-and">commit timings</a>, etc), and raises our awareness of the human costs of open source.</p>
<blockquote>
<p><em>Having a crisp mental model around a problem, being able to break it down into steps that are tractable, perfect first-principle thinking, sometimes being prepared (and able to) debate a stubborn AI — these are the skills that will make a great engineer in the future, and likely the same consideration applies to many job categories.</em></p>
<p><strong>―  Why Engineers Should Study Philosophy ―  Harvard Business Review</strong> [<a target="_blank" rel="noopener" href="https://hbr.org/2024/04/why-engineers-should-study-philosophy?utm_medium=email&utm_source=circ_other&utm_campaign=subbenemail_digitalcontent_monthinreview&hideIntromercial=true&tpcc=subbenemail&deliveryName=SUB_Ben_DigitialContent_MonthInReview_20240507">Link</a>]</p>
</blockquote>
<p>Human comes into a new stage of learning: smartly asking AI questions to get answers as accurate as possible. So prompt engineering is a very important skill in AI era. In order to master prompt engineering, we need to have divide and conquer mindset, perfect first-principle thinking, critical thinking, and skepticism.</p>
<blockquote>
<p><strong>Chatbot Arena results are in: Llama 3 dominates the upper and mid cost-performance front (full analysis) ― Reddit</strong> [<a target="_blank" rel="noopener" href="https://www.reddit.com/r/LocalLLaMA/comments/1cakcfq/chatbot_arena_results_are_in_llama_3_dominates/">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Efficiently fine-tune Llama 3 with PyTorch FSDP and Q-Lora</strong> [<a target="_blank" rel="noopener" href="https://www.philschmid.de/fsdp-qlora-llama3">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Mamba Explained</strong> [<a target="_blank" rel="noopener" href="https://thegradient.pub/mamba-explained/">Link</a>]</p>
</blockquote>
<p>A very in-depth explanation of Mamba architecture.</p>
<h3 id="YouTube"><a href="#YouTube" class="headerlink" title="YouTube"></a>YouTube</h3><blockquote>
<p><em>I don’t have an answer to peace in the Middle East, I wish I did, but I do have a very strong view that we are not going to get to peace when we are apologizing or denying crimes against humanity and crime mass rape of women. That’s not the path to peace, the path to peace is not saying this didn’t happen, the path to peace is saying this happened no matter what side of the fence you are on no matter what side of the world you are on, if you are the far right the far left, anywhere on the world, we are not going to let this happen again and we are going to get to peace to make sure.  - Sheryl Sandberg</em></p>
<p><strong>―  In conversation with Sheryl Sandberg, plus open-source AI gene editing explained</strong> [<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=OxbtNsenZJY&ab_channel=All-InPodcast">Link</a>]</p>
<p>U.N. to Study Reports of Sexual Violence in Israel During Oct. 7 Attack [<a target="_blank" rel="noopener" href="https://www.nytimes.com/2024/01/29/world/middleeast/israel-hamas-sexual-violence-un.html">Link</a>]</p>
<p>Western media concocts ‘evidence’ UN report on Oct 7 sex crimes failed to deliver [<a target="_blank" rel="noopener" href="https://thegrayzone.com/2024/03/07/media-concocts-un-hamas-rape-report/">Link</a>]</p>
</blockquote>
<p>It’s crazy that what is happening right now in some of the colleges is not to protest sexual violence as a tool of war by Hamas. This kind of ignorance or denial of sexual violence is horrible. People are so polarized to black and white that if something does not fit into their view, they are going to reject it. There are more than two sides to the Middle East story, one of them is sexual violence - mass rape, genital mutilation of men and women, women tied to trees naked bloody leg spread…</p>
<p>There is a long history of the involvement of women’s bodies in Wars. It’s only 30 years ago, people started to say rape is not a tool of War and should be prosecuted as a war crime against humanity. The feminist, human rights, and civil rights groups made this happen. Now it happened again in Gaza according to the report released by U.N., however there are a lot difficulties in proving and testifying the truth e.g. they couldn’t locate a single victim, or they don’t have the victim rights to take pictures. But victims are dead and they cannot speak up. Denying the fact of sexual violence is just unacceptable. And there is such a great <a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=zAr9oGSXgak&ab_channel=ScreamsBeforeSilence">documentary</a> shedding lights on the unspeakable sexual violence committed on Oct 7, 2023 that I think everyone should watch.</p>
<p>Good news is that the testimony of eyewitness meets the criteria of any international or global court. So crimes can be proven by any eyewitness for sure.</p>
<blockquote>
<p><strong>John Schulman - Reinforcement Learning from Human Feedback: Progress and Challenges</strong> [<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=hhiLw5Q_UFg&ab_channel=BerkeleyEECS">Link</a>]</p>
</blockquote>
<p>John Schulman is a research scientist and cofounder of OpenAI, focusing on Reinforcement Learning (RL) algorithms. He gave a talk on making AI more truthful on Apr 24, 2023 in UCB. The ideas and discussions are still helpful and insightful today.</p>
<p>In this talk, John discussed the issue of hallucination with large language models. He claims that behavior cloning or supervised learning is not enough to fix the hallucination problem, instead, reinforcement learning from human feedback (RLHF) can help improve the model’s truthfulness by 1) adjusting output distribution so model is allowed to express uncertainty, challenge premise, admit error, and 2) learning behavior boundaries. In his conceptual model, fine-tuning leads the model to hallucinate when it lacks knowledge. Retrieval and citing external sources can help improve verifiability. John discusses models that can browse the web to answer technical questions, citing relevant sources. </p>
<p>John mentioned three open problems in LLM: 1) how to train models to express uncertainty in natural language, 2) go beyond what human labelers can easily verify (“scalable oversight”), and 3) optimizing for true knowledge rather than human approval.</p>
<blockquote>
<p><strong>The 1-Year Old AI Startup That’s Rivaling OpenAI —  Redpoint’s AI Podcast</strong> [<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=_N2KPEdh69s&ab_channel=UnsupervisedLearning:Redpoint%27sAIPodcast">Link</a>]</p>
</blockquote>
<p>A great interview with the CEO of Mistral Arthur Mensch on the topic of sovereignty and open models as a business strategy. Here are some highlighted points from Arthur:</p>
<ol>
<li>Open-source is going to solidify in the future. It is an infrastructure technology and at the end of the day it should be modifiable and owned by customers. Now Mistral has two offerings, open source one and commercial one, and the aim is to find out the business model to sustain the open source development.</li>
<li>The things that Mistral is best at 1) training model, and 2) specializing models. </li>
<li>The way they think about partnership strategy is to look at what enterprises would need, where they were operating, where the developers were operating, and figure out the channels that would facilitate adoption and spread. To be a multiplatform solution and to replicate the solution to different platforms is a strategy that Mistral is following.</li>
<li>There is still an efficiency upper bound to be pushed. Other than compute to spend on pre-training, there is still research to do on improving model efficiency and strength. On architecture side, we can be more efficient than plain Transformer which spends same amount of compute on every token. Mistral is making model faster. By making model faster, we open up a lot of applications that involve an LLM as a basic brick and then we can figure out how to do planning, explorations, etc. By increasing efficiency, we open up areas of research.</li>
<li>Meta has more GPUs than Mistral do. But Mistral has a good concentration of GPU (number of GPU per person). This is the way to be as efficient as possible to come up with creative ways of training models. Also unit economics need to be considered to make sure that $1 that you spend on training compute eventually accrues to more than $1 revenue.</li>
<li>Transformer is not an optimal architecture. It’s been out there for 7 years now. Everything is co-adapted to it such as training methods, debug methods, the algorithms, and hardware. It’s challenging to find a better one and also beat the baseline. But there are a lot of research on modification of attention to boost memory efficiencies and a lot of things can be done in that direction and similar directions.</li>
<li>About AI regulations and EU AI Act, Arthur states that it does not solve the actual problem of how to make AI safe. Because making AI safe is a hard problem (stochastic model), different from the way we evaluate software before. It’s more like a product problem rather than a regulation problem. We need to rethink continuous integration, verifications, etc and make sure everything is happening as it should be.</li>
<li>Mistral recently released Le Chat to help enterprise start incorporating AI. It gives an assistant that is contextualized on their enterprise data. It’s a tool to be closer to the end user to get feedback for the developer platform and also a tool to get the enterprise into GenAI.</li>
</ol>
<blockquote>
<p><strong>Open Source AI is AI we can Trust — with Soumith Chintala of Meta AI</strong> [<a target="_blank" rel="noopener" href="https://www.latent.space/p/soumith">Link</a>]</p>
</blockquote>
<p>Synthetic data is the next rage of LLM. Soumith pointed out that synthetic data is where we as humans already have good symbolic models off, we need to impart that knowledge to neural networks, and we figured out the synthetic data is a vehicle to impart this knowledge to it. Related to synthetic data but in an unusual way, there is new research on distilling GPT-4 by creating synthetic data from GPT-4, creating mock textbooks inspired by Phi-2 and then fine tuning open source models like Lambda.  </p>
<p>Open source means different things to different people and we haven’t had a community norm definition yet at this very early stage of LLM. When being asked about open source, people in this field are used to highlight the definition of it in advance. In the open source topic, Soumith pointed out that the most beneficial value of open is it makes the distribution very wide and available with no friction so that people can do transformative things in a way that is very accessible. </p>
<blockquote>
<p><strong>Berkshire Hathaway 2024 Annual Meeting Movie: Tribute to Charlie Munger</strong> [<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=uGrf5PRFSJY&ab_channel=TheCapitalist">Link</a>]</p>
</blockquote>
<p>First year that the annual meeting movie is made public. First year that the annual meeting is without Charlie. Already started to miss his jokes.</p>
<blockquote>
<p><em>I think the reason why the car could have been completely reimagined by Apple is that they have a level of credibility and trust that I think probably no other company has, and absolutely no other tech company has. I think this was the third Steve Jobs story that I left out but in 2001, I launched a 99 cent download store and Steve Jobs just ran total circles around us, but the reason he was able to is he had all the credibility to go to the labels and get deals done for licensing music that nobody could get done before. I think that is an example of what Apple’s able to do which is to use their political capital to change the rules. So if the thing that we could all want is safer roads and autonomous vehicles, there are regions in every town and city that could be completely converted to level 5 autonomous zones. If I had to pick one company that had the credibility to go and change those rules, it’s them. Because they could demonstrate that there was a methodical safe approach to doing something. So the point is that even in these categories that could be totally reimagined, it’s not for a lack of imagination, again it just goes back to a complete lack of will. I understand because if you had 200B dollars of capital on your balance sheet, I think it’s probably easy to get fat and lazy.</em> </p>
<p><strong>―  In conversation with Sam Altman — All-In Podcast</strong> [<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=nSM0xd8xHUM&ab_channel=All-InPodcast">Link</a>]</p>
</blockquote>
<h3 id="Papers-and-Reports"><a href="#Papers-and-Reports" class="headerlink" title="Papers and Reports"></a>Papers and Reports</h3><blockquote>
<p><strong>Large Language Models: A Survey</strong> [<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2402.06196">Link</a>]</p>
</blockquote>
<p>This is a must-read paper if you would like to have a comprehensive overview of SOTA LLMs, technical details, applications, datasets, benchmarks, challenges, and future directions.</p>
<blockquote>
<p><strong>Little Guide to Building Large Language Models in 2024 - HuggingFace</strong> [<a target="_blank" rel="noopener" href="https://docs.google.com/presentation/d/1IkzESdOwdmwvPxIELYJi8--K3EZ98_cL6c5ZcLKSyVg/edit?usp=sharing">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>Are ChatGPT and GPT-4 General-Purpose Solvers for Financial Text Analytics? A Study on Several Typical Tasks</strong> [<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2305.05862">Link</a>]</p>
</blockquote>
<p>Bloomberg fine-tuned GPT-3.5 on their financial data only to find that GPT-4 8k, without specialized finance fine-tuning, beat it on almost all finance tasks. So there is really a moat?</p>
<blockquote>
<p><strong>Jamba: A Hybrid Transformer-Mamba Language Model</strong> [<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2403.19887">Link</a>] [<a target="_blank" rel="noopener" href="https://www.ai21.com/blog/announcing-jamba">Link</a>]</p>
</blockquote>
<p><a target="_blank" rel="noopener" href="https://openreview.net/forum?id=AL1fq05o7H">Mamba paper</a> has been rejected while fruits are reaped fast: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2401.04081">MoE-Mamba</a>, <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2401.09417">Vision Mamba</a>, and Jamba. It’s funny to see the asymmetric impact in ML sometimes, e.g. FlashAttention has &lt;500 citations and is used everywhere. Github repos used by 10k+ has &lt;100 citations, etc.</p>
<blockquote>
<p><strong>KAN: Kolmogorov-Arnold Networks</strong> [<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.19756">Link</a>] [<a target="_blank" rel="noopener" href="https://github.com/KindXiaoming/pykan?tab=readme-ov-file#authors-note">authors-note</a>]</p>
</blockquote>
<p>This is a mathematically beautiful idea. The main difference between traditional MLP and KAN is that KAN have learnable activation function on weights, so all weights in KAN are non-linear. KAN outperforms MLP in accuracy and interpretability. Whether in the future KAN is able to replace MLP depends on whether there could be suitable learning algorithms like SGD, AdamW, etc and whether it will be GPU friendly.</p>
<blockquote>
<p><strong>DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model</strong> [<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.04434">Link</a>] </p>
</blockquote>
<blockquote>
<p><strong>What matters when building vision-language models</strong> [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2405.02246">Link</a>]</p>
</blockquote>
<blockquote>
<p><strong>The Unreasonable Ineffectiveness of the Deeper Layers</strong> [<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2403.17887">Link</a>]</p>
</blockquote>
<h3 id="News"><a href="#News" class="headerlink" title="News"></a>News</h3><blockquote>
<p><strong>Intel Inside Ohio</strong> [<a target="_blank" rel="noopener" href="https://www.bloomberg.com/features/2024-intel-comeback-chipmaking/">Link</a>]</p>
<p>Intel Ohio One Campus Video Rendering [<a target="_blank" rel="noopener" href="https://vimeo.com/939198943">Link</a>]</p>
</blockquote>
<p>Intel Corp has committed $28B to build a “mega fab” called Ohio One which could be the biggest chip factory on Earth. The Biden administration has agreed to provide Intel with $19.5B in loans and grants to support finance the project.</p>
<blockquote>
<p><strong>EveryONE Medicines: Designing Drugs for Rare Diseases, One at a Time</strong> [<a target="_blank" rel="noopener" href="https://www.wsj.com/articles/everyone-medicines-designing-drugs-for-rare-diseases-one-at-a-time-a6f98afc">Link</a>]</p>
</blockquote>
<p>Startup EveryONE Medicine aims to develop drugs designed based on genetic information for individual children who have rare, life-threatening neurological diseases. Since the number of patients with diseases caused by rare mutation is significant, the market share is large if EveryONE can scale its process. Although the cost won’t be the same as a standard drugmaker that runs large clinical trials, the challenge is safety without a standard clinical-testing protocol. To be responsible to patients, the initial drugs will have a temporary effect and a wide therapeutic window, so the potential toxicity will be minimized or stopped if there is.</p>
<blockquote>
<p><strong>Voyager 1’s Communication Malfunctions May Show the Spacecraft’s Age</strong> [<a target="_blank" rel="noopener" href="https://www.discovermagazine.com/the-sciences/voyager-1s-communication-malfunctions-may-show-the-spacecrafts-age">Link</a>]</p>
</blockquote>
<p>In Nov 2023, NASA’s over 46-year-old Voyager 1 spacecraft started sending nonsense to Earth. Voyager 1 was initially intended to study Jupiter and Saturn and was built to survive only 5 years of flight, however the trajectory was forged further and further into space and so the mission converted from a two-planet mission to an interstellar mission.</p>
<p>In Dec 2023, the mission team restarted the Flight Data Subsystem (FDS)  but failed to return the subsystem to functional state. On Mar 1 2023, they sent a command “poke” to the probe and received a response on Mar 3. On Mar 10, the mission team finally determined the response carried a readout of FDS memory. By comparing the readout with those received before the issue, the team confirmed that 3% of FDS memory was corrupted. On Apr 4, the team concluded the affected code was contained on a computer chip. To solve the problem, the team decided to divide these affected code into smaller sections and to insert those smaller sections into other operative places in the FDS memory. During Apr 18-20, the team sent out the orders to move some of the affected code and received responses with intelligible systems information. </p>
<blockquote>
<p><strong>Editing the Human Genome with AI</strong> [<a target="_blank" rel="noopener" href="https://www.profluent.bio/blog/editing-the-human-genome-with-ai">Link</a>]</p>
</blockquote>
<p>Berkeley based startup Profluent Bio used an AI based protein language model to create and train on an entirely new library of Cas proteins that do not exist in nature today and eventually find one called ‘OpenCRISPR-1’ that is able to replace or improve the ones that are on the market today. The goal of this AI model is to learn what sequence of DNA generated what structure of protein that’s really good at gene editing. The new library of Cas proteins is created by simulation of trillions of letters. They made ‘OpenCRISPR-1’ publicly available under an open source license so anyone can use this particular Cas protein.</p>
<blockquote>
<p><strong>Sony and Apollo in Talks to Acquire Paramount</strong> [<a target="_blank" rel="noopener" href="https://www.nytimes.com/2024/05/05/business/media/sony-apollo-paramount.html">Link</a>]</p>
</blockquote>
<p>Paramount’s stock declined 44% in 2022 and another 12% in 2023. It’s experiencing declining revenue as consumers abandon traditional pay-TV and it’s losing streaming business. Berkshire sold its entire Paramount shares in March 2023 and soon Sony Pictures and Apollo Globals Management reached out to Paramount board expressing interest of acquisition. Now Paramount decided to open negotiation with them after exclusive talks with Hollywood studio Skydance. This deal would break the Paramount and potentially transform the media landscape if successful. Otherwise an office of the CEO as the replacement of CEO Bob Bakish will be preparing a long term plan for the company.</p>
<blockquote>
<p><strong>AlphaFold 3 predicts the structure and interactions of all of life’s molecules</strong> [<a target="_blank" rel="noopener" href="https://blog.google/technology/ai/google-deepmind-isomorphic-alphafold-3-ai-model/">Link</a>]</p>
</blockquote>
<p>Previously, Google DeepMind AlphaFold project took 3D images of proteins and the DNA sequence that codes for those proteins and then they built a predictive model that predicted the 3D structure of protein base on DNA sequence. What is difference in AlphaFold 3 is that all small molecules are included. The way how small molecules are bind together with the protein is part of the predictive model. This is a breakthrough in that off target effect could be minimized by taking consideration of other molecules’ interactions in the biochemistry environment. Google has a drug development subsidiary called Isomorphic Labs. They kept all of IP for AlphaFold 3. They published a web viewer for non-commercial scientists to do fundamental research but only Isomorphic Labs can make it for commercial use. </p>
<blockquote>
<p><strong>Introducing GPT-4o and making more capabilities available for free in ChatGPT</strong> [<a target="_blank" rel="noopener" href="https://openai.com/index/spring-update/">Link</a>]</p>
</blockquote>
<p>I missed the live announcement but watched the recording. GPT-4o is amazing.</p>
<blockquote>
<p>Other News:</p>
<p><strong>Encampment Protesters Set Monday Deadline for Harvard to Begin Negotiations</strong> [<a target="_blank" rel="noopener" href="https://www.thecrimson.com/article/2024/5/3/yard-encampment-negotiation-deadline/">Link</a>]</p>
<p><strong>Israel Gaza war: History of the conflict explained</strong> [<a target="_blank" rel="noopener" href="https://www.bbc.com/news/newsbeat-44124396">Link</a>]</p>
<p><strong>Cyber Stuck: First Tesla Cybertruck On Nantucket Has A Rough Day</strong>  [<a target="_blank" rel="noopener" href="https://nantucketcurrent.com/news/cyber-stuck-first-tesla-cybertruck-on-nantucket-has-a-rough-day">Link</a>]</p>
<p><strong>Apple apologizes after ad backlash</strong> [<a target="_blank" rel="noopener" href="https://www.linkedin.com/news/story/apple-apologizes-after-ad-backlash-6024884/">Link</a>]</p>
<p><strong>Apple nears deal with OpenAI to put ChatGPT on iPhone: Report</strong> [<a target="_blank" rel="noopener" href="https://www.businesstoday.in/technology/news/story/apple-nears-deal-with-openai-to-put-chatgpt-on-iphone-report-429178-2024-05-11">Link</a>]</p>
</blockquote>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/digital-di/tags/readings/" rel="tag"># readings</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/digital-di/2024/04/30/2024-April/" rel="prev" title="2024 April">
                  <i class="fa fa-angle-left"></i> 2024 April
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/digital-di/2024/05/11/Is-AI-a-Bubble/" rel="next" title="Is AI a Bubble?">
                  Is AI a Bubble? <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2024</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Di Zhen</span>
  </div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/digital-di/js/comments.js"></script><script src="/digital-di/js/utils.js"></script><script src="/digital-di/js/motion.js"></script><script src="/digital-di/js/schemes/muse.js"></script><script src="/digital-di/js/next-boot.js"></script>

  






  





</body>
</html>
